{"posts":[{"title":"redis","content":"https://segmentfault.com/a/1190000022808576","link":"https://chriswsq.github.io/post/redis/"},{"title":"cita链管理服务部署","content":"1.链管理服务依赖环境•python&gt;=3.7.9•docker&gt;=19.0安装3.7.9版本参考：https://blog.csdn.net/lkgCSDN/article/details/84403329https://chriswsq.github.io/post/elk-gao-jing-zhi-elastalert-bu-shu-ji-pei-zhi/ln-s/usr/local/python3/bin/pip3/usr/bin/pip3ln-s/usr/local/python3/usr/bin/python3ln-s/usr/local/python3/bin/gunicorn/usr/bin/gunicorn2.链管理服务部署步骤解压源码包并进入目录tarxzvfbsn-cita-chain-manager.tar.gzcdbsn-cita-chain-manager安装依赖包pip3install-rrequirements.txt-ihttps://pypi.douban.com/simple生成pub_key、pri_key通过cita-cli工具，执行./cita-clikeycreate--algorithmsm2，将address字段写入pub_key，将private字段写入pri_key修改配置文件vimconf/config.py#链节点密钥对的路径及文件名称PUBKEY_PATH='/opt/pub_key'PRIKEY_PATH='/opt/pri_key'#cita镜像名称，rebirth镜像名称以及rebirth依赖mysql的镜像名称（下文详细说明）CITA_IMAGE_VERSION='cita/cita-ee:1.3.1-sm2-sm3'MYSQL_IMAGE_VERSION='cita-for-bsn-mysql:5.7'REBIRTH_IMAGE_VERSION='cita-for-bsn-rebirth:0.1'#链管理服务产生链配置的目录CHAIN_CONFIG_PATH='/opt/all-chain-config'#链节点TLS证书所在目录TLS_PATH='/opt'#solc编译器镜像名称SOLC_IMAGE_VERSION='ethereum/solc:0.4.24'","link":"https://chriswsq.github.io/post/cita-lian-guan-li-fu-wu-bu-shu/"},{"title":"docker-compsoe部署kafka集群","content":"前言kafka依赖于zookeeper存放元数据,所以在创建kafka集群之前需要创建zookeeper;更多关于zookeeper集群创建见:使用Docker部署zookeeper集群拉取镜像dockerpullzookeeper:3.4dockerpullzookeeper:3.4dockerpullwurstmeister/kafka_2.13-2.6.0其中kafka版本中的2.12为Scala版本创建子网段dockernetworkcreate--subnet172.30.1.0/16--gateway172.30.0.1kafkaversion:'3'services:zoo1:image:zookeeper:3.4restart:alwayshostname:zoo1container_name:zoo1ports:-2184:2181volumes:-/home/zk/workspace/volumes/zkcluster/zoo1/data:/data-/home/zk/workspace/volumes/zkcluster/zoo1/datalog:/datalogenvironment:ZOO_MY_ID:1ZOO_SERVERS:server.1=0.0.0.0:2888:3888server.2=zoo2:2888:3888server.3=zoo3:2888:3888networks:kafka:ipv4_address:172.30.0.11zoo2:image:zookeeper:3.4restart:alwayshostname:zoo2container_name:zoo2ports:-2185:2181volumes:-/home/zk/workspace/volumes/zkcluster/zoo2/data:/data-/home/zk/workspace/volumes/zkcluster/zoo2/datalog:/datalogenvironment:ZOO_MY_ID:2ZOO_SERVERS:server.1=zoo1:2888:3888server.2=0.0.0.0:2888:3888server.3=zoo3:2888:3888networks:kafka:ipv4_address:172.30.0.12zoo3:image:zookeeper:3.4restart:alwayshostname:zoo3container_name:zoo3ports:-2186:2181volumes:-/home/zk/workspace/volumes/zkcluster/zoo3/data:/data-/home/zk/workspace/volumes/zkcluster/zoo3/datalog:/datalogenvironment:ZOO_MY_ID:3ZOO_SERVERS:server.1=zoo1:2888:3888server.2=zoo2:2888:3888server.3=0.0.0.0:2888:3888networks:kafka:ipv4_address:172.30.0.13kafka1:image:wurstmeister/kafkarestart:alwayshostname:kafka1container_name:kafka1privileged:trueports:-9092:9092environment:KAFKA_ADVERTISED_HOST_NAME:kafka1KAFKA_LISTENERS:PLAINTEXT://kafka1:9092KAFKA_ADVERTISED_LISTENERS:PLAINTEXT://kafka1:9092KAFKA_ADVERTISED_PORT:9092KAFKA_ZOOKEEPER_CONNECT:zoo1:2181,zoo2:2181,zoo3:2181volumes:-/home/zk/workspace/volumes/kafkaCluster/kafka1/logs:/kafkanetworks:kafka:ipv4_address:172.30.1.11extra_hosts:-zoo1:172.30.0.11-zoo2:172.30.0.12-zoo3:172.30.0.13depends_on:-zoo1-zoo2-zoo3external_links:-zoo1-zoo2-zoo3kafka2:image:wurstmeister/kafkarestart:alwayshostname:kafka2container_name:kafka2privileged:trueports:-9093:9093environment:KAFKA_ADVERTISED_HOST_NAME:kafka2KAFKA_LISTENERS:PLAINTEXT://kafka2:9093KAFKA_ADVERTISED_LISTENERS:PLAINTEXT://kafka2:9093KAFKA_ADVERTISED_PORT:9093KAFKA_ZOOKEEPER_CONNECT:zoo1:2181,zoo2:2181,zoo3:2181volumes:-/home/zk/workspace/volumes/kafkaCluster/kafka2/logs:/kafkanetworks:kafka:ipv4_address:172.30.1.12extra_hosts:-zoo1:172.30.0.11-zoo2:172.30.0.12-zoo3:172.30.0.13depends_on:-zoo1-zoo2-zoo3external_links:-zoo1-zoo2-zoo3kafka3:image:wurstmeister/kafkarestart:alwayshostname:kafka3container_name:kafka3privileged:trueports:-9094:9094environment:KAFKA_ADVERTISED_HOST_NAME:kafka3KAFKA_LISTENERS:PLAINTEXT://kafka3:9094KAFKA_ADVERTISED_LISTENERS:PLAINTEXT://kafka3:9094KAFKA_ADVERTISED_PORT:9094KAFKA_ZOOKEEPER_CONNECT:zoo1:2181,zoo2:2181,zoo3:2181volumes:-/home/zk/workspace/volumes/kafkaCluster/kafka3/logs:/kafkanetworks:kafka:ipv4_address:172.30.1.13extra_hosts:-zoo1:172.30.0.11-zoo2:172.30.0.12-zoo3:172.30.0.13depends_on:-zoo1-zoo2-zoo3external_links:-zoo1-zoo2-zoo3networks:kafka:ipam:config:-subnet:172.30.0.0/16在kafka服务中声明了depends_on,所以在所有zookeeper启动之后才会真正启动kafka容器","link":"https://chriswsq.github.io/post/docker-compsoe-bu-shu-kafka-ji-qun/"},{"title":"Loki配置邮件告警","content":"监控的目的就是及时发现问题去解决处理，那么在告警就是必不可少的。本次告警配置是loki结合grafana来进行邮件告警grafana配置告警1.通过标签定位到需要查看的服务,并通过关键字过滤出想要查看的日志内容为尽快得到告警邮件这里我们设置带有info字符的日志2.看下过去一段时间内，日志中带有info字段的趋势这里可以看到，日志在不停的打印带有info字段的日志在数据源中添加一个Prometheus类型，并在地址栏中填写上http://loki:3100/loki,这样我们就能通过像查询prometheus一样查询日志的走势这时候再创建一个pannel来查询info日志的趋势就可以得到如下结果：接下来的工作，就是在Grafana上添加一个Alert小铃铛，让它每分钟去Loki里面查询有没有出现info字段的日志出现，如果计算出来的结果大于0，就让Grafana通过邮件告警出来。配置完成后不急保存，我们先Testrule看看rule是否能生效。看到state:&quot;alerting&quot;说明现在已经开始报警了grafana配置邮件[smtp]enabled=truehost=smtp.163.com:25user=你的邮箱#Ifthepasswordcontains#or;youhavetowrapitwithtriplequotes.Ex&quot;&quot;&quot;#password;&quot;&quot;&quot;password=你的密码;cert_file=;key_file=;skip_verify=falsefrom_address=你的邮箱from_name=Grafana修改后，保存退出，重启grafana服务登入grafana页面创建邮件发送规则可以为不同的业务组创建不同的通知通道，例如，运维通道为Ops，研发为Dev。然后可以为一个Metrics指定多个通知通道。Name为通道名称Type为通道类型，此处选Email，也可选择钉钉告警或者AlertManager等告警类型。Default(sendonallalerts)开启后，表示在所有GraphPanel中配置告警规则时默认都会选用此通道。该选项默认为关闭。Includeimage开启后，表示是发送告警图片。DisableResoveMessage开启后，表示发送恢复邮件，默认关闭。Sendreminders开启后，还需设置下方的发送间隔，表示发送告警邮件的间隔，默认关闭表示若某个告警发生后即使持续很长时间也仅发送一次邮件。多个邮箱地址间用分号&quot;;&quot;隔开。例：xxx@163.com;xxx@qq.com创建成功后，点击SendTest按钮，测试邮件是否能发送成功。（如果发送不成功，可在grafana.log中查看日志信息）在告警中添加配置的告警邮件联系人组收到告警邮件也可以发送图片,根据提示安装插件即可安装图片插件官方建议单独新建一个容器去取加载插件而非直接在grafana容器中安装插件version:'2'services:grafana:image:grafana/grafana:7.0.0-beta1ports:-3000environment:GF_RENDERING_SERVER_URL:http://renderer:8081/renderGF_RENDERING_CALLBACK_URL:http://grafana:3000/GF_LOG_FILTERS:rendering:debugrenderer:image:grafana/grafana-image-renderer:2.0.0-beta1ports:-8081environment:ENABLE_METRICS:'true'结果为参考文档：https://grafana.com/blog/2020/05/07/grafana-7.0-preview-new-image-renderer-plugin-to-replace-phantomjs/https://www.jianshu.com/p/0982a8ee204c","link":"https://chriswsq.github.io/post/loki-pei-zhi-you-jian-gao-jing/"},{"title":"轻量级日志系统Loki原理简介和使用","content":"目前公司未采用传统的zabbix对服务器监控，而采用了prometheus。以前监控日志的时候是采用elastalert+logstash+kibana+filebeat+zookeeper+kafka来做日志方面的监控告警。后来了解到有loki这样一个日志系统，上网搜了一番资料做了一些对比日志监控系统elk，lokiELK优势：1、功能丰富，允许复杂的操作（增减字段，调整字段书序，数据json化）劣势：1、主流的ELK（全文检索）或者EFK比较重2、ES复杂的搜索功能很多都用不上规模复杂，资源占用高，操作苦难大多数查询只关注一定时间范围和一些简单的参数（如host、service等）3、Kibana和Grafana之间切换，影响用户体验4、倒排索引的切分和共享的成本较高Loki1、最小化度量和日志的切换成本有助于减少异常事件的响应时间和提高用户的体验2、在查询语言的易操作性和复杂性之间可以达到一个权衡3、更具成本效益项目主页https://github.com/grafana/lokiloki组件介绍Promtail用来将容器日志发送到Loki或者Grafana服务上的日志收集工具该工具主要包括发现采集目标以及给日志流添加上Label标签然后发送给LokiPromtail的服务发现是基于Prometheus的服务发现机制实现的Loki受Prometheus启发的可以水平扩展、高可用以及支持多租户的日志聚合系统使用了和Prometheus相同的服务发现机制，将标签添加到日志流中而不是构建全文索引从Promtail接收到的日志和应用的metrics指标就具有相同的标签集不仅提供了更好的日志和指标之间的上下文切换，还避免了对日志进行全文索引Grafana一个用于监控和可视化观测的开源平台支持非常丰富的数据源在Loki技术栈中它专门用来展示来自Prometheus和Loki等数据源的时间序列数据可进行查询、可视化、报警等操作可以用于创建、探索和共享数据Dashboard鼓励数据驱动安装loki这里我使用docker安装，下面是compose文件version:&quot;3&quot;services:loki:image:&quot;grafana/loki:1.5.0&quot;container_name:&quot;loki&quot;restart:&quot;always&quot;volumes:-&quot;/etc/localtime:/etc/localtime&quot;-&quot;/usr/local/src/loki/:/etc/loki&quot;-&quot;./loki:/loki&quot;ports:-&quot;3100:3100&quot;command:&quot;-config.file=/etc/loki/local-config.yaml&quot;记得要修改loki文件夹的所有者为10001不然会提示权限不足chown-Rf10001:10001loki之后使用docker-compose启动docker-composeup-d就这样loki的服务端就ok了，下面说下几个坑，第一个是^[level=warnts=2020-07-06T06:54:12.754273854Zcaller=client.go:242component=clienthost=192.179.11.1:3100msg=&quot;errorsendingbatch,willretry&quot;status=429error=&quot;serverreturnedHTTPstatus429TooManyRequests(429):Ingestionratelimitexceeded(limit:6291456bytes/sec)whileattemptingtoingest'221'linestotaling'101946'bytes,reducelogvolumeorcontactyourLokiadministratortoseeifthelimitcanbeincreased&quot;当你搭建完成promtail，并且启动发送日志到loki的时候很有可能会碰到这个错误，因为你要收集的日志太多了，超过了loki的限制，所以会报429，如果你要增加限制可以修改loki的配置文件,在limits_config中添加ingestion_rate_mb:15如果你是老版本的loki，那么是添加ingestion_rate:25000详细可以看下面https://github.com/grafana/loki/pull/1278/files/f468d5d258a42316036290fad1b795c40bec22e4#diff-935fd110763ed3367d3ea740a3d3c072还有一个坑是level=errorts=2020-07-06T03:58:02.217480067Zcaller=client.go:247component=clienthost=192.179.11.1:3100msg=&quot;finalerrorsendingbatch&quot;status=400error=&quot;serverreturnedHTTPstatus400BadRequest(400):entryforstream'{app=\\&quot;app_error\\&quot;,filename=\\&quot;/error.log\\&quot;,host=\\&quot;192.179.11.12\\&quot;}'hastimestamptoonew:2020-07-0603:58:01.175699907+0000UTC&quot;这个是两台机器的时间相差太大了，我promtail这台机器的时间没有和ntp服务器同步时间，所以就报了这个错误，只要把时间都同步了就好了我的最终配置文件auth_enabled:falseserver:http_listen_port:3100ingester:lifecycler:address:127.0.0.1ring:kvstore:store:inmemoryreplication_factor:1final_sleep:0schunk_idle_period:5mchunk_retain_period:30sschema_config:configs:-from:2020-10-12store:boltdbobject_store:filesystemschema:v11index:prefix:index_period:168hstorage_config:boltdb:directory:/loki/indexfilesystem:directory:/loki/chunkslimits_config:enforce_metric_name:falsereject_old_samples:truereject_old_samples_max_age:168hingestion_rate_mb:15chunk_store_config:max_look_back_period:0stable_manager:retention_deletes_enabled:trueretention_period:336h安装promtail这里我使用docker安装，下面是compose文件version:&quot;3&quot;services:promtail:image:grafana/promtail:1.5.0container_name:promtailrestart:alwaysvolumes:-$PWD:/etc/promtail-/bsn/xuperchain/output/logs/xchain.log:/bsn/xuperchain/output/logs/xchain.logcommand:-config.file=/etc/promtail/promtail-docker-config.yaml配置文件如下server:http_listen_port:9080grpc_listen_port:0positions:filename:./positions.yamlclients:-url:http://192.168.40.6:3100/loki/api/v1/pushscrape_configs:-job_name:xuperchainstatic_configs:-labels:app:local_xchainhost:192.168.40.7env:prod__path__:/bsn/xuperchain/output/logs/xchain.log之后使用docker-compose启动docker-composeup-d安装grafanaversion:&quot;3&quot;services:grafana:image:grafana/grafana:7.1.5container_name:grafanarestart:alwaysvolumes:-/bsn/prometheus/grafana:/var/lib/grafana-/bsn/prometheus/grafana/grafana.ini:/etc/grafana/grafana.ini-/etc/localtime:/etc/localtimeports:-3000:3000配置文件下载一个grafana的tar包用即可，也可下载镜像后拷出来grafana.ini文件即可访问grafana界面http://192.168.40.6:3000/默认的登陆账号admin/admin然后添加loki数据源url添加http://192.168.40.6:3100点击Exporter选择loki选择相应的labelLoki日志查询语言基本的LogQL查询由两部分组成：logstreamselector、filterexpressionLogstreamselector它由一个或多个键值对组成，每个键是一个日志标签，值的话是标签的值，例如{app=&quot;local_xchain&quot;,host=&quot;192.168.40.7&quot;}在这个例子中，记录具有的标签流app，其值是local_xhcain和的一个标签host，它的值192.168.40.7将被包括在查询结果。注意，这将匹配其标签至少包含192.168.40.7其名称标签的任何日志流；如果有多个包含该标签的流，则所有匹配流的日志将显示在结果中。支持以下标签匹配运算符：=：完全相等。!=：不相等。=~：正则表达式匹配。!~：正则表达式不匹配。适用于Prometheus标签选择器的相同规则也适用于Loki日志流选择器。Filterexpression写入日志流选择器后，可以使用搜索表达式进一步过滤生成的日志集。搜索表达式可以只是文本或正则表达式：{job=“mysql”}|=“error”{name=“kafka”}|~“tsdb-ops.*io:2003”{instance=~“kafka-[23]”,name=“kafka”}!=kafka.server:type=ReplicaManager运算符说明|=：日志行包含字符串。!=：日志行不包含字符串。|~：日志行匹配正则表达式。!~：日志行与正则表达式不匹配。指标查询范围向量LogQL与Prometheus具有相同的范围向量概念，不同之处在于所选的样本范围包括每个日志条目的值1。可以在所选范围内应用聚合，以将其转换为实例向量。注：对于此种查询，需要添加数据源，选择promethes，但是地址为loki的地址，并在最后添加/lok即可当前支持的操作功能为：rate：计算每秒的条目数count_over_time：计算给定范围内每个日志流的条目。//对fluent-bit作业在最近五分钟内的所有日志行进行计数。count_over_time({job=&quot;fluent-bit&quot;}[5m])获取fluent-bit作业在过去十秒内所有非超时错误的每秒速率。rate({job=&quot;fluent-bit&quot;}|=&quot;error&quot;!=&quot;timeout&quot;[10s]集合运算符与PromQL一样，LogQL支持内置聚合运算符的一个子集，可用于聚合单个向量的元素，从而产生具有更少元素但具有集合值的新向量：sum：计算标签上的总和min：选择最少的标签max：选择标签上方的最大值avg：计算标签上的平均值stddev：计算标签上的总体标准差stdvar：计算标签上的总体标准方差count：计算向量中元素的数量bottomk：通过样本值选择最小的k个元素topk：通过样本值选择最大的k个元素可以通过包含awithout或by子句，使用聚合运算符聚合所有标签值或一组不同的标签值：&lt;aggr-op&gt;([parameter,]&lt;vectorexpression&gt;)[without|by(&lt;labellist&gt;)]举例：统计最高日志吞吐量按container排序前十的应用程序topk(10,sum(rate({job=&quot;fluent-bit&quot;}[5m]))by(container))获取最近五分钟内的日志计数，按级别分组sum(count_over_time({job=&quot;fluent-bit&quot;}[5m]))by(level)更多内容请参考：https://github.com/grafana/loki/blob/master/docs/logql.md参考文档：https://blog.csdn.net/weixin_44267608/article/details/105264432https://segmentfault.com/a/1190000023379491https://www.bboy.app/2020/07/08/%E4%BD%BF%E7%94%A8loki%E8%BF%9B%E8%A1%8C%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86/","link":"https://chriswsq.github.io/post/qing-liang-ji-ri-zhi-xi-tong-loki-yuan-li-jian-jie-he-shi-yong/"},{"title":"xuperchian  建链未成功手动修复","content":"从idc服务器日志中找到链名的日志会有创建该链的json文件#手动修复#step1.创建群组./xchain-cliwasminvokegroup_chain--methodaddChain-a'{&quot;bcname&quot;:&quot;CounterChain1&quot;}'#step2.添加节点./xchain-cliwasminvokegroup_chain--methodaddNode-a'{&quot;bcname&quot;:&quot;CounterChain1&quot;,&quot;ip&quot;:&quot;/ip4/127.0.0.1/tcp/47101/p2p/QmVxeNubpg1ZQjQT8W5yZC9fD7ZB1ViArwvyGUB53sqf8e&quot;,#step3.创建平行链./xchain-clitransfer--toCounterChain1--amount100--desccreateCounterChain1.json","link":"https://chriswsq.github.io/post/xuperchian change Common node/"},{"title":"xuperchian 增减共识节点、添加群组操作步骤","content":"加入群组查看当前群组节点./xchain-cliwasmquerygroup_chain--methodlistNode-a'{&quot;bcname&quot;:&quot;app000120200824165811835422&quot;}'节点加入群组./xchain-cliwasminvokegroup_chain--methodaddNode-a'{&quot;bcname&quot;:&quot;app000120200824165811835422&quot;,&quot;ip&quot;:&quot;/ip4/192.168.40.6/tcp/40001/p2p/QmY7UxwcLpD8QK8p6gAk99xuz3Q16jcaCoU8iMokVWYa83&quot;,&quot;address&quot;:&quot;2B1rDQhq7W4TStSHoD88N1SUYXrCDV821v&quot;}'将bcname、ip、address改为实际的参数即可更改共识节点查询金额./xchain-cliaccountbalancezALW2YRp55LFuro1uAjgfoGTqCjgz3nHc--nameapp000120200824165811835422查看块高./xchain-clistatus-H127.0.0.1:37101|jq'.blockchains[]|{&quot;name&quot;:.name,&quot;height&quot;:.ledger.trunkHeight}'|grep-A1app000120200824165811835422提案./xchain-clitransfer--toscocy5ZTaFykhRGGYxN9KSEkpxCD1cd72--descproposal_app000120200824165811835422.json--amount1--nameapp000120200824165811835422ae074dc967e0a03b81346a8e4796b01fe15b51472d3ba34f191d6b08e24a1918投票./xchain-clivoteae074dc967e0a03b81346a8e4796b01fe15b51472d3ba34f191d6b08e24a1918--frozen138160--amount52000000071795360000--nameapp00012020082416581183542274217bf729af8fc1b6acfbc5f953fbda029fb9078360ec301dfc113e690ac016查看交易内容./xchain-clitxquery74217bf729af8fc1b6acfbc5f953fbda029fb9078360ec301dfc113e690ac016查看共识节点./xchain-clitdposstatus--nameapp000120200824165811835422提案内容{&quot;module&quot;:&quot;proposal&quot;,&quot;method&quot;:&quot;Propose&quot;,&quot;args&quot;:{&quot;min_vote_percent&quot;:51,&quot;stop_vote_height&quot;:688970},&quot;trigger&quot;:{&quot;height&quot;:688980,&quot;module&quot;:&quot;consensus&quot;,&quot;method&quot;:&quot;update_consensus&quot;,&quot;args&quot;:{&quot;name&quot;:&quot;tdpos&quot;,&quot;config&quot;:{&quot;version&quot;:&quot;20&quot;,&quot;proposer_num&quot;:&quot;2&quot;,&quot;period&quot;:&quot;5000&quot;,&quot;alternate_interval&quot;:&quot;5000&quot;,&quot;term_interval&quot;:&quot;10000&quot;,&quot;block_num&quot;:&quot;720&quot;,&quot;vote_unit_price&quot;:&quot;1&quot;,&quot;init_proposer&quot;:{&quot;1&quot;:[&quot;scocy5ZTaFykhRGGYxN9KSEkpxCD1cd72&quot;,&quot;qaXhH7gJcdfpapmWkbHdLNqUFq3Vst6Am&quot;]}}}}}注意：以上将address、p2p、proposer_num等改为实际数据即可命令添加群组，一次性添加两个以上群组不生效,须一个一个添加用命令行创建平行链，如果命令写错不报命令的错误，而会报其他的问题例如地址不在白名单后续加入群组的节点，需要重启才会同步块数据在进行提案的时候经常会导致xuper链或其他调整的链快高不一致的情况，这时候重启解决问题如果同一个链既要增加节点又要删减节点，那么是需要分开提案来做的newpJsfQecriScf4ZA6MHhjEpMMAiBbct5Sv24rqLhCMozBJrmsXhtrR68wLAr72zDfuELqJV7qfGdf2GAZUcrx6v71ahQ69nYYWGpahuainan2app00012020073020564807516812A8cTP6dFjKoZPXyCx7T5APbQKCUzbgBsxpecX9eVDd368J3GBfdPNtVUgnTu4nvD7b28HsYtaS1p7DqZ3QqXzCSsEk47dX4seU6oapp00012020073100005906175832A8cTP6dFjKoZPXyCx7T5APbQKCUzbgBsx27xoBigJ6HRqtqQtohvaoMyBt55uECtNbVsMbiWCdbiAHn84p77zjepAzGn3F7ztzADapp00012020080113314154970002A8cTP6dFjKoZPXyCx7T5APbQKCUzbgBsxpecX9eVDd368J3GBfdPNtVUgnTu4nvD7bqaXhH7gJcdfpapmWkbHdLNqUFq3Vst6Am","link":"https://chriswsq.github.io/post/xuperchian-change-Common-node/"},{"title":"docker容器保持后台运行的两种方式","content":"有的服务支持服务后台运行，那么在用docker启动服务时可以使用前台运行的命令，除此之外一些服务器是不支持前台运行，那么就要靠其他方式使用进程挂在前台。方法1，run一个容器：[root@localhost~]#dockerrun-dit--hostnamecentos--namecentos--restartalwaysa8493f5f50ff/bin/bash-dit是后台运行、交互模式、分配终端，容器启动后不会退出如果没有it参数，run一个容器以后，dockerps-a容器状态就exited了--restartalways容器可以随docker服务启动而启动方法2，run一个容器，并一直发送ping包[root@localhost~]#dockerrun-d--nametest--hostnametesta8493f5f50ffping127.0.0.1这样容器就一直在发送ping包，不会退出。意思就是然容器一直运行一个程序，不退出，容器就不会停止本文链接地址https://www.rootop.org/pages/3736.html","link":"https://chriswsq.github.io/post/docker-rong-qi-bao-chi-hou-tai-yun-xing-de-liang-chong-fang-shi/"},{"title":"记一次服务器内存过高问题","content":"监控系统发邮件告警，一台服务器内存过高，已达90%以上，之前这台机器内存就高到85%以上，上去看了下服务正常top指令看了下没什么太高的进程占用，就重启了下上面的服务（前提是上面的服务重启是没什么影响的）。今天有来告警了，内存占用率已经到90%，照常上去看了下使用free-m查看内存，一共是16G已经用了13Gtop查看进程占用资源情况，发现最高的应用占用也不过百分之五左右怀疑是被攻击了，看了下进程psaux发现进程很多psaux|wc-l进程都6000多了，大部分的进程都是sshd:root@notty的进程，网上查了下这是被ssh暴力破解了，很奇怪，端口号改了不是22了，攻击者怎么知道的呢。。。。先解决问题吧foriin`psaux|greproot@notty|more|awk'{print$2}'`;dokill$i;done","link":"https://chriswsq.github.io/post/ji-yi-ci-fu-wu-qi-nei-cun-guo-gao-wen-ti/"},{"title":"docker部署Prometheus监控服务器及容器并发送告警","content":"记录一下利用prometheus监控服务器信息和容器服务并发送邮件告警基本原理Prometheus的基本原理是通过HTTP协议周期性抓取被监控组件的状态，任意组件只要提供对应的HTTP接口就可以接入监控。不需要任何SDK或者其他的集成过程。这样做非常适合做虚拟化环境监控系统，比如VM、Docker、Kubernetes等。输出被监控组件信息的HTTP接口被叫做exporter。目前互联网公司常用的组件大部分都有exporter可以直接使用，比如Varnish、Haproxy、Nginx、MySQL、Linux系统信息(包括磁盘、内存、CPU、网络等等)。相关组件Prometheus:PrometheusDaemon负责定时去目标上抓取metrics(指标)数据，每个抓取目标需要暴露一个http服务的接口给它定时抓取。Grafana:接入prometheus数据，图形化展示监控信息Node-exporter:负责收集host硬件和操作系统数据。它将以容器方式运行在所有host上。Cadvisor:负责收集容器数据。它将以容器方式运行在所有host上。Alertmanager:警告管理器，用来进行报警。主机名ip服务test-1192.168.40.6cadvisor、node-exporter、grafana、prometheus、alertmanager.ymltest-2192.168.40.7node-exporter、cadvisortest-3192.168.40.8node-exporter、cadvisor安装docker、docker-compose安装docker#安装依赖包yuminstall-yyum-utilsdevice-mapper-persistent-datalvm2#添加Docker软件包源yum-config-manager--add-repohttps://download.docker.com/linux/centos/docker-ce.repo#安装DockerCEyuminstalldocker-ce-y#启动systemctlstartdocker#开机启动systemctlenabledocker#查看Docker信息dockerinfo安装docker-composecurl-Lhttps://github.com/docker/compose/releases/download/1.23.2/docker-compose-`uname-s`-`uname-m`-o/usr/local/bin/docker-composechmod+x/usr/local/bin/docker-compose添加配置文件mkdir-p/usr/local/src/configcd/usr/local/src/config添加prometheus.yml配置文件vimprometheus.yml#myglobalconfigglobal:scrape_interval:15sevaluation_interval:15salerting:alertmanagers:-static_configs:-targets:-192.168.40.6:9093rule_files:-&quot;/etc/prometheus/config/rule/*rule.yml&quot;scrape_configs:-job_name:'prometheus'scrape_interval:5sstatic_configs:-targets:['192.168.40.6:9090']-job_name:'cadvisor'scrape_interval:5sstatic_configs:-targets:['192.168.40.6:8080','192.168.40.7:8080','192.168.40.8']-job_name:'node-exporter'scrape_interval:5sstatic_configs:-targets:['192.168.40.6:9100']labels:instance:test-1-192.168.40.6service:node-service-targets:['192.168.40.7:9100']labels:instance:test-2-192.168.40.7service:node-service-targets:['192.168.40.8:9100']labels:instance:test-3-192.168.40.8service:node-service添加邮件告警配置文件添加配置文件alertmanager.yml，配置收发邮件邮箱vimalertmanager.ymlglobal:#ThesmarthostandSMTPsenderusedformailnotifications.用于邮件通知的智能主机和SMTP发件人。smtp_smarthost:'smtp.163.com:25'smtp_from:'xxxxxxxxx@163.com'smtp_auth_username:'xxxxxxxxxxxxxxx@163.com'smtp_auth_password:'xxxxxxxxxxxxx'#TheauthtokenforHipchat.Hipchat的身份验证令牌。templates:-'/etc/alertmanager/default-monitor.tmpl'route:group_by:['alertname']group_wait:10sgroup_interval:10srepeat_interval:5mreceiver:'mail'receivers:-name:'mail'email_configs:-to:'xxxxxxxxxxxxxxxx@163.com,xxxxxxxxxxxxxx@qq.com'send_resolved:true#告警恢复通知html:'{{template&quot;default-monitor.html&quot;.}}'#应用那个模板headers:{Subject:&quot;[WARN]报警邮件&quot;}#邮件主题信息如果不写headers也可以在模板中定义默认加载email.default.subject这个模板添加报警规则mkdir-p/usr/local/src/config/rulecd/usr/local/src/config/rule创建两个文件node-exporter-record-rule.ymlnode-exporter-alert-rule.yml第一个文件用于记录规则，第二个是报警规则。由于之前我们在prometheus.yml中已经引用了所有已rule结尾的文件，所以我们不用在修改prometheus.yml配置文件。创建node-exporter-record-rule.ymlgroups:-name:node-exporter-recordrules:-expr:up{job=~&quot;node-exporter&quot;}record:node_exporter:uplabels:desc:&quot;节点是否在线,在线1,不在线0&quot;unit:&quot;&quot;job:&quot;node-exporter&quot;-expr:time()-node_boot_time_seconds{}record:node_exporter:node_uptimelabels:desc:&quot;节点的运行时间&quot;unit:&quot;s&quot;job:&quot;node-exporter&quot;###############################################################################################cpu#-expr:(1-avgby(environment,instance)(irate(node_cpu_seconds_total{job=&quot;node-exporter&quot;,mode=&quot;idle&quot;}[5m])))*100record:node_exporter:cpu:total:percentlabels:desc:&quot;节点的cpu总消耗百分比&quot;unit:&quot;%&quot;job:&quot;node-exporter&quot;-expr:(avgby(environment,instance)(irate(node_cpu_seconds_total{job=&quot;node-exporter&quot;,mode=&quot;idle&quot;}[5m])))*100record:node_exporter:cpu:idle:percentlabels:desc:&quot;节点的cpuidle百分比&quot;unit:&quot;%&quot;job:&quot;node-exporter&quot;-expr:(avgby(environment,instance)(irate(node_cpu_seconds_total{job=&quot;node-exporter&quot;,mode=&quot;iowait&quot;}[5m])))*100record:node_exporter:cpu:iowait:percentlabels:desc:&quot;节点的cpuiowait百分比&quot;unit:&quot;%&quot;job:&quot;node-exporter&quot;-expr:(avgby(environment,instance)(irate(node_cpu_seconds_total{job=&quot;node-exporter&quot;,mode=&quot;system&quot;}[5m])))*100record:node_exporter:cpu:system:percentlabels:desc:&quot;节点的cpusystem百分比&quot;unit:&quot;%&quot;job:&quot;node-exporter&quot;-expr:(avgby(environment,instance)(irate(node_cpu_seconds_total{job=&quot;node-exporter&quot;,mode=&quot;user&quot;}[5m])))*100record:node_exporter:cpu:user:percentlabels:desc:&quot;节点的cpuuser百分比&quot;unit:&quot;%&quot;job:&quot;node-exporter&quot;-expr:(avgby(environment,instance)(irate(node_cpu_seconds_total{job=&quot;node-exporter&quot;,mode=~&quot;softirq|nice|irq|steal&quot;}[5m])))*100record:node_exporter:cpu:other:percentlabels:desc:&quot;节点的cpu其他的百分比&quot;unit:&quot;%&quot;job:&quot;node-exporter&quot;#############################################################################################################################################################################################memory#-expr:node_memory_MemTotal_bytes{job=&quot;node-exporter&quot;}record:node_exporter:memory:totallabels:desc:&quot;节点的内存总量&quot;unit:bytejob:&quot;node-exporter&quot;-expr:node_memory_MemFree_bytes{job=&quot;node-exporter&quot;}record:node_exporter:memory:freelabels:desc:&quot;节点的剩余内存量&quot;unit:bytejob:&quot;node-exporter&quot;-expr:node_memory_MemTotal_bytes{job=&quot;node-exporter&quot;}-node_memory_MemFree_bytes{job=&quot;node-exporter&quot;}record:node_exporter:memory:usedlabels:desc:&quot;节点的已使用内存量&quot;unit:bytejob:&quot;node-exporter&quot;-expr:node_memory_MemTotal_bytes{job=&quot;node-exporter&quot;}-node_memory_MemAvailable_bytes{job=&quot;node-exporter&quot;}record:node_exporter:memory:actualusedlabels:desc:&quot;节点用户实际使用的内存量&quot;unit:bytejob:&quot;node-exporter&quot;-expr:(1-(node_memory_MemAvailable_bytes{job=&quot;node-exporter&quot;}/(node_memory_MemTotal_bytes{job=&quot;node-exporter&quot;})))*100record:node_exporter:memory:used:percentlabels:desc:&quot;节点的内存使用百分比&quot;unit:&quot;%&quot;job:&quot;node-exporter&quot;-expr:((node_memory_MemAvailable_bytes{job=&quot;node-exporter&quot;}/(node_memory_MemTotal_bytes{job=&quot;node-exporter&quot;})))*100record:node_exporter:memory:free:percentlabels:desc:&quot;节点的内存剩余百分比&quot;unit:&quot;%&quot;job:&quot;node-exporter&quot;###############################################################################################load#-expr:sumby(instance)(node_load1{job=&quot;node-exporter&quot;})record:node_exporter:load:load1labels:desc:&quot;系统1分钟负载&quot;unit:&quot;&quot;job:&quot;node-exporter&quot;-expr:sumby(instance)(node_load5{job=&quot;node-exporter&quot;})record:node_exporter:load:load5labels:desc:&quot;系统5分钟负载&quot;unit:&quot;&quot;job:&quot;node-exporter&quot;-expr:sumby(instance)(node_load15{job=&quot;node-exporter&quot;})record:node_exporter:load:load15labels:desc:&quot;系统15分钟负载&quot;unit:&quot;&quot;job:&quot;node-exporter&quot;###############################################################################################disk#-expr:node_filesystem_size_bytes{job=&quot;node-exporter&quot;,fstype=~&quot;ext4|xfs&quot;}record:node_exporter:disk:usage:totallabels:desc:&quot;节点的磁盘总量&quot;unit:bytejob:&quot;node-exporter&quot;-expr:node_filesystem_avail_bytes{job=&quot;node-exporter&quot;,fstype=~&quot;ext4|xfs&quot;}record:node_exporter:disk:usage:freelabels:desc:&quot;节点的磁盘剩余空间&quot;unit:bytejob:&quot;node-exporter&quot;-expr:node_filesystem_size_bytes{job=&quot;node-exporter&quot;,fstype=~&quot;ext4|xfs&quot;}-node_filesystem_avail_bytes{job=&quot;node-exporter&quot;,fstype=~&quot;ext4|xfs&quot;}record:node_exporter:disk:usage:usedlabels:desc:&quot;节点的磁盘使用的空间&quot;unit:bytejob:&quot;node-exporter&quot;-expr:(1-node_filesystem_avail_bytes{job=&quot;node-exporter&quot;,fstype=~&quot;ext4|xfs&quot;}/node_filesystem_size_bytes{job=&quot;node-exporter&quot;,fstype=~&quot;ext4|xfs&quot;})*100record:node_exporter:disk:used:percentlabels:desc:&quot;节点的磁盘的使用百分比&quot;unit:&quot;%&quot;job:&quot;node-exporter&quot;-expr:irate(node_disk_reads_completed_total{job=&quot;node-exporter&quot;}[1m])record:node_exporter:disk:read:count:ratelabels:desc:&quot;节点的磁盘读取速率&quot;unit:&quot;次/秒&quot;job:&quot;node-exporter&quot;-expr:irate(node_disk_writes_completed_total{job=&quot;node-exporter&quot;}[1m])record:node_exporter:disk:write:count:ratelabels:desc:&quot;节点的磁盘写入速率&quot;unit:&quot;次/秒&quot;job:&quot;node-exporter&quot;-expr:(irate(node_disk_written_bytes_total{job=&quot;node-exporter&quot;}[1m]))/1024/1024record:node_exporter:disk:read:mb:ratelabels:desc:&quot;节点的设备读取MB速率&quot;unit:&quot;MB/s&quot;job:&quot;node-exporter&quot;-expr:(irate(node_disk_read_bytes_total{job=&quot;node-exporter&quot;}[1m]))/1024/1024record:node_exporter:disk:write:mb:ratelabels:desc:&quot;节点的设备写入MB速率&quot;unit:&quot;MB/s&quot;job:&quot;node-exporter&quot;###############################################################################################filesystem#-expr:(1-node_filesystem_files_free{job=&quot;node-exporter&quot;,fstype=~&quot;ext4|xfs&quot;}/node_filesystem_files{job=&quot;node-exporter&quot;,fstype=~&quot;ext4|xfs&quot;})*100record:node_exporter:filesystem:used:percentlabels:desc:&quot;节点的inode的剩余可用的百分比&quot;unit:&quot;%&quot;job:&quot;node-exporter&quot;##############################################################################################filefd#-expr:node_filefd_allocated{job=&quot;node-exporter&quot;}record:node_exporter:filefd_allocated:countlabels:desc:&quot;节点的文件描述符打开个数&quot;unit:&quot;%&quot;job:&quot;node-exporter&quot;-expr:node_filefd_allocated{job=&quot;node-exporter&quot;}/node_filefd_maximum{job=&quot;node-exporter&quot;}*100record:node_exporter:filefd_allocated:percentlabels:desc:&quot;节点的文件描述符打开百分比&quot;unit:&quot;%&quot;job:&quot;node-exporter&quot;##############################################################################################network#-expr:avgby(environment,instance,device)(irate(node_network_receive_bytes_total{device=~&quot;eth0|eth1|ens33|ens37&quot;}[1m]))record:node_exporter:network:netin:bit:ratelabels:desc:&quot;节点网卡eth0每秒接收的比特数&quot;unit:&quot;bit/s&quot;job:&quot;node-exporter&quot;-expr:avgby(environment,instance,device)(irate(node_network_transmit_bytes_total{device=~&quot;eth0|eth1|ens33|ens37&quot;}[1m]))record:node_exporter:network:netout:bit:ratelabels:desc:&quot;节点网卡eth0每秒发送的比特数&quot;unit:&quot;bit/s&quot;job:&quot;node-exporter&quot;-expr:avgby(environment,instance,device)(irate(node_network_receive_packets_total{device=~&quot;eth0|eth1|ens33|ens37&quot;}[1m]))record:node_exporter:network:netin:packet:ratelabels:desc:&quot;节点网卡每秒接收的数据包个数&quot;unit:&quot;个/秒&quot;job:&quot;node-exporter&quot;-expr:avgby(environment,instance,device)(irate(node_network_transmit_packets_total{device=~&quot;eth0|eth1|ens33|ens37&quot;}[1m]))record:node_exporter:network:netout:packet:ratelabels:desc:&quot;节点网卡发送的数据包个数&quot;unit:&quot;个/秒&quot;job:&quot;node-exporter&quot;-expr:avgby(environment,instance,device)(irate(node_network_receive_errs_total{device=~&quot;eth0|eth1|ens33|ens37&quot;}[1m]))record:node_exporter:network:netin:error:ratelabels:desc:&quot;节点设备驱动器检测到的接收错误包的数量&quot;unit:&quot;个/秒&quot;job:&quot;node-exporter&quot;-expr:avgby(environment,instance,device)(irate(node_network_transmit_errs_total{device=~&quot;eth0|eth1|ens33|ens37&quot;}[1m]))record:node_exporter:network:netout:error:ratelabels:desc:&quot;节点设备驱动器检测到的发送错误包的数量&quot;unit:&quot;个/秒&quot;job:&quot;node-exporter&quot;-expr:node_tcp_connection_states{job=&quot;node-exporter&quot;,state=&quot;established&quot;}record:node_exporter:network:tcp:established:countlabels:desc:&quot;节点当前established的个数&quot;unit:&quot;个&quot;job:&quot;node-exporter&quot;-expr:node_tcp_connection_states{job=&quot;node-exporter&quot;,state=&quot;time_wait&quot;}record:node_exporter:network:tcp:timewait:countlabels:desc:&quot;节点timewait的连接数&quot;unit:&quot;个&quot;job:&quot;node-exporter&quot;-expr:sumby(environment,instance)(node_tcp_connection_states{job=&quot;node-exporter&quot;})record:node_exporter:network:tcp:total:countlabels:desc:&quot;节点tcp连接总数&quot;unit:&quot;个&quot;job:&quot;node-exporter&quot;##############################################################################################process#-expr:node_processes_state{state=&quot;Z&quot;}record:node_exporter:process:zoom:total:countlabels:desc:&quot;节点当前状态为zoom的个数&quot;unit:&quot;个&quot;job:&quot;node-exporter&quot;##############################################################################################other#-expr:abs(node_timex_offset_seconds{job=&quot;node-exporter&quot;})record:node_exporter:time:offsetlabels:desc:&quot;节点的时间偏差&quot;unit:&quot;s&quot;job:&quot;node-exporter&quot;#############################################################################################-expr:countby(instance)(countby(instance,cpu)(node_cpu_seconds_total{mode='system'}))record:node_exporter:cpu:count创建node-exporter-alert-rule.ymlgroups:-name:node-exporter-alertrules:-alert:node-exporter-downexpr:node_exporter:up==0for:1mlabels:severity:'critical'annotations:summary:&quot;instance:{{$labels.instance}}宕机了&quot;description:&quot;instance:{{$labels.instance}}\\n-job:{{$labels.job}}关机了，时间已经1分钟了。&quot;value:&quot;{{$value}}&quot;instance:&quot;{{$labels.instance}}&quot;-alert:node-exporter-cpu-highexpr:node_exporter:cpu:total:percent&gt;80for:3mlabels:severity:infoannotations:summary:&quot;instance:{{$labels.instance}}cpu使用率高于{{$value}}&quot;description:&quot;instance:{{$labels.instance}}\\n-job:{{$labels.job}}CPU使用率已经持续三分钟高过80%。&quot;value:&quot;{{$value}}&quot;instance:&quot;{{$labels.instance}}&quot;-alert:node-exporter-cpu-iowait-highexpr:node_exporter:cpu:iowait:percent&gt;=12for:3mlabels:severity:infoannotations:summary:&quot;instance:{{$labels.instance}}cpuiowait使用率高于{{$value}}&quot;description:&quot;instance:{{$labels.instance}}\\n-job:{{$labels.job}}cpuiowait使用率已经持续三分钟高过12%&quot;value:&quot;{{$value}}&quot;instance:&quot;{{$labels.instance}}&quot;-alert:node-exporter-load-load1-highexpr:(node_exporter:load:load1)&gt;(node_exporter:cpu:count)*1.2for:3mlabels:severity:infoannotations:summary:&quot;instance:{{$labels.instance}}load1使用率高于{{$value}}&quot;description:&quot;&quot;value:&quot;{{$value}}&quot;instance:&quot;{{$labels.instance}}&quot;-alert:node-exporter-memory-highexpr:node_exporter:memory:used:percent&gt;85for:3mlabels:severity:infoannotations:summary:&quot;instance:{{$labels.instance}}memory使用率高于{{$value}}&quot;description:&quot;&quot;value:&quot;{{$value}}&quot;instance:&quot;{{$labels.instance}}&quot;-alert:node-exporter-disk-highexpr:node_exporter:disk:used:percent&gt;88for:10mlabels:severity:infoannotations:summary:&quot;instance:{{$labels.instance}}disk使用率高于{{$value}}&quot;description:&quot;&quot;value:&quot;{{$value}}&quot;instance:&quot;{{$labels.instance}}&quot;-alert:node-exporter-disk-read:count-highexpr:node_exporter:disk:read:count:rate&gt;3000for:2mlabels:severity:infoannotations:summary:&quot;instance:{{$labels.instance}}iopsread使用率高于{{$value}}&quot;description:&quot;&quot;value:&quot;{{$value}}&quot;instance:&quot;{{$labels.instance}}&quot;-alert:node-exporter-disk-write-count-highexpr:node_exporter:disk:write:count:rate&gt;3000for:2mlabels:severity:infoannotations:summary:&quot;instance:{{$labels.instance}}iopswrite使用率高于{{$value}}&quot;description:&quot;&quot;value:&quot;{{$value}}&quot;instance:&quot;{{$labels.instance}}&quot;-alert:node-exporter-disk-read-mb-highexpr:node_exporter:disk:read:mb:rate&gt;60for:2mlabels:severity:infoannotations:summary:&quot;instance:{{$labels.instance}}读取字节数高于{{$value}}&quot;description:&quot;&quot;instance:&quot;{{$labels.instance}}&quot;value:&quot;{{$value}}&quot;-alert:node-exporter-disk-write-mb-highexpr:node_exporter:disk:write:mb:rate&gt;60for:2mlabels:severity:infoannotations:summary:&quot;instance:{{$labels.instance}}写入字节数高于{{$value}}&quot;description:&quot;&quot;value:&quot;{{$value}}&quot;instance:&quot;{{$labels.instance}}&quot;-alert:node-exporter-filefd-allocated-percent-highexpr:node_exporter:filefd_allocated:percent&gt;80for:10mlabels:severity:infoannotations:summary:&quot;instance:{{$labels.instance}}打开文件描述符高于{{$value}}&quot;description:&quot;&quot;value:&quot;{{$value}}&quot;instance:&quot;{{$labels.instance}}&quot;-alert:node-exporter-network-netin-error-rate-highexpr:node_exporter:network:netin:error:rate&gt;4for:1mlabels:severity:infoannotations:summary:&quot;instance:{{$labels.instance}}包进入的错误速率高于{{$value}}&quot;description:&quot;&quot;value:&quot;{{$value}}&quot;instance:&quot;{{$labels.instance}}&quot;-alert:node-exporter-network-netin-packet-rate-highexpr:node_exporter:network:netin:packet:rate&gt;35000for:1mlabels:severity:infoannotations:summary:&quot;instance:{{$labels.instance}}包进入速率高于{{$value}}&quot;description:&quot;&quot;value:&quot;{{$value}}&quot;instance:&quot;{{$labels.instance}}&quot;-alert:node-exporter-network-netout-packet-rate-highexpr:node_exporter:network:netout:packet:rate&gt;35000for:1mlabels:severity:infoannotations:summary:&quot;instance:{{$labels.instance}}包流出速率高于{{$value}}&quot;description:&quot;&quot;value:&quot;{{$value}}&quot;instance:&quot;{{$labels.instance}}&quot;-alert:node-exporter-network-tcp-total-count-highexpr:node_exporter:network:tcp:total:count&gt;40000for:1mlabels:severity:infoannotations:summary:&quot;instance:{{$labels.instance}}tcp连接数量高于{{$value}}&quot;description:&quot;&quot;value:&quot;{{$value}}&quot;instance:&quot;{{$labels.instance}}&quot;-alert:node-exporter-process-zoom-total-count-highexpr:node_exporter:process:zoom:total:count&gt;10for:10mlabels:severity:infoannotations:summary:&quot;instance:{{$labels.instance}}僵死进程数量高于{{$value}}&quot;description:&quot;&quot;value:&quot;{{$value}}&quot;instance:&quot;{{$labels.instance}}&quot;-alert:node-exporter-time-offset-highexpr:node_exporter:time:offset&gt;0.03for:2mlabels:severity:infoannotations:summary:&quot;instance:{{$labels.instance}}{{$labels.desc}}{{$value}}{{$labels.unit}}&quot;description:&quot;&quot;value:&quot;{{$value}}&quot;instance:&quot;{{$labels.instance}}&quot;添加告警模板mkdirtemplatevimtemplate/default-monitor.tmpl{{define&quot;default-monitor.html&quot;}}{{range.Alerts}}=========start==========&lt;br&gt;告警程序:prometheus_alert&lt;br&gt;告警级别:{{.Labels.severity}}级&lt;br&gt;告警类型:{{.Labels.alertname}}&lt;br&gt;故障主机:{{.Labels.instance}}&lt;br&gt;告警主题:{{.Annotations.summary}}&lt;br&gt;告警详情:{{.Annotations.description}}&lt;br&gt;触发时间:{{.StartsAt.Format&quot;2019-08-0416:58:15&quot;}}&lt;br&gt;=========end==========&lt;br&gt;{{end}}{{end}}编写docker-compose文件vimdocker-compose-monitor.ymlversion:'2'networks:monitor:driver:bridgeservices:prometheus:image:prom/prometheus:v2.16.0container_name:prometheusrestart:alwaysports:-9090:9090volumes:-/bsn/prometheus/prometheus:/prometheus-/bsn/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml-/bsn/prometheus/alert/alert.rules:/usr/local/prometheus/rules/alert.rules-/etc/localtime:/etc/localtimecommand:-'--config.file=/etc/prometheus/prometheus.yml'-'--storage.tsdb.path=/prometheus/'-'--storage.tsdb.retention.time=90d'depends_on:-alertmanagergrafana:image:grafana/grafana:6.4.2container_name:grafanarestart:alwaysvolumes:-/bsn/prometheus/grafana:/var/lib/grafana-/bsn/prometheus/grafana/grafana.ini:/etc/grafana/grafana.ini-/etc/localtime:/etc/localtimeports:-3000:3000depends_on:-prometheusalertmanager:image:prom/alertmanager:v0.21.0-rc.0container_name:alertmanagervolumes:-/bsn/prometheus/alert/alertmanager.yml:/etc/alertmanager/alertmanager.yml-/bsn/prometheus/alert/email.tmpl:/etc/alertmanager/template/email.tmpl-/etc/localtime:/etc/localtimecommand:-'--config.file=/etc/alertmanager/alertmanager.yml'ports:-9093:9093restart:alwaysnode-exporter:image:quay.io/prometheus/node-exportercontainer_name:node-exporterhostname:$HOSTNAMErestart:alwaysports:-&quot;9100:9100&quot;volumes:-/usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro-/proc:/host/proc:ro-/sys:/host/sys:ro-/:/rootfs:rorestart:alwayscommand:-'--path.procfs=/host/proc'-'--path.sysfs=/host/sys'-'--path.rootfs=/rootfs'cadvisor:image:google/cadvisor:latestcontainer_name:cadvisorhostname:cadvisorrestart:alwaysvolumes:-/:/rootfs:ro-/var/run:/var/run:rw-/sys:/sys:ro-/var/lib/docker/:/var/lib/docker:roports:-&quot;8080:8080&quot;启动docker-compose#启动容器：docker-compose-f/usr/local/src/config/docker-compose-monitor.ymlup-d#删除容器：docker-compose-f/usr/local/src/config/docker-compose-monitor.ymldown#重启容器：dockerrestartid在其他节点分别启动cadvisor和node-exporter容器version:'3'services:node-exporter:image:quay.io/prometheus/node-exportercontainer_name:node-exporterhostname:$HOSTNAMErestart:alwaysports:-&quot;9100:9100&quot;volumes:-/usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro-/proc:/host/proc:ro-/sys:/host/sys:ro-/:/rootfs:rorestart:alwayscommand:-'--path.procfs=/host/proc'-'--path.sysfs=/host/sys'-'--path.rootfs=/rootfs'cadvisor:image:google/cadvisor:latestcontainer_name:cadvisorhostname:cadvisorrestart:alwaysvolumes:-/:/rootfs:ro-/var/run:/var/run:rw-/sys:/sys:ro-/var/lib/docker/:/var/lib/docker:roports:-&quot;8080:8080&quot;容器启动如下：prometheustargets界面如下：备注：如果State为Down，应该是防火墙问题，参考下面防火墙配置。prometheustargets界面如下：备注：如果没有数据，同步下时间。配置grafana添加Prometheus数据源配置dashboards说明：可以用自带模板，也可以去https://grafana.com/dashboards，下载对应的模板。添加监控服务器模板此处用的模板id是8919也可使用（1860）添加监控容器模板此处用893也可用（8321）也可用综合的9276告警停止192.168.40.7的cadvisor和node-exporter容器收到告警邮件注：如果日期有问题则将告警模板的触发时间参数改为{{.StartsAt.Format&quot;2019-08-0416:58:15&quot;}}&lt;br&gt;即可。参考博客：https://juejin.im/post/6844903809517371406https://blog.csdn.net/w342164796/article/details/105079231/https://blog.csdn.net/aixiaoyang168/article/details/98474494","link":"https://chriswsq.github.io/post/docker-bu-shu-prometheus-jian-kong-fu-wu-qi-ji-rong-qi-bing-fa-song-gao-jing/"},{"title":"k8s 如何关联pvc到特定的pv?","content":"部署有状态的应用时需要挂载相应的配置文件，了解下最常用的pv（nfs）和pvc绑定关系首先肯定要在放置配置文件的地方配置nfs服务如何关联pvc到特定的pv?我们可以使用对pv打label的方式，具体如下：创建pv，指定labelcatnfs-test-pv.yamlapiVersion:v1kind:PersistentVolumemetadata:name:nfs-test-pv#namespace:test-pv-testlabels:pv:nfs-test-pvspec:capacity:storage:100MiaccessModes:-ReadWriteManynfs:#FIXME:usetherightIPserver:192.168.40.6path:&quot;/test/&quot;然后创建pvc，使用matchLabel来关联刚创建的pv:nfs-test-pvcatnfs-test-pvc.yamlapiVersion:v1kind:PersistentVolumeClaimmetadata:name:nfs-test-pvc#namespace:test-pv-testspec:accessModes:-ReadWriteManystorageClassName:&quot;&quot;resources:requests:storage:90Miselector:matchLabels:pv:nfs-test-pv下面开始测试：创建pvkubectlapply-fnfs-test-pv.yaml查看pvkubectlgetpv[root@test-1~]#kubectlgetpvNAMECAPACITYACCESSMODESRECLAIMPOLICYSTATUSCLAIMSTORAGECLASSREASONAGEca-service100MiRWXRetainBounddefault/ca-service-pvc30m然后创建pvckubectlapply-fnfs-test-pvc.yaml查看pvc[root@test-1~]#kubectlgetpvcNAMESTATUSVOLUMECAPACITYACCESSMODESSTORAGECLASSAGEca-service-pvcBoundca-service100MiRWX30m已都正确绑定","link":"https://chriswsq.github.io/post/k8s-ru-he-guan-lian-pvc-dao-te-ding-de-pv/"},{"title":"k8s pv,pvc无法删除问题","content":"解决k8spv,pvc无法删除问题一般删除步骤为：先删pod再删pvc最后删pv但是遇到pv始终处于“Terminating”状态，而且delete不掉。如下：[root@test-1pv]#kubectlgetpvNAMECAPACITYACCESSMODESRECLAIMPOLICYSTATUSCLAIMSTORAGECLASSREASONAGEca-service100MiRWXRetainTerminatingdefault/ca-service-pvc17h解决方法：直接删除k8s中的记录：kubectlpatchpvca-service-p'{&quot;metadata&quot;:{&quot;finalizers&quot;:null}}'","link":"https://chriswsq.github.io/post/k8s-pvpvc-wu-fa-shan-chu-wen-ti/"},{"title":"kubeadm部署k8s","content":"部署环境主机名centos版本IPdocker-versionflanel-versionkeepalived-version主机配置k8s-master17.4192.168.40.618.09.9v0.11.0v1.3.52C4Gk8s-node17.4192.168.40.718.09.9v0.11.0v1.3.52C4Gk8s-node27.4192.168.40.818.09.9v0.11.0v1.3.52C4G二、高可用架构注：此图为负载为loadbanacer为云上的负载方案在此就说下本人在这里犯的小错误1.拉镜像时各个节点都要拉取2.部署flannel时因为网络的原因可以先down下载，在执行kubectlapply-fkube-flannel.ymlwgethttps://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml3.部署后会发现主节点Ready而从节点都是NotReady，这是因为master节点的这个文件/etc/cni/net.d/10-flannel.conflist是因为安装flannel生成的，而node节点是通过master分配生成的所以少一个参数&quot;cniVersion&quot;:&quot;0.3.1&quot;,直接把maser推过去到node节点，或者手动改下就可以了kubeadm部署kubeadmjoin192.168.154.6:6443--tokenhum394.uf5ehv9ye6w661bz--discovery-token-ca-cert-hashsha256:6bd3388a191ae0afb2ca86102e655a2ced4c6e6b6bef82afebbdef8154a57b61node{&quot;name&quot;:&quot;cbr0&quot;,&quot;plugins&quot;:[{&quot;type&quot;:&quot;flannel&quot;,&quot;delegate&quot;:{&quot;hairpinMode&quot;:true,&quot;isDefaultGateway&quot;:true}},{&quot;type&quot;:&quot;portmap&quot;,&quot;capabilities&quot;:{&quot;portMappings&quot;:true}}]}master{&quot;name&quot;:&quot;cbr0&quot;,&quot;cniVersion&quot;:&quot;0.3.1&quot;,&quot;plugins&quot;:[{&quot;type&quot;:&quot;flannel&quot;,&quot;delegate&quot;:{&quot;hairpinMode&quot;:true,&quot;isDefaultGateway&quot;:true}},{&quot;type&quot;:&quot;portmap&quot;,&quot;capabilities&quot;:{&quot;portMappings&quot;:true}}]}参考https://www.kubernetes.org.cn/6632.html","link":"https://chriswsq.github.io/post/kubeadm-bu-shu-k8s/"},{"title":"logstash配置","content":"补上一份自己在工作中用的一份logstash的配置文件，万变不离其宗，可以参看此内容这一份匹配的日志是，前部分是字符串，后一部分是json格式的日志input{kafka{bootstrap_servers=&gt;&quot;kafka1:9092,kafka2:9092,kafka3:9092&quot;topics=&gt;[&quot;tomcat&quot;]type=&gt;&quot;tomcat&quot;codec=&gt;json{charset=&gt;&quot;UTF-8&quot;}}}filter{if[type]==&quot;tomcat&quot;{grok{match=&gt;{'message'=&gt;'%{TIMESTAMP_ISO8601:access_time}%{LOGLEVEL:loglevel}%{INT:number}---\\[%{DATA:thread_name}\\]%{URIHOST:type_name}%{SPACE}*:%{HOSTNAME:status}%{HOSTNAME:action}%{URIHOST:usetime}((?&lt;request&gt;(.*)(?=Creating)/?)|)(%{GREEDYDATA:sql}|)'}}json{source=&gt;&quot;request&quot;#target=&gt;&quot;parsedJson&quot;remove_field=&gt;[&quot;request&quot;]}mutate{remove_field=&gt;[&quot;agent&quot;]remove_field=&gt;[&quot;[ecs][version]&quot;,&quot;[fields][log_topics]&quot;,&quot;[fields][registry_file]&quot;,&quot;[host][name]&quot;,&quot;[input][type]&quot;,&quot;[@version]&quot;,&quot;[tag]&quot;,&quot;[id]&quot;,&quot;[score]&quot;]}}}output{if[type]==&quot;tomcat&quot;{elasticsearch{hosts=&gt;[&quot;elasticsearch1:9200&quot;,&quot;elasticsearch2:9200&quot;,&quot;elasticsearch3:9200&quot;]index=&gt;&quot;tomcat-%{+YYYY.MM.dd}&quot;user=&gt;'elastic'password=&gt;'d27DuDQjTChIdv3sE8oI'}}}这一份是整个的日志都是json格式input{kafka{bootstrap_servers=&gt;&quot;kafka1:9092,kafka2:9092,kafka3:9092&quot;topics=&gt;[&quot;gateway&quot;]type=&gt;&quot;gateway&quot;codec=&gt;json{charset=&gt;&quot;UTF-8&quot;}}}filter{if[type]==&quot;gateway&quot;{json{source=&gt;&quot;message&quot;#target=&gt;&quot;doc&quot;remove_field=&gt;[&quot;message&quot;]}mutate{remove_field=&gt;[&quot;agent&quot;]remove_field=&gt;[&quot;[ecs][version]&quot;,&quot;[fields][log_topics]&quot;,&quot;[fields][registry_file]&quot;,&quot;[host][name]&quot;,&quot;[input][type]&quot;,&quot;[@version]&quot;,&quot;[tag]&quot;,&quot;[id]&quot;,&quot;[score]&quot;]}}}output{if[type]==&quot;gateway&quot;{elasticsearch{hosts=&gt;[&quot;elasticsearch1:9200&quot;,&quot;elasticsearch2:9200&quot;,&quot;elasticsearch3:9200&quot;]index=&gt;&quot;gateway-%{+YYYY.MM.dd}&quot;user=&gt;'elastic'password=&gt;'d27DuDQjTChIdv3sE8oI'}}}这是一份整个日志都是字符串的格式input{kafka{bootstrap_servers=&gt;&quot;kafka1:9092,kafka2:9092,kafka3:9092&quot;topics=&gt;[&quot;gb_core_ngx&quot;]type=&gt;&quot;gb_core_ngx&quot;codec=&gt;json{charset=&gt;&quot;UTF-8&quot;}}}filter{if[type]==&quot;gb_core_ngx&quot;{grok{match=&gt;{&quot;message&quot;=&gt;&quot;%{URIHOST:hostname}:\\[%{TIMESTAMP_ISO8601:local_time}\\](%{NUMBER:status}|-)%{NUMBER:siteid}-%{WORD:idc_num}-%{HOSTNAME:Random_value}(%{HOSTNAME:X_system}|-)*(%{URIHOST:remote_hostname}|-),%{URIHOST:outsite_hostname}\\[(%{URIHOST:upstream_addr}|-)]%{NUMBER:request_time}(%{NUMBER:upstraem_response_time}|-)(%{WORD:request_method}|-)(%{HOSTNAME:http_host}|-)(%{GREEDYDATA:uri}|-)(%{NUMBER:body_bytes_sent}|-)\\|(%{IPORHOST:client_ip}|-)&quot;}}mutate{remove_field=&gt;&quot;tags&quot;remove_field=&gt;&quot;port&quot;remove_field=&gt;&quot;prospector&quot;remove_field=&gt;&quot;beat&quot;remove_field=&gt;&quot;source&quot;remove_field=&gt;&quot;offset&quot;remove_field=&gt;&quot;fields&quot;remove_field=&gt;&quot;host&quot;remove_field=&gt;&quot;@version&quot;remove_field=&gt;&quot;message&quot;remove_field=&gt;&quot;input&quot;convert=&gt;[&quot;body_bytes_sent&quot;,&quot;float&quot;]convert=&gt;[&quot;request_time&quot;,&quot;float&quot;]convert=&gt;[&quot;upstream_response_time&quot;,&quot;float&quot;]add_field=&gt;{&quot;uuid&quot;=&gt;&quot;%{siteid}-%{idc_num}-%{Random_value}&quot;}}#date{#match=&gt;[&quot;local__time&quot;,&quot;yyyy-MM-ddHH:mm:ss&quot;]#}geoip{source=&gt;&quot;client_ip&quot;target=&gt;&quot;geoip&quot;database=&gt;&quot;/opt/GeoLite2-City.mmdb&quot;add_field=&gt;[&quot;[geoip][coordinates]&quot;,&quot;%{[geoip][longitude]}&quot;]add_field=&gt;[&quot;[geoip][coordinates]&quot;,&quot;%{[geoip][latitude]}&quot;]}mutate{convert=&gt;[&quot;[geoip][coordinates]&quot;,&quot;float&quot;]}}}output{if[type]==&quot;gb_core_ngx&quot;{elasticsearch{hosts=&gt;[&quot;elasticsearch1:9200&quot;,&quot;elasticsearch2:9200&quot;,&quot;elasticsearch3:9200&quot;]index=&gt;&quot;logstash-gb_core_ngx-%{+YYYY.MM.dd}&quot;user=&gt;'elastic'password=&gt;'d27DuDQjTChIdv3sE8oI'}}}添加&amp;组合字段filter{grok{match=&gt;{&quot;message&quot;=&gt;'&quot;(%{GREEDYDATA:cust_date})&quot;,&quot;(%{TIME:cust_time})&quot;,&quot;(%{NUMBER:author})&quot;'}add_field=&gt;{&quot;date_time&quot;=&gt;&quot;%{cust_date};%{cust_time}&quot;}}date{match=&gt;[&quot;date_time&quot;,&quot;yyyy-MM-dd;hh:mm:ss&quot;]target=&gt;&quot;@timestamp&quot;add_field=&gt;{&quot;debug&quot;=&gt;&quot;timestampMatched&quot;}}","link":"https://chriswsq.github.io/post/logstash-pei-zhi/"},{"title":"elk用户认证问题","content":"elk7+的用户认证免费了，不过没有集成在安装包里，还需要自己再进行一些操作，在此记录下自己再配置用户认证时遇到的问题开启安全功能首先在es配置文件中开启安全功能.....xpack.security.enabled:truexpack.security.transport.ssl.enabled然后重启es服务生成证书，配置Node间SSL通信创建ca证书$bin/elasticsearch-certutilca-v一路回车即可默认的CA证书存放在$ES_HOME目录中这个命令生成格式为PKCS#12名称为elastic-stack-ca.p12的keystore文件，包含CA证书和私钥。创建节点间认证用的证书$./bin/elasticsearch-certutilcert--caelastic-stack-ca.p12一路回车即可配置ES节点使用这个证书$mkdirconfig/certs$mvelastic-*config/certs/llconfig/certs/-rw-------1elasticsearchroot3443Jun915:22elastic-certificates.p12-rw-------1elasticsearchroot2527Jun915:22elastic-stack-ca.p12拷贝这个目录到所有的ES节点中config/certs目录中不需要拷贝CA证书文件，只拷贝cert文件即可配置elasticsearch.yml配置文件，注意所有的node节点都需要配置，这里的配置是使用PKCS#12格式的证书。$vimconfig/elasticsearch.ymlxpack.security.enabled:truexpack.security.transport.ssl.enabled:truexpack.security.transport.ssl.verification_mode:certificate#认证方式使用证书xpack.security.transport.ssl.keystore.path:certs/elastic-certificates.p12xpack.security.transport.ssl.truststore.path:certs/elastic-certificates.p12测试能够正常启动了。好了，我们再来继续之前的生成密码：在随意一台节点即可。生成密码有两个命令，一个为自动生成，一个为手动生成自动bin/elasticsearch-setup-passwordsauto手动bin/elasticsearch-setup-passwordsinteractive这里我们选择自动[root@elasticsearch1elasticsearch]#bin/elasticsearch-setup-passwordsautoInitiatingthesetupofpasswordsforreserveduserselastic,apm_system,kibana,logstash_system,beats_system,remote_monitoring_user.Thepasswordswillberandomlygeneratedandprintedtotheconsole.Pleaseconfirmthatyouwouldliketocontinue[y/N]yChangedpasswordforuserapm_systemPASSWORDapm_system=EKPbqziKu4v3P0MYoYIJChangedpasswordforuserkibanaPASSWORDkibana=48Sf7tIFArn4rmyMW2x4Changedpasswordforuserlogstash_systemPASSWORDlogstash_system=O09XtxP495uNRxHRZxtCChangedpasswordforuserbeats_systemPASSWORDbeats_system=ULC810uP0sVCAKhbwxvEChangedpasswordforuserremote_monitoring_userPASSWORDremote_monitoring_user=zqnO9YBFsoRu5QIsgOi6ChangedpasswordforuserelasticPASSWORDelastic=d27DuDQjTChIdv3sE8oI查看集群节点数量：[root@test-1conf]#curl-uelastic192.168.154.6:9200/_cat/nodesEnterhostpasswordforuser'elastic':10.0.0.67328810.120.080.43ilmr-node-110.0.0.92148010.120.070.22dilmrt*node-210.0.0.93127910.000.030.24dilmrt-node-3kibana的安装配置编辑配置文件$catkibana/config/kibana.yml|grep-Ev&quot;^$|^#&quot;server.port:5601server.host:&quot;0.0.0.0&quot;server.name:&quot;mykibana&quot;elasticsearch.hosts:[&quot;http://localhost:9200&quot;]kibana.index:&quot;.kibana&quot;elasticsearch.username:&quot;kibana&quot;elasticsearch.password:&quot;UKuHceHWudloJk9NvHlX&quot;#i18n.locale:&quot;en&quot;i18n.locale:&quot;zh-CN&quot;xpack.security.encryptionKey:Hz*9yFFaPejHvCkhT*ddNx%WsBgxVSCQ页面访问kibana输入上面生成的管理员elastic的用户和密码，就可以登陆了，我们查看一下license许可吧：一个使用永不过期的Basic许可的免费License，开启了基本的Auth认证和集群间SSL/TLS认证的Elasticsearch集群就创建完毕了。等等，你有没有想过Kibana的配置文件中使用着明文的用户名密码，这里只能通过LInux的权限进行控制了，有没有更安全的方式呢，有的，就是keystore。kibanakeystore安全配置参考官方查看kibana-keystore命令帮助：$./bin/kibana-keystore--helpUsage:bin/kibana-keystore[options][command]AtoolformanagingsettingsstoredintheKibanakeystoreOptions:-V,--versionoutputtheversionnumber-h,--helpoutputusageinformationCommands:create[options]CreatesanewKibanakeystorelist[options]Listentriesinthekeystoreadd[options]&lt;key&gt;Addastringsettingtothekeystoreremove[options]&lt;key&gt;Removeasettingfromthekeystore首先我们创建keystore：$bin/kibana-keystorecreateCreatedKibanakeystorein/opt/elk74/kibana-7.4.2-linux-x86_64/data/kibana.keystore#默认存放位置增加配置：我们要吧kibana.yml配置文件中的敏感信息，比如：elasticsearch.username和elasticsearch.password，给隐藏掉，或者直接去掉；所以这里我们增加两个配置：分别是elasticsearch.password和elasticsearch.username:#查看add的命令帮助：$./bin/kibana-keystoreadd--helpUsage:add[options]&lt;key&gt;AddastringsettingtothekeystoreOptions:-f,--forceoverwriteexistingsettingwithoutprompting-x,--stdinreadsettingvaluefromstdin-s,--silentpreventalllogging-h,--helpoutputusageinformation#创建elasticsearch.username这个key：注意名字必须是kibana.yml中的key$./bin/kibana-keystoreaddelasticsearch.usernameEntervalueforelasticsearch.username:******#输入key对应的value，这里是kibana连接es的账号：kibana#创建elasticsearch.password这个key$./bin/kibana-keystoreaddelasticsearch.passwordEntervalueforelasticsearch.password:********************#输入对应的密码：UKuHceHWudloJk9NvHlX好了，我们把kibana.yml配置文件中的这两项配置删除即可，然后直接启动kibana，kibana会自动已用这两个配置的。最终的kibana.yml配置如下：server.port:5601server.host:&quot;0.0.0.0&quot;server.name:&quot;mykibana&quot;elasticsearch.hosts:[&quot;http://localhost:9200&quot;]kibana.index:&quot;.kibana&quot;#i18n.locale:&quot;en&quot;i18n.locale:&quot;zh-CN&quot;xpack.security.encryptionKey:Hz*9yFFaPejHvCkhT*ddNx%WsBgxVSCQ这样配置文件中就不会出现敏感信息了，达到了更高的安全性。类似的Keystore方式不只是Kibana支持，ELK的产品都是支持的。注意：两个证书文件属主须是启动用户，certs权限为744即可，权限不够会报错,其他节点权限也一样elk_elasticsearch1.1.rqtnwnm1sqzw@test-1|uncaughtexceptioninthread[main]elk_elasticsearch1.1.rqtnwnm1sqzw@test-1|ElasticsearchSecurityException[failedtoloadSSLconfiguration[xpack.security.transport.ssl]];nested:ElasticsearchException[failedtoinitializeSSLTrustManager-notpermittedtoreadtruststorefile[/usr/share/elasticsearch/config/certs/elastic-certificates.p12]];nested:AccessDeniedException[/usr/share/elasticsearch/config/certs/elastic-certificates.p12];elk_elasticsearch1.1.jp9k598g34hq@test-1|&quot;...6more&quot;]}elk_elasticsearch1.1.yk71h6aom47z@test-1|&quot;atorg.elasticsearch.node.Node.&lt;init&gt;(Node.java:481)~[elasticsearch-7.7.0.jar:7.7.0]&quot;,elk_elasticsearch1.1.w26656n32a86@test-1|&quot;...6more&quot;]}elk_elasticsearch1.1.w26656n32a86@test-1|ElasticsearchSecurityException[failedtoloadSSLconfiguration[xpack.security.transport.ssl]];nested:ElasticsearchException[failedtoinitializeSSLTrustManager-notpermittedtoreadtruststorefile[/usr/share/elasticsearch/config/certs/elastic-certificates.p12]];nested:AccessDeniedException[/usr/share/elasticsearch/config/certs/elastic-certificates.p12];elk_elasticsearch1.1.rqtnwnm1sqzw@test-1|Likelyrootcause:java.nio.file.AccessDeniedException:/usr/share/elasticsearch/config/certs/elastic-certificates.p12elk_elasticsearch1.1.jp9k598g34hq@test-1|ElasticsearchSecurityException[failedtoloadSSLconfiguration[xpack.security.transport.ssl]];nested:ElasticsearchException[failedtoinitializeSSLTrustManager-notpermittedtoreadtruststorefile[/usr/share/elasticsearch/config/certs/elastic-certificates.p12]];nested:AccessDeniedException[/usr/share/elasticsearch/config/certs/elastic-certificates.p12];参考自logstash配置参考","link":"https://chriswsq.github.io/post/elk-yong-hu-ren-zheng-wen-ti/"},{"title":"ELK告警之elastalert部署及配置","content":"ELK7版本的日志告警Elastalert配置安装Elastalert环境CentOS：7.6Python：3.7.9pip：19.3elastalert：0.2.0elk：7.7.0配置Python3.6.9环境安装依赖包yum-yinstallwgetopensslopenssl-develgccgcc-c++下载包wgethttps://www.python.org/ftp/python/3.7.9/Python-3.7.9.tgz安装依赖https://blog.csdn.net/lkgCSDN/article/details/84403329安装tarxfPython-3.7.9.tgzcdPython-3.7.9;./configure--prefix=/usr/local/python3--with-openssl=/usr/local/openssl--enable-optimizationsmake&amp;&amp;makeinstall配置ln-s/usr/local/python3/bin/python3/usr/bin/python3ln-s/usr/local/python3/bin/pip3/usr/bin/pip3pipinstall--upgradepip注意,所有依赖python2的脚本或者命令,需要更改为python2.7,因为现在默认的python版本为3.7,例如sed-i'1s/python/python2.7/g'/usr/bin/yumsed-i'1s/python/python2.7/g'/usr/libexec/urlgrabber-ext-down验证$python-VPython3.7.9$pip-Vpip19.3from/usr/local/python/lib/python3.6/site-packages/pip(python3.6).安装elastalert下载包gitclonehttps://github.com/Yelp/elastalert.gitcdelastalert安装pipinstall&quot;elasticsearch&lt;7,&gt;6&quot;pipinstall-rrequirements.txtpythonsetup.pyinstall安装成功后可以看到四个命令ll/usr/local/python/bin/elastalert*/usr/local/python/bin/elastalert/usr/local/python/bin/elastalert-create-index/usr/local/python/bin/elastalert-rule-from-kibana/usr/local/python/bin/elastalert-test-rule软连接到/usr/bin下,方便使用ln-s/usr/local/python/bin/elastalert*/usr/binelastalert报警执行的命令,会根据报警规则执行相应操作。elastalert-create-index会创建一个索引，ElastAlert会把执行记录存放到这个索引中，默认情况下，索引名叫elastalert_status。其中有4个_type，都有自己的@timestamp字段，所以同样也可以用kibana来查看这个索引的日志记录情况。elastalert-rule-from-kibana从Kibana3已保存的仪表盘中读取Filtering设置，帮助生成config.yaml里的配置。不过注意，它只会读取filtering，不包括queries。elastalert-test-rule测试自定义配置中的rule设置。使用官方文档：https://elastalert.readthedocs.io规则文档：https://elastalert.readthedocs.io/en/latest/ruletypes.html主配置文件首先是主配置文件的模板为config.yaml.example,生成全局配置cpconfig.yaml.exampleconfig.yamlvimconfig.yaml#用来加载rule的目录，默认是example_rulesrules_folder:rules#用来设置定时向elasticsearch发送请求，也就是告警执行的频率run_every:seconds:30#用来设置请求里时间字段的范围buffer_time:seconds:30#elasticsearch的host地址,端口es_host:node01es_port:9200es_username:elasticelasticsearch认证用户es_password:d27DuDQjTChIdv3sE8oI#elastalert产生的日志在elasticsearch中的创建的索引writeback_index:elastalert_statuswriteback_alias:elastalert_alerts#失败重试的时间限制alert_time_limit:days:2创建告警索引执行elastalert-create-index命令在ES创建索引，这不是必须的步骤，但是强烈建议创建。因为对于审计和测试很有用，并且重启ES不影响计数和发送alert[root@test-1elastalert]#elastalert-create-indexElasticVersion:7.7.0ReadingElastic6indexmappings:Readingindexmapping'es_mappings/6/silence.json'Readingindexmapping'es_mappings/6/elastalert_status.json'Readingindexmapping'es_mappings/6/elastalert.json'Readingindexmapping'es_mappings/6/past_elastalert.json'Readingindexmapping'es_mappings/6/elastalert_error.json'Newindexelastalert_statuscreatedDone!看到这个输出，就说明创建成功了，也可以请求一下看看：[root@test-1elastalert]#curl-uelastic127.0.0.1:9200/_cat/indices?vEnterhostpasswordforuser'elastic':healthstatusindexuuidprirepdocs.countdocs.deletedstore.sizepri.store.sizegreenopenelastalert_status_statusx-ewAZWNR0-vKhvO1gMCnA1100416b208bgreenopenelastalert_statuswsOzkDHnT-G0TOzMowBuDQ1100416b208bgreenopen.apm-agent-configurationadCDI2foQaybW_GKiexeFg1100416b208bgreenopenelastalert_status_past1f6xlhtEQvqZSj6Tmyi0pw1100416b208bgreenopenelastalert_status_silenceRxi1GdubRdq94cgNShsm_A1100416b208bgreenopen.kibana_2pXVkkt3aS6CxT0k83AM8Ag116916254.5kb127.2kbgreenopenca-db-2020.06.11Mh7Yk1XcQb2T7ACTBzKHbQ1159059.3kb34.9kbgreenopen.kibana_1lCyb8DQqRZ2PxexfMxAmTQ111013.7kb6.8kbgreenopentomcat-2020.06.100M34ANeqSxerM-BYj0-O_A113035.6kb17.8kbgreenopengateway-2020.06.10fKGjgXg-QiCsCRHT6XxuRw1114430808.8kb404.4kbgreenopen.tasksXDOZG-L5TNOeHHCR7hcd4w111012.8kb6.4kbgreenopen.security-7aPEpVA4eSXSZYWowWT_LZw11431158.7kb79.3kbgreenopenca-server-2020.06.11Ue3C6cSvR4-6LoUdEiLI4A1126048.3kb27.3kbgreenopen.apm-custom-linkrvjLGExARVattrRQnFSH2A1100416b208bgreenopen.kibana_task_manager_1amwNW3LbT2eat9fZTAPdyw11515102.7kb56.3kbgreenopenpeer_test-2020.06.109ig45KJcQwy2y4YYFwVKGA1112850920.7kb460.3kbgreenopen.async-searchAcrUB2uUSGueXXr_ueHHzw111119.1kb9.5kbgreenopenelastalert_status_errorwtpRUid1S5CA3Oa6_wAOpg1100416b208bRule配置所有的告警规则，通过在rule目下创建配置文件进行定义，这里简单创建一个来作为演示。首先我已经在elk集群中配置了一个NGINX日志采集的流水线，现在去kibana中利用检索规则，过滤出我想要的告警内容，比如我想让状态码是404的请求，触发告警通知，就用如下语句进行查询：response:404其中group是kafka里边定义的组，后边是状态码，还可以写更多条件进行匹配。然后来到服务器添加一条规则：vimnginx_404.yamlname:Nginx_erruse_strftine_index:trueindex:nginx_info*type:anyaggregation:seconds:10filter:-query:query_string:query:&quot;response:404&quot;alert:-&quot;email&quot;email:-&quot;test@qq.com&quot;smtp_host:smtp.163.comsmtp_port:25smtp_auth_file:/opt/elastalert/smtp_auth_file.yamlfrom_addr:test01@163.comemail_reply_to:teast02@163.com注意里边在配置邮件通知的时候，还需要引用外部的一个文件，这个文件里用于存放对应邮箱的用户名密码。vim/opt/elastalert/smtp_auth_file.yaml15726632807@163.comZSMIKAVFCLTASBHXV规则测试刚刚已经添加了一条规则，现在可以用自身的命令测试一下刚刚添加的规则。elastalert-test-rule--configconfig.yamlnginx_404.yaml","link":"https://chriswsq.github.io/post/elk-gao-jing-zhi-elastalert-bu-shu-ji-pei-zhi/"},{"title":"关于Logstash中grok插件的正则表达式例子","content":"日志收集里最重要的就是logstash对日志的过滤，Logstash里提供了一系列的filter来让我们转换日志。Grok就是这些filters里最重要的一个插件，下面就了解下Grok提供的常用Patterns说明及举例大多数Linux使用人员都有过用正则表达式来查询机器中相关文件或文件里内容的经历，在Grok里，我们也是使用正则表达式来识别日志里的相关数据块。有两种方式来使用正则表达式：直接写正则来匹配用Grok表达式映射正则来匹配常用表达式GREEDYDATA匹配任意字符USERNAME或USER用户名，由数字、大小写及特殊字符(._-)组成的字符串比如：1234、Bob、Alex.Wong等EMAILLOCALPART电子邮件用户名部分，首位由大小写字母组成，其他位由数字、大小写及特殊字符(_.+-=:)组成的字符串。注意，国内的QQ纯数字邮箱账号是无法匹配的，需要修改正则比如：stone、Gary_Lu、abc-123等EMAILADDRESS电子邮件比如：wwwwaf@abc.com、Gadf_adf@gmail.com、abc-123@163.com等HTTPDUSERApache服务器的用户，可以是EMAILADDRESS或USERNAMEINT整数，包括0和正负整数比如：0、-123、43987等BASE10NUM或NUMBER十进制数字，包括整数和小数比如：0、18、5.23等BASE16NUM十六进制数字，整数比如：0x0045fa2d、-0x3F8709等WORD字符串，包括数字和大小写字母比如：String、3529345、ILoveYou等NOTSPACE不带任何空格的字符串SPACE空格字符串{SPACE}***匹配任意连续空格QUOTEDSTRING或QS带引号的字符串比如：&quot;Thisisanapple&quot;、'Whatisyourname?'等UUID标准UUID比如：550E8400-E29B-11D4-A716-446655440000MACMAC地址，可以是Cisco设备里的MAC地址，也可以是通用或者Windows系统的MAC地址IPIP地址，IPv4或IPv6地址比如：127.0.0.1、FE80:0000:0000:0000:AAAA:0000:00C2:0002等HOSTNAME主机名称IPORHOSTIP或者主机名称HOSTPORT主机名(IP)+端口比如：127.0.0.1:3306、api.stozen.net:8000等PATH路径，Unix系统或者Windows系统里的路径格式比如：/usr/local/nginx/sbin/nginx、c:\\windows\\system32\\clr.exe等URIPROTOURI协议比如：http、ftp等URIHOSTURI主机比如：www.baidu.net、10.0.0.1:22等URIPATHURI路径比如：//www.baidu.net/abc/、/api.php等URIPARAMURI里的GET参数比如：?a=1&amp;b=2&amp;c=3URIPATHPARAMURI路径+GET参数比如：//www.google.net/abc/api.php?a=1&amp;b=2&amp;c=3URI完整的URI比如：http://www.google.net/abc/api.php?a=1&amp;b=2&amp;c=3日期时间表达式MONTH月份名称比如：Jan、January等MONTHNUM月份数字比如：03、9、12等MONTHDAY日期数字比如：03、9、31等DAY星期几名称比如：Mon、Monday等YEAR年份数字HOUR小时数字MINUTE分钟数字SECOND秒数字TIME时间比如：00:01:23DATE_US美国日期格式比如：10-15-1982、10/15/1982等DATE_EU欧洲日期格式比如：15-10-1982、15/10/1982、15.10.1982等ISO8601_TIMEZONEISO8601时间格式比如：+10:23、-1023等TIMESTAMP_ISO8601ISO8601时间戳格式比如：2016-07-03T00:34:06+08:00DATE日期，美国日期%{DATE_US}或者欧洲日期%{DATE_EU}DATESTAMP完整日期+时间比如：07-03-201600:34:06HTTPDATEhttp默认日期格式比如：03/Jul/2016:00:36:53+0800Log表达式LOGLEVEL日志等级比如：Alert、alert、ALERT、Error等创建自己的Grok表达式在业务领域中，可能会有越来越多的日志格式出现在我们眼前，而Grok的默认表达式显然已无法满足我们的需求（比如用户身份证号、手机号等信息），所以，我们需要自己动手添加些表达式。表达式正则表达式说明DATE_CHS%{YEAR}[./-]%{MONTHNUM}[./-]%{MONTHDAY}中国人习惯的日期格式ZIPCODE_CHS[1-9]\\d{5}国内邮政编码GAME_ACCOUNT[a-zA-Z][a-zA-Z0-9_]{4,15}游戏账号，首字符为字母，4-15位字母、数字、下划线组成其他grok正则表达式：(?(.)(?=Report)/?)获取Report之前的字符grok正则表达式：(?(?=Report)(.)/?)获取Report之后的字符参考","link":"https://chriswsq.github.io/post/guan-yu-logstash-zhong-grok-cha-jian-de-zheng-ze-biao-da-shi-li-zi/"},{"title":"生产环境部署fabric","content":"环境准备ansible临时添加主机信息修改服务器密码关闭防火墙dockerdocker-compose挂载数据盘传送镜像文件ansible临时添加主机信息因为后续需要根据不同节点传递不同的镜像文件，所以先临时添加节点信息。例如：龙岩节点fz[fz]z1ansible_host=121.204.20.15z2ansible_host=121.204.220.15z3ansible_host=121.204.20.14z4ansible_host=121.204.20.15z5ansible_host=121.204.20.11z6ansible_host=121.204.20.1z7ansible_host=121.204.22.14z8ansible_host=121.204.20.14z9ansible_host=121.204.20.14z10ansible_host=121.204.220.1[fz:vars]ansible_ssh_port=650ansible_ssh_pass='123456'关闭防火墙ansiblefz-mshell-a&quot;systemctlstopfirewalld.service&quot;ansiblefz-mshell-a&quot;systemctldisablefirewalld.service&quot;修改服务器密码/application/ansible-playbook/optimization/#推送ansible服务器秘钥ansible-playbook-ehost=fzpush-ssh.yaml#更改服务器密码ansible-playbook-ehost=fzchangepw.yaml安装dockerplaybook地址：/application/ansible-playbook/optimizationansible-playbook-ehost=fzinstall-docker.yaml安装docker#安装依赖ansiblefz-mshell-a&quot;yuminstall-yyum-utils;yum-config-manager--add-repohttps://download.docker.com/linux/centos/docker-ce.repo&quot;#从ansible主机传送docker-ce-18.03.1.ce-1.el7.centos.x86_64.rpm文件ansiblefz-mcopy-a&quot;src=/home/ansible-playbook/optimization/common/docker-ce-18.03.1.ce-1.el7.centos.x86_64.rpmdest=/root/&quot;#安装ansiblefz-mshell-a&quot;yum-yinstalldocker-ce-18.03.1.ce-1.el7.centos.x86_64.rpm&quot;拷贝daemon.json文件ansiblefz-mshell-a&quot;mkdir/etc/docker&quot;ansiblefz-mcopy-a&quot;src=/application/ansible-playbook/localnode/fabirc/peer/install-peer/template/daemon.jsondest=/etc/docker/&quot;安装docker-compose可在传送镜像是安装也可执行下面命令安装curl-Lhttps://get.daocloud.io/docker/compose/releases/download/1.12.0/docker-compose-`uname-s`-`uname-m`&gt;./docker-composemv./docker-compose/usr/local/bin/docker-composechmod+x/usr/local/bin/docker-composedocker-compose-v或ansiblefz-mcopy-a&quot;src=/application/ansible-playbook/common/iso/docker-composedest=/usr/bin/docker-composemode=0755&quot;注：如果是手动安装docker需要调整docker的数据目录/usr/lib/systemd/system/docker.service挂载数据盘#查看数据盘fdisk-l#磁盘分区fdisk/dev/vdbnp回车回车回车w#格式化磁盘mkfs.xfs-nftype=1/dev/vdb1#添加永久挂载echo'/dev/vdb1/bsnxfsdefaults00'&gt;&gt;/etc/fstab#创建挂在目录mkdir/bsn#初始化挂载mount-a#查看挂在情况df-hT传送镜像文件根据节点不同的服务类型来传送不同的镜像传送服务节点的镜像使用ansible服务器上/application/ansible-playbook/optimization/copy.yaml文件进行传送-hosts:&quot;{{host}}&quot;remote_user:roottasks:#-name:安装zip#yum:name=unzipstate=latest-name:创建镜像目录/bsn/isofile:path:/bsn/isostate:directorymode:'0755'-name:传镜像copy:src:/application/ansible-playbook/common/iso/{{item}}dest:/bsn/isowith_items:#-bsnCa-tomcat9.zip#-zipkin.zip-docker-compose-fabric-peer1.4.3.1.zip-fabric-tools1.4.3.1.zip-fabric-couchdb.zip-fabric-ccenv.zip-docker-ce-18.03.1.ce-1.el7.centos.x86_64.rpm-fabric-ca-postgresql.zip-fabric-ca-service.zip-webase-mysql.zip-fabric-gateway.zip-fisco-gm-0.3.zip-webase-mysql.zip-webasechainmanager.zip-webasesign.zip-gateway-service-v4.zip-zipkin.zip-mcs.zip-name:解压shell:cd/bsn/iso&amp;&amp;unzip\\*.zipignore_errors:yes#根据节点所需镜像进行传送，可先将不需要的注释掉peer节点-fabric-peer1.4.3.1.zip-fabric-tools1.4.3.1.zip-fabric-couchdb.zip-fabric-ccenv.zipca节点-fabric-ca-postgresql.zip-fabric-ca-service.zip-webase-mysql.zip网关节点-fabric-gateway.zip-rabbitmq.zip-consul.zipfisco节点-fisco-gm-0.3.zipfisco签名服务链管理-webase-mysql.zip-webasechainmanager.zip-webasesign.zip微服务节点-zipkin.zip-mcs.zippeer节点（z1z2z3）ansible-playbook-ehost=fzcopy.yaml-lz1,z2,z3#保留copy.yaml文件中四个文件，其他注释ca节点（z6）ansible-playbook-ehost=fzcopy.yaml-lz6#保留copy.yaml文件中三个文件，其他注释网关节点（z4）ansible-playbook-ehost=fzcopy.yaml-lz4#保留copy.yaml文件中三个文件，其他注释注：因镜像文件较大，传送时间较长，需耐心等候传送镜像服务结束之后将临时创建的变量文件移除掉（一般移除至/mnt下）部署peer服务修改peer变量文件/etc/ansible/inventory/peer1peer2peer3两个方面调整，添加主机组、主机组添加至子组（例如：peer1）#添加主机组[fuzhou_peer1]fuzhou_peer1ansible_host=121.204.220.16ansible_ssh_port=22[fuzhou_peer1:vars]peer=peer1name=fuzhouMSP=Fuzhounode1=172.31.0.3node2=172.31.0.1node3=172.31.0.2cjq=172.31.0.2#添加子组[peer1:children]suzhou_peer1上传peer相关文件fuzhou-peer1.zipfuzhou-peer2.zipfuzhou-peer3.zip上传至/application/ansible-playbook/localnode/fabirc/peer/install-peer/peer-package部署peer123执行目录：/application/ansible-playbook/localnode/fabirc/peer/install-peeransible-playbook-ehost=peer1install-peer1.yaml-lfuzhou_peer1ansible-playbook-ehost=peer2install-peer2.yaml-lfuzhou_peer2ansible-playbook-ehost=peer3install-peer3.yaml-lfuzhou_peer3查看链码这块会报错，忽略即可部署ca服务修改ca变量文件/etc/ansible/inventory/ca两个方面调整，添加主机组、主机组添加至子组#添加群组[fuzhou_ca]fuzhou_caansible_host=121.204.220.14ansible_ssh_port=22name=fuzhou#添加子组[ca:children]fuzhou_ca加入链码部分会报错，待采集器部署完后之后手动加入链码并安装（命令可见playbook文件底部）上传ca相关文件fuzhou-ca.zip上传至/application/ansible-playbook/localnode/fabirc/ca/install-ca/ca-package部署ca-mysql执行目录/application/ansible-playbook/localnode/fabirc/ca/install-mysqlansible-playbook-ehost=cainstall-mysql.yaml-lfuzhou_ca部署ca-service执行目录：/application/ansible-playbook/localnode/fabirc/ca/install-caansible-playbook-ehost=cainstall-ca.yaml-lfuzhou_ca检查状态查看数据库表状态dockerexeccjq-gate-mysqlmysql-uroot-pRed80@80-e&quot;usebsnflowdb;select*fromtb_nc_config&quot;部署gateway服务修改ca变量文件/etc/ansible/inventory/gateway_M两个方面调整，添加主机组、主机组添加至子组#添加主机组[fuzhou_gateway_M]fuzhou_gateway_Mansible_host=121.204.220.10ansible_ssh_port=22[fuzhou_gateway_M:vars]name=fuzhouMSP=FuzhoutitleName=福州code=ORG20200804#根据当天日志填写即可node1=172.31.0.31#三个peer节点的内网地址node2=172.31.0.23node3=172.31.0.26sign=1.1.1.1#没有fisco填写1.1.1.1ca=172.31.0.24mcs=1.1.1.1#没有微服务填写1.1.1.1#添加子组[gateway_M:children]nanping_gateway_M上传gateway相关文件fuzhou-certs.zip上传至/application/ansible-playbook/localnode/fabirc/gateway/install-gate/gateway-certs部署gateway执行目录：/application/ansible-playbook/localnode/fabirc/gateway/install-gateansible-playbook-ehost=gateway_Minstall-gate.yaml-lfuzhou_gateway_M部署采集器服务修改cjq变量文件[fuzhou_cjq]fuzhou_cjqansible_host=121.204.220.18ansible_ssh_port=22[fuzhou_cjq:vars]name=fuzhouMSP=FuzhoutitleName=福州code=ORG20200804node1=172.31.0.31node2=172.31.0.23node3=172.31.0.26ca=172.31.0.24peerport=22fiscocron=false#没有fisco则填写falsefiscosign=1.1.1.1#没有fisco则填写1.1.1.1xupercron=false#没有xupercron则填写falsemcs=1.1.1.1#没有微服务则填写1.1.1.1gmfabriccron=false#没有微服务则填写false上传采集器相关文件fuzhou-collect.zip上传至/application/ansible-playbook/localnode/fabirc/cjq/install-cjq/install-package部署CJQ执行目录：/application/ansible-playbook/localnode/fabirc/cjq/install-cjqansible-playbook-ehost=cjqinstall-cjq.yaml-lfuzhou_cjq查看服务日志/bsn/go/src/node-collect/logstail-100f/bsn/go/src/node-collect/logs/catalina.out如果出现以下报错，则需要重新加入链，安装链2020-08-0413:57:55.381ERROR4179---[ost-startStop-1]c.b.n.u.C.BaseConfigUtils$Companion:/bsn/go/src/node-collect/webapps/nodecollectConf/conf/xuperChainBlockResult.properties(Nosuchfileordirectory)2020-08-0413:57:55.487INFO4179---[ost-startStop-1]c.b.n.utils.NetUtils$Companion:请求地址：http://1.1.1.1:5005/WeBASE-Chain-Manager/chain/all2020-08-0414:00:02.770INFO4179---[ost-startStop-1]c.b.n.utils.NetUtils$Companion:处理异常：Connectiontimedout(Connectiontimedout),附加信息：fisco获取所有链编号2020-08-0414:00:02.794ERROR4179---[ost-startStop-1]c.b.n.u.C.BaseConfigUtils$Companion:/bsn/go/src/node-collect/webapps/nodecollectConf/conf/sm2FabricBlockResult.properties(Nosuchfileordirectory)2020-08-0414:00:02.830INFO4179---[ost-startStop-1]c.b.nodecollector.task.FiscoTimeTask:fiscofbTimetaskopenorclose==false2020-08-0414:00:02.831INFO4179---[ost-startStop-1]c.b.nodecollector.task.FiscoTimeTask:fiscochannelsUploadTime==12000002020-08-0414:00:02.831INFO4179---[ost-startStop-1]c.b.nodecollector.task.FiscoTimeTask:fiscoblocksUploadTime==12000002020-08-0414:00:02.831INFO4179---[ost-startStop-1]c.b.nodecollector.task.FiscoTimeTask:fiscopeerResourceUploadTime==600000在peer的3个节点执行#加入链码dockerexecclipeerchanneljoin-bblocktx/netchannel.block#安装链码dockerexecclipeerchaincodeinstallblocktx/cc_sys.3.0.pak注意如果重启cd节点docker服务时，因为ca-service启动比ca数据库要快，所以需要等数据库起来后重启ca-service服务","link":"https://chriswsq.github.io/post/sheng-chan-huan-jing-bu-shu-fabric/"},{"title":"Gridea添加Valine评论系统","content":"给自己的博客添加评论系统参考：https://kveln.cn/post/qE678A4ce/注意点：修改主题模板在post.ejs文章详情页模板中文章末尾的位置添加下面的代码&lt;!--载入js，在&lt;/body&gt;之前插入即可--&gt;&lt;!--Leancloud操作库:--&gt;&lt;scriptsrc=&quot;//cdn1.lncld.net/static/js/3.0.4/av-min.js&quot;&gt;&lt;/script&gt;&lt;!--Valine的核心代码库--&gt;&lt;scriptsrc=&quot;/media/scripts/Valine.min.js&quot;&gt;&lt;/script&gt;&lt;divclass=&quot;comment&quot;&gt;&lt;/div&gt;&lt;script&gt;newValine({//AV对象来自上面引入av-min.js(老司机们不要开车➳♡゛扎心了老铁)av:AV,el:'.comment',lang:'zh-cn',//设置评论语言emoticon_url:'https://cloud.panjunwen.com/alu',emoticon_list:[&quot;狂汗.png&quot;,&quot;不说话.png&quot;,&quot;汗.png&quot;,&quot;坐等.png&quot;,&quot;献花.png&quot;,&quot;不高兴.png&quot;,&quot;中刀.png&quot;,&quot;害羞.png&quot;,&quot;皱眉.png&quot;,&quot;小眼睛.png&quot;,&quot;暗地观察.png&quot;],app_id:'你的id',app_key:'你的key',placeholder:'评论留言'});&lt;/script&gt;添加位置添加到&lt;!DOCTYPEhtml&gt;&lt;html&gt;&lt;head&gt;&lt;%-include('./_blocks/head',{siteTitle:`${post.title}|${themeConfig.siteName}`})%&gt;&lt;linkrel=&quot;canonical&quot;href=&quot;&lt;%=post.link%&gt;&quot;/&gt;&lt;/head&gt;&lt;body&gt;&lt;%-include('./_blocks/header')%&gt;&lt;divclass=&quot;main&quot;&gt;&lt;divclass=&quot;main-inner&quot;&gt;&lt;divclass=&quot;content&quot;&gt;&lt;articleclass=&quot;post&quot;&gt;&lt;h2class=&quot;post_titlesm_margin&quot;&gt;&lt;a&gt;&lt;%=post.title%&gt;&lt;/a&gt;&lt;/h2&gt;&lt;script&gt;functionlan(){if(document.getElementById(&quot;lan&quot;).innerText==&quot;繁&quot;){vars=document.getElementById(&quot;tongwenlet_cn&quot;);if(s!=null){document.body.removeChild(s)}vars=document.createElement(&quot;script&quot;);s.language=&quot;javascript&quot;;s.type=&quot;text/javascript&quot;;s.src=&quot;https://cdn.jsdelivr.net/gh/qyxtim/Static@1.1/bookmarklet_tw.js&quot;;s.id=&quot;tongwenlet_cn&quot;;document.body.appendChild(s);document.getElementById(&quot;lan&quot;).innerHTML=&quot;简&quot;}else{if(document.getElementById(&quot;lan&quot;).innerText==&quot;簡&quot;){vars=document.getElementById(&quot;tongwenlet_cn&quot;);if(s!=null){document.body.removeChild(s)}vars=document.createElement(&quot;script&quot;);s.language=&quot;javascript&quot;;s.type=&quot;text/javascript&quot;;s.src=&quot;https://cdn.jsdelivr.net/gh/qyxtim/Static@1.1/bookmarklet_cn.js&quot;;s.id=&quot;tongwenlet_cn&quot;;document.body.appendChild(s);document.getElementById(&quot;lan&quot;).innerHTML=&quot;繁&quot;}}};&lt;/script&gt;&lt;sectionclass=&quot;post_details&quot;&gt;&lt;iclass=&quot;iconfonticon-calendar&quot;&gt;&lt;/i&gt;&lt;spanstyle=&quot;margin-right:15px&quot;&gt;&lt;%=post.dateFormat%&gt;&lt;/span&gt;&lt;iclass=&quot;iconfonticon-browse&quot;&gt;&lt;/i&gt;&lt;spanstyle=&quot;margin-right:15px&quot;&gt;&lt;spanid=&quot;busuanzi_value_page_pv&quot;&gt;&lt;/span&gt;Views&lt;/span&gt;&lt;iclass=&quot;iconfonticon-category&quot;&gt;&lt;/i&gt;&lt;spanclass=&quot;weaklink&quot;style=&quot;margin-right:15px&quot;&gt;&lt;%post.tags.forEach(function(tag,index){%&gt;&lt;ahref=&quot;&lt;%=tag.link%&gt;&quot;class=&quot;tag&quot;&gt;&lt;%=tag.name%&gt;&lt;/a&gt;&lt;%if(index!==post.tags.length-1){%&gt;|&lt;%}%&gt;&lt;%});%&gt;&lt;/span&gt;&lt;iclass=&quot;iconfonticon-caret-down&quot;&gt;&lt;/i&gt;&lt;spanstyle=&quot;margin-right:15px&quot;&gt;&lt;%=post.stats.words%&gt;字&lt;/span&gt;&lt;iclass=&quot;iconfonticon-naozhong&quot;&gt;&lt;/i&gt;&lt;spanstyle=&quot;margin-right:15px&quot;&gt;&lt;%=post.stats.text%&gt;&lt;/span&gt;&lt;aid=&quot;lan&quot;href=&quot;javascript:void(0);&quot;onclick=&quot;lan();&quot;title=&quot;调整简繁体&quot;style=&quot;margin-right:15px;&quot;&gt;繁&lt;/a&gt;&lt;%if(post.isTop){%&gt;&lt;fontcolor=7D26CD&gt;置顶&lt;/font&gt;&lt;%}%&gt;&lt;/section&gt;&lt;%if(themeConfig.showFeatureImage&amp;&amp;post.feature){%&gt;&lt;imgclass=&quot;featureImg&quot;alt=&quot;featureimg&quot;src=&quot;&lt;%=post.feature%&gt;&quot;referrerpolicy=&quot;no-referrer&quot;&gt;&lt;%}%&gt;&lt;divstyle=&quot;display:flex&quot;&gt;&lt;divclass=&quot;md_block&quot;id=&quot;md_block&quot;&gt;&lt;divclass=&quot;round-shape-one&quot;&gt;&lt;/div&gt;&lt;%-post.content%&gt;&lt;spanid=&quot;footnote&quot;&gt;&lt;/span&gt;&lt;divid=&quot;warn&quot;&gt;&lt;/div&gt;&lt;/div&gt;&lt;divclass=&quot;toc-container&quot;&gt;&lt;%-post.toc%&gt;&lt;/div&gt;&lt;/div&gt;&lt;divid=&quot;fullPage&quot;&gt;&lt;canvasid=&quot;canvas&quot;&gt;&lt;/canvas&gt;&lt;/div&gt;&lt;/article&gt;&lt;divid=&quot;eof&quot;&gt;&lt;span&gt;EOF&lt;/span&gt;&lt;/div&gt;&lt;divclass=&quot;round-shape-one&quot;&gt;&lt;/div&gt;&lt;section&gt;&lt;divclass=&quot;doc_comments&quot;&gt;&lt;!--载入js，在&lt;/body&gt;之前插入即可--&gt;&lt;!--Leancloud操作库:--&gt;&lt;scriptsrc=&quot;//cdn1.lncld.net/static/js/3.0.4/av-min.js&quot;&gt;&lt;/script&gt;&lt;!--Valine的核心代码库--&gt;&lt;scriptsrc=&quot;/media/scripts/Valine.min.js&quot;&gt;&lt;/script&gt;&lt;divclass=&quot;comment&quot;&gt;&lt;/div&gt;&lt;script&gt;newValine({//AV对象来自上面引入av-min.js(老司机们不要开车➳♡゛扎心了老铁)av:AV,el:'.comment',lang:'zh-cn',//设置评论语言emoticon_url:'https://cloud.panjunwen.com/alu',emoticon_list:[&quot;狂汗.png&quot;,&quot;不说话.png&quot;,&quot;汗.png&quot;,&quot;坐等.png&quot;,&quot;献花.png&quot;,&quot;不高兴.png&quot;,&quot;中刀.png&quot;,&quot;害羞.png&quot;,&quot;皱眉.png&quot;,&quot;小眼睛.png&quot;,&quot;暗地观察.png&quot;],app_id:'nCwKjHbRmhxBO6Dl20UfM5yw-gzGzoHsz',app_key:'DiIo4l1dsGeJ9VKtPoGOjfN5',placeholder:'唠两块钱儿'});&lt;/script&gt;&lt;%if(typeofcommentSetting!=='undefined'&amp;&amp;commentSetting.showComment){%&gt;&lt;%if(commentSetting.commentPlatform==='gitalk'){%&gt;&lt;divid=&quot;gitalk-container&quot;&gt;&lt;/div&gt;&lt;%-include('./_blocks/gitalk')%&gt;&lt;%}%&gt;&lt;%if(commentSetting.commentPlatform==='disqus'){%&gt;&lt;%-include('./_blocks/disqus')%&gt;&lt;%}%&gt;&lt;%}%&gt;&lt;/div&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;script&gt;&quot;usestrict&quot;;!function(){for(varn=document.getElementsByTagName(&quot;pre&quot;),e=n.length,s=0;s&lt;e;s++){n[s].innerHTML='&lt;spanclass=&quot;line-number&quot;&gt;&lt;/span&gt;'+n[s].innerHTML+'&lt;spanclass=&quot;cl&quot;&gt;&lt;/span&gt;';for(vara=n[s].innerHTML.split(/\\n/).length,r=0;r&lt;a-1;r++){n[s].getElementsByTagName(&quot;span&quot;)[0].innerHTML+=&quot;&lt;span&gt;&quot;+(r+1)+&quot;&lt;/span&gt;&quot;}}}();letmainNavLinks=document.querySelectorAll(&quot;.markdownIt-TOCa&quot;);window.addEventListener(&quot;scroll&quot;,event=&gt;{letfromTop=window.scrollY;mainNavLinks.forEach((link,index)=&gt;{letsection=document.getElementById(decodeURI(link.hash).substring(1));letnextSection=nullif(mainNavLinks[index+1]){nextSection=document.getElementById(decodeURI(mainNavLinks[index+1].hash).substring(1));}if(section.offsetTop&lt;=fromTop){if(nextSection){if(nextSection.offsetTop&gt;fromTop){link.classList.add(&quot;currentToc&quot;);}else{link.classList.remove(&quot;currentToc&quot;);}}else{link.classList.add(&quot;currentToc&quot;);}}else{link.classList.remove(&quot;currentToc&quot;);}});});varlist=document.querySelectorAll(&quot;.katex&quot;);for(vari=0;i&lt;list.length;i++){list[i].style.display=&quot;unset&quot;};varh=document.documentElement,b=document.body,st=&quot;scrollTop&quot;,sh=&quot;scrollHeight&quot;,progress=document.querySelector(&quot;.progress&quot;),scroll;document.addEventListener(&quot;scroll&quot;,function(){scroll=(h[st]||b[st])/((h[sh]||b[sh])-h.clientHeight)*100;progress.style.setProperty(&quot;--scroll&quot;,scroll+&quot;%&quot;)});varwxScale=newWxScale({fullPage:document.querySelector(&quot;#fullPage&quot;),canvas:document.querySelector(&quot;#canvas&quot;)});varimgBox=document.querySelectorAll(&quot;#md_blockimg&quot;);for(vari=0;i&lt;imgBox.length;i++){imgBox[i].onclick=function(e){wxScale.start(this)}};content=&quot;本文最后更新于&lt;%=post.dateFormat%&gt;，已超过1年没有更新，涉及的内容可能已经失效！&quot;;vardate1=&quot;&lt;%=post.date%&gt;&quot;;date1=date1.replace(&quot;-&quot;,&quot;/&quot;);vardate2=newDate();vardate3=date2.getTime()-newDate(date1).getTime();vardays=Math.floor(date3/(24*3600*1000));if(days&gt;=365){document.getElementById(&quot;warn&quot;).innerHTML=content};&lt;/script&gt;&lt;style&gt;#magnifyImg{position:fixed;left:0;top:0;text-align:center;width:100%;display:none;z-index:9999}#magnifyImgimg{object-fit:contain;background:#eaecef;padding:15px;border-radius:10px;height:auto;width:auto;vertical-align:middle}&lt;/style&gt;&lt;%-include('./_blocks/footer')%&gt;&lt;scripttype=&quot;text/javascript&quot;asyncsrc=&quot;&lt;%=themeConfig.domain%&gt;/media/js/prism.js&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;","link":"https://chriswsq.github.io/post/gridea-tian-jia-valine-ping-lun-xi-tong/"},{"title":"cita服务","content":"根据公司需求研究部署cita服务关于CITACITA是一个面向企业级应用的支持智能合约的高性能区块链内核，旨在为企业级区块链应用提供一个稳固、高效、灵活、可适应未来的运行平台。CITA将区块链节点的必要功能解耦为六个微服务：RPC，Auth，Consensus，Chain，Executor，Network。各组件之间通过消息总线交换信息相互协作。通过配置和定制相应的服务，CITA能够满足企业级用户的全部需要。基础环境docker、docker-compose部署方式docker-compose镜像版本cita/cita-release:20.2.0-sm2-sm3注意必选配置项有super_admin和nodes，系统不提供默认配置。安装CITA客户端工具创建目录mkdir-p/bsn/cita2.切换目录cd/bsn/cita3.下载CITA-CLI安装包wgethttps://github.com/citahub/cita-cli/releases/download/20.2.2/cita-cli-x86_64-musl-tls-20.2.2.tar.gz解压程序tarzxvfcita-cli-x86_64-musl-tls-20.2.2.tar.gz5.复制CITA-CLI到系统可执行文件目录下cp-rpcita-cli/usr/local/bin/./cita-cli查看当前所在的链和加密算法cita&gt;info输出结果：url:当前所在的链encryption:加密算法[url]:http://127.0.0.1:1337[pwd]:/Users/yanli/soft/CITA/toolcita[color]:true[debug]:true[json]:true[encryption]:secp256k1[completion_style]:List[edit_style]:Vi[save_private]:true切换链cita&gt;switch--urlhttps://testnet.citahub.com切换加密算法cita&gt;switch--algorithm--help输出结果：&gt;&gt;switchSwitchenvironmentvariables,suchasurl/algorithmUSAGE:switch[FLAGS][OPTIONS]FLAGS:--colorSwitchingcolorforrpcinterface--debugSwitchingdebugmode--jsonSwitchingjsonformat--completion_styleSwitchingcompletionstyle--edit_styleSwitchingeditstyle--save_privateSwitchingwhethersaveprivatekey-h,--helpPrintshelpinformationOPTIONS:--url&lt;url&gt;Switchurl--algorithm&lt;algorithm&gt;Selecttheencryptionalgorithmyouwant,thedefaultissecp256k1[possiblevalues:secp256k1,ed25519,sm2]如：切换至sm2加密算法cita&gt;switch--algorithmsm2创建超级管理员账户地址、私钥、公钥执行命令：cita&gt;keycreate命令返回：{&quot;address&quot;:&quot;0xbac32b0a26bb3380e3f1a8b99af8f32922708730&quot;,&quot;private&quot;:&quot;0x7664a098547142fdf7fc3a732607f707512f98df4d6b7040f009858813e257d5&quot;,&quot;public&quot;:&quot;0x91f9dad655d7ccec4830fa036b0e5311c995acf960bc84df901f54fd232060b99fbc2672d64075263d0f4f4b0d50b1ddf6c22bc9b402626dfa726b2d532918b0&quot;}注：此处为示例公私钥对，不要在生产环境复制使用，其中：&quot;address&quot;：账户地址&quot;private&quot;：账户私钥&quot;public&quot;：账户公钥docker-compose部署cita服务1.生成节点配置文件docker-compose-conf.ymlversion:'3'networks:main:services:config:container_name:configenvironment:-SUPER_ADMIN=0x4b5ae4567ad5d9fb92bc9afd6a657e6fa13a2523-NODES_CONFIG=192.168.40.6:4000,192.168.40.7:4000,192.168.40.8image:cita/cita-release:20.2.0-sm2-sm3hostname:confignetworks:main:aliases:-configvolumes:-./:/opt/cita-runcommand:|bash-c'echo&quot;Createconfigfiles...&quot;;citacreate--super_admin&quot;$$SUPER_ADMIN&quot;--nodes&quot;$$NODES_CONFIG&quot;;echo&quot;Doneconfig&quot;'dockercreatenetworkmaindocker-compose-fdocker-compose-conf.yml创建3个共识节点的目录结构如下:$lstest-chain/012template$ls0addressconsensus.tomlforever.tomllogsauth.tomldatagenesis.jsonnetwork.tomlchain.tomlexecutor.tomljsonrpc.tomlprivkey将配置文件拷贝到其他节点privkey:存放私钥address:存放地址*.toml:各个微服务配置文件，详细说明见微服务说明genesis.json：生成genesis块文件，其中timestamp为时间戳，秒为单位；prevhash-指前一个块哈希，这里是默认值；而alloc指部署到创世块的合约内容；test-chain/template目录下是模板文件，包括这个链的共识节点地址test-chain/template/-authorities.list，系统参数test-chain/template/init_data.yml,节点端口地址test-chain/template/nodes.list等信息logs:记录链运行的日志信息data:数据存储CITA有一些保留端口，设置节点网络端口，或者自定义端口的时候要避免产生端口冲突。保留端口有：默认的jsonrpc端口：1337到1337+N默认的websocket端口：4337到4337+N因为我们是不同的机器所以不用担心端口冲突的问题，可以修改jsonrpc.toml文件上面两个端口为一致2.节点部署文件node-1节点文件docker-compose-node0.ymlversion:'3'networks:main:services:node0:container_name:node0image:cita/cita-release:20.2.0-sm2-sm3hostname:node0networks:main:aliases:-node0volumes:-./:/opt/cita-runports:-&quot;1337:1337&quot;command:|bash-c'while[[!-d/opt/cita-run/test-chain]];dosleep1;done;sleep10;citasetuptest-chain/0;citastarttest-chain/0;sleepinfinity'node-2节点文件docker-compose-node1.ymlversion:'3'networks:main:services:node1:container_name:node1image:cita/cita-release:20.2.0-sm2-sm3hostname:node1networks:main:aliases:-node1volumes:-./:/opt/cita-runports:-&quot;1337:1337&quot;command:|bash-c'while[[!-d/opt/cita-run/test-chain]];dosleep1;done;sleep10;citasetuptest-chain/1;citastarttest-chain/1;sleepinfinity'node-2节点文件docker-compose-node2.yml...3.启动服务docker-compose-node0.ymldocker-compose-node1.ymldocker-compose-node2.yml验证CITA是否运行正常进入容器执行命令检查服务状态dockerexec-itnode0bashroot@node0:/opt/cita-run#citatoptest-chain/0root204210Aug25?00:00:00cita-foreverroot205120420Aug25?00:05:51cita-auth-cauth.tomlroot206920420Aug25?00:02:10cita-bft-cconsensus.toml-pprivkeyroot206120420Aug25?00:00:24cita-chain-cchain.tomlroot205420420Aug25?00:01:50cita-executor-cexecutor.tomlroot206220420Aug25?00:00:55cita-jsonrpc-cjsonrpc.tomlroot205320420Aug25?00:05:40cita-network-cnetwork.toml检查7个服务是否都已经启动。分别去其他节点依次检查检查日志有无报错情况cita-network.log日志会有ERROR-[NodeManager]Cannotgetnodestatusfromknown_addr,thisshould·nothappen!忽略即可基本命查看块高度cita-clirpcblockNumber{&quot;id&quot;:1,&quot;jsonrpc&quot;:&quot;2.0&quot;,&quot;result&quot;:&quot;0x13678&quot;}其中result表示本次查询时的块高度为0x13678。CITA默认3s出一个块，重复执行查询命令，可以观察到result在持续的增长。部署合约执行命令cita-clirpcsendRawTransaction\\--code0x608060405234801561001057600080fd5b5060df8061001f6000396000f3006080604052600436106049576000357c0100000000000000000000000000000000000000000000000000000000900463ffffffff16806360fe47b114604e5780636d4ce63c146078575b600080fd5b348015605957600080fd5b5060766004803603810190808035906020019092919050505060a0565b005b348015608357600080fd5b50608a60aa565b6040518082815260200191505060405180910390f35b8060008190555050565b600080549050905600a165627a7a723058205aed214856a5c433292a354261c9eb88eed1396c83dabbe105bde142e49838ac0029\\--private-key0x3ef2627393529fed043c7dbfd9358a4ae47a88a59949b07e7631722fd6959002--urlhttp://127.0.0.1:1337--algorithmsm2该命令将合约的字节码部署到区块链上。命令返回：{&quot;id&quot;:4,&quot;jsonrpc&quot;:&quot;2.0&quot;,&quot;result&quot;:{&quot;hash&quot;:&quot;0x36e7c55e48df9e465200c55cad1cb14fe677edc8ea087b97c9b8c1ed9495670e&quot;,&quot;status&quot;:&quot;OK&quot;}}查看回执执行命令：cita-clirpcgetTransactionReceipt--hash0x36e7c55e48df9e465200c55cad1cb14fe677edc8ea087b97c9b8c1ed9495670e其中的hash为部署合约的交易hash，所查询的结果为部署合约的交易回执。命令返回：{&quot;id&quot;:1,&quot;jsonrpc&quot;:&quot;2.0&quot;,&quot;result&quot;:{&quot;blockHash&quot;:&quot;0xb9e2167e07550c55406acc22f8d3d83fa81cfd29430c9b5ffb5a7e58fbf130c0&quot;,&quot;blockNumber&quot;:&quot;0x136b3&quot;,&quot;contractAddress&quot;:&quot;0xb701924639d802dc8093d1a43dffa9939a4506de&quot;,&quot;cumulativeQuotaUsed&quot;:&quot;0x17d9d&quot;,&quot;errorMessage&quot;:null,&quot;logs&quot;:[],&quot;logsBloom&quot;:&quot;0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000&quot;,&quot;quotaUsed&quot;:&quot;0x17d9d&quot;,&quot;root&quot;:null,&quot;transactionHash&quot;:&quot;0x36e7c55e48df9e465200c55cad1cb14fe677edc8ea087b97c9b8c1ed9495670e&quot;,&quot;transactionIndex&quot;:&quot;0x0&quot;}}查看交易执行命令：cita-clirpcgetTransaction--hash0x36e7c55e48df9e465200c55cad1cb14fe677edc8ea087b97c9b8c1ed9495670e其中的hash为部署合约的交易hash，所查询的结果为部署合约的交易信息。命令返回：{&quot;id&quot;:1,&quot;jsonrpc&quot;:&quot;2.0&quot;,&quot;result&quot;:{&quot;blockHash&quot;:&quot;0xb9e2167e07550c55406acc22f8d3d83fa81cfd29430c9b5ffb5a7e58fbf130c0&quot;,&quot;blockNumber&quot;:&quot;0x136b3&quot;,&quot;content&quot;:&quot;0x0af202122032343532366161336362383834626164626166656261313437623932663939661880ade2042089ee042afe01608060405234801561001057600080fd5b5060df8061001f6000396000f3006080604052600436106049576000357c0100000000000000000000000000000000000000000000000000000000900463ffffffff16806360fe47b114604e5780636d4ce63c146078575b600080fd5b348015605957600080fd5b5060766004803603810190808035906020019092919050505060a0565b005b348015608357600080fd5b50608a60aa565b6040518082815260200191505060405180910390f35b8060008190555050565b600080549050905600a165627a7a723058205aed214856a5c433292a354261c9eb88eed1396c83dabbe105bde142e49838ac00293220000000000000000000000000000000000000000000000000000000000000000040025220000000000000000000000000000000000000000000000000000000000000000112413fd75865ad692e8903d05c4cadcca4b52621c5b6a4798c9300c2927f6c1b915210d5be6956d0cac8dea79b6a9ac0f1de09dee275eeb4bf871e08ad5a920f8fbb01&quot;,&quot;from&quot;:&quot;0x37d1c7449bfe76fe9c445e626da06265e9377601&quot;,&quot;hash&quot;:&quot;0x36e7c55e48df9e465200c55cad1cb14fe677edc8ea087b97c9b8c1ed9495670e&quot;,&quot;index&quot;:&quot;0x0&quot;}}其中content为交易数据。由于数据经过了签名处理，所以与部署合约时指定的--code不一致。可以使用cita-cli提供的decode-unverifiedTransaction命令来解码。执行命令：cita-clitxdecode-unverifiedTransaction--content0x0af202122032343532366161336362383834626164626166656261313437623932663939661880ade2042089ee042afe01608060405234801561001057600080fd5b5060df8061001f6000396000f3006080604052600436106049576000357c0100000000000000000000000000000000000000000000000000000000900463ffffffff16806360fe47b114604e5780636d4ce63c146078575b600080fd5b348015605957600080fd5b5060766004803603810190808035906020019092919050505060a0565b005b348015608357600080fd5b50608a60aa565b6040518082815260200191505060405180910390f35b8060008190555050565b600080549050905600a165627a7a723058205aed214856a5c433292a354261c9eb88eed1396c83dabbe105bde142e49838ac00293220000000000000000000000000000000000000000000000000000000000000000040025220000000000000000000000000000000000000000000000000000000000000000112413fd75865ad692e8903d05c4cadcca4b52621c5b6a4798c9300c2927f6c1b915210d5be6956d0cac8dea79b6a9ac0f1de09dee275eeb4bf871e08ad5a920f8fbb01命令返回：{&quot;crypto&quot;:0,&quot;signature&quot;:&quot;0x3fd75865ad692e8903d05c4cadcca4b52621c5b6a4798c9300c2927f6c1b915210d5be6956d0cac8dea79b6a9ac0f1de09dee275eeb4bf871e08ad5a920f8fbb01&quot;,&quot;transaction&quot;:{&quot;chain_id&quot;:0,&quot;chain_id_v1&quot;:&quot;0x0000000000000000000000000000000000000000000000000000000000000001&quot;,&quot;data&quot;:&quot;0x608060405234801561001057600080fd5b5060df8061001f6000396000f3006080604052600436106049576000357c0100000000000000000000000000000000000000000000000000000000900463ffffffff16806360fe47b114604e5780636d4ce63c146078575b600080fd5b348015605957600080fd5b5060766004803603810190808035906020019092919050505060a0565b005b348015608357600080fd5b50608a60aa565b6040518082815260200191505060405180910390f35b8060008190555050565b600080549050905600a165627a7a723058205aed214856a5c433292a354261c9eb88eed1396c83dabbe105bde142e49838ac0029&quot;,&quot;encrypted_hash&quot;:&quot;0x8d434bd11a8f94f6fb128b1b941daa573daa21d795858f366fed1593f6eb30ff&quot;,&quot;nonce&quot;:&quot;24526aa3cb884badbafeba147b92f99f&quot;,&quot;pub_key&quot;:&quot;0x9dc6fc7856f5271e6e8c45e5c5fe22d2ff699ac3b24497599be77803d3c25fb4e2fe7da616c65a291910c947c89923009f354634421bddd0a25cd0a509bcf6a9&quot;,&quot;quota&quot;:10000000,&quot;sender&quot;:&quot;0x37d1c7449bfe76fe9c445e626da06265e9377601&quot;,&quot;to&quot;:&quot;&quot;,&quot;to_v1&quot;:&quot;0x0000000000000000000000000000000000000000&quot;,&quot;valid_until_block&quot;:79625,&quot;value&quot;:&quot;0x0000000000000000000000000000000000000000000000000000000000000000&quot;,&quot;version&quot;:2}}其中的data数据便与部署合约中指定的code相一致。调用合约执行命令，调用set函数：cita-clirpcsendRawTransaction\\--code0x60fe47b10000000000000000000000000000000000000000000000000000000000000001\\--private-key0x3ef2627393529fed043c7dbfd9358a4ae47a88a59949b07e7631722fd6959002\\--address0xb701924639d802dc8093d1a43dffa9939a4506de注意：这里的address要与部署合约回执中的contractAddress相一致。命令返回：{&quot;id&quot;:4,&quot;jsonrpc&quot;:&quot;2.0&quot;,&quot;result&quot;:{&quot;hash&quot;:&quot;0xd68853e6df7893b5b2299abaf8cb2631e37b4047dbab55511c96c459efd2242b&quot;,&quot;status&quot;:&quot;OK&quot;}}修改个别配置操作示例起链后，也就是说创世块一旦生成，除chainName、operator、website三项可以在运行时更改，其他配置均无法修改。接下来我们用cita-cli来进行演示，以超级管理员修改chainName作为示例：确保你的链正常运行，进入cita-cli交互式模式，输入命令：$scmSysConfigsetChainName--chain-name&quot;AAA&quot;--admin-private\\0x5f0258a4778057a8a7d97809bd209055b2fbafa654ce7d31ec7191066b9225e6查询交易回执无误后，我们成功的把链名称从默认的test-chain更改为AAA。我们可以通过getMeta查询更改后的结果，示例如下：rpcgetMetaData输出：{&quot;id&quot;:1,&quot;jsonrpc&quot;:&quot;2.0&quot;,&quot;result&quot;:{&quot;blockInterval&quot;:3000,&quot;chainId&quot;:1,&quot;chainName&quot;:&quot;AAA&quot;,&quot;economicalModel&quot;:1,&quot;genesisTimestamp&quot;:1538101178583,&quot;operator&quot;:&quot;test-operator&quot;,&quot;tokenAvatar&quot;:&quot;https://cdn.citahub.com/icon_cita.png&quot;,&quot;tokenName&quot;:&quot;CITATestToken&quot;,&quot;tokenSymbol&quot;:&quot;CTT&quot;,&quot;validators&quot;:[&quot;0x185e7072f53574666cf8ed8ec080e09b7e39c98f&quot;],&quot;version&quot;:1,&quot;website&quot;:&quot;https://www.example.com&quot;}}节点管理节点分类CITA区块链分为两大类节点：共识节点：共识节点具有出块和投票权限，交易由共识节点排序并打包成块，共识完成后即被确认为合法区块。普通节点：普通节点没有出块和投票权限，其他方面和共识节点相同。可以同步和验证链上所有的原始数据，接受交易数据并向其他节点广播。###普通节点管理普通节点的管理，是指普通节点的添加与删除。当前CITA的节点发现提供两种机制：自动发现机制，本节点会自动通过所配置的peers发现网络中的其它节点并尝试连接。配置文件发现机制，本节点仅连接peers中所配置的节点。CITA的节点发现机制通过network网络配置中的enable_discovery配置项设置。添加普通节点（以下以4号节点举例）假设目前的工作目录在cita/target/install/下：$lstest-chain/0123templatetemplate中保存了当前节点的公钥地址template/authorities.list，以及创世块信息template/configs/genesis.json，目前地址有四个。生成新node：$./bin/citaappend--chain_nametest-chain--node&quot;127.0.0.1:4004&quot;$lstest-chain/01234templateappend子命令，在指定链中增加对应ip地址的节点。脚本将自动生成4号节点，并在原有节点中test-chain/*/network.toml中插入新节点的ip及端口配置。注：如果不同机器的节点就要手动去修改network.toml文件了启动新节点：新节点只需要按照正常流程启动，就可以连接入网络，并开始同步链上的块数据。注意：a.此时的新节点为普通节点，不参与共识选举，即只能同步数据和接收jsonrpc请求。b.当network网络配置为enable_discovery=false时，需要在原来节点的network.toml文件中peers域添加新节点信息。$./bin/citasetuptest-chain/4$./bin/citastarttest-chain/4删除普通节点当network网络配置为enable_discovery=false时，到对应节点目录下，找到network.toml，删除对应peers条目即可。当network网络配置为enable_discovery=true时，一个节点主动下线，该节点便在网络中被移除。共识节点管理增加共识节点节点需先被添加成为普通节点（参考普通节点管理），才能申请成为共识节点，由超级管理员（拥有超级管理员角色的账号）确认才完成了添加操作。从普通节点升级到共识节点，具体操作需要用到上面合约方法approveNode(address)。共识节点管理合约是系统合约，默认将放在创世块上，下面使用solc命令（solidity的命令行编译器，在CITA镜像中已安装）查看共识节点管理合约的hash：$solc--hashessystem/node_manager.sol--allow-paths.contractaddress:0xffffffffffffffffffffffffffffffffff020001Functionsignatures:dd4c97a0:approveNode(address)2d4ede93:deleteNode(address)30ccebb5:getStatus(address)609df32f:listNode()6ed3876d:listStake()51222d50:setStake(address,uint64)0c829315:stakePermillage(address)645b8b1b:status(address)首先需要启动一条链，具体方法见快速入门部分接下来的测试，用cita-cli命令行模式（与交互式模式的命令是一致的）进行演示。查看当前的共识节点列表：cita-cliscmNodeManagerlistNode--urlhttp://127.0.0.1:1337输出:{&quot;id&quot;:1,&quot;jsonrpc&quot;:&quot;2.0&quot;,&quot;result&quot;:&quot;0x000000000000000000000000000000000000000000000000000000000000002000000000000000000000000000000000000000000000000000000000000000020000000000000000000000004dd0951bd05d394324279cbcb2b40b4398938f56000000000000000000000000a222f39026a7423592de6fe00383de27facdd51a&quot;}返回值为目前的共识节点地址列表。下面我们需要将新增的普通节点通过交易的方式升级为共识节点，新增的普通节点的公钥地址，演示中，为0xa800dab858271557b1e6e9e62eb8b2f22560da4b发送交易$cita-cliscmNodeManagerapproveNode\\--address0xa800dab858271557b1e6e9e62eb8b2f22560da4b\\--admin-private0x7664a098547142fdf7fc3a732607f707512f98df4d6b7040f009858813e257d5\\--urlhttp://127.0.0.1:1337--algorithmsm2","link":"https://chriswsq.github.io/post/cita-fu-wu/"},{"title":"Kubernetes 基礎教學（三）Helm 介紹與建立 Chart","content":"接下來在這篇文章中，我們會介紹一個建立Kubernetes應用變得輕鬆簡單的好幫手：HelmHelm在如何建立一個Pod以及Kubernetes進階三元件我們介紹了多個Kubernetes的元件與他們所對應到的yaml設定檔。假設我們今天有一個複雜的服務，裡面同時包含了很多種設定檔時，如何同時做好版本控制、管理、更新這些設定檔就變得不太容易，且要快速部署這個含有多個設定檔的服務也變得困難。因此Helm就是一個用來解決上述問題的工具。簡單來說，Helm就是一個管理設定檔的工具。他會把Kubernetes一個服務中各種元件裡的yaml檔統一打包成一個叫做chart的集合，然後透過給參數的方式，去同時管理與設定這些yaml檔案。使用一個現有HelmChart接下來我們要來示範用一個現有的HelmChart來嘗試部署一個Wordpress的服務。首先，我們的第一步當然就是要下載Helm。MacOS中我們可以直接使用Homebrew安裝，其他環境可以參考Helm的Github。brewinstallkubernetes-helm下載完後，我們記得要Helm把Cluster配置初始化helminit接下來讓我們安裝Wordpress的Chart，我們可以直接透過指令helminstallstable/wordpress這個指令會讓我們直接到ChartRepository去載入Chart檔並將它部署到我們的KubernetesCluster上，我們現在可以透過指令檢查我們的ClusterkubectlgetallNAMEREADYSTATUSRESTARTSpod/peddling-hog-mariadb-01/1Running0pod/peddling-hog-wordpress-7bf6d69c8b1/1Running1​NAMETYPECLUSTER-IPservice/peddling-hog-mariadbClusterIP10.109.96.113service/peddling-hog-wordpressLoadBalancer10.101.157.184EXTERNAL-IPPORT(S)&lt;none&gt;3306/TCP&lt;pending&gt;80:30439/TCP,443:31824/TCP​NAMEREADYUP-TO-DATEAVAILABLEdeployment.apps/peddling-hog-wordpress1/111​NAMEDESIREDreplicaset.apps/peddling-hog-wordpress-7bf6d69c8b1​NAMEREADYAGEstatefulset.apps/peddling-hog-mariadb1/160s可以看到我們透過Chart一次就安裝與部署了兩個Pod、兩個Service以及其他各種元件。如果要一次把所有Chart所安裝的元件刪除，我們可以先透過helmlist列出我們所有的Chart。NAMEREVISIONUPDATEDSTATUSCHARTpeddling-hog1FriApr2616:08:302019DEPLOYEDwordpress然後輸入helmdeletepeddling-hog就可以一次把所有元件刪除。Chart的運作方式嘗試完從Chart部署元件後，我們可以進一步來暸解Chart是如何運作的。我們可以到Wordpresschart的Github上觀察這個Chart的檔案結構，或是透過指令來建立一個最簡單的Charthelmcreatehelm-demo接下來我們來看看./helm-demo的資料夾.├──Chart.yaml├──charts├──templates│├──deployment.yaml│├──ingress.yaml│└──service.yaml└──values.yaml把這個Chart的檔案結構化簡後就如上所見。Chart.yaml定義了這個Chart的Metadata，包括Chart的版本、名稱、敘述等charts在這個資料夾裡可以放其他的Chart，這裡稱作SubChatemplates定義這個Chart服務需要的Kubernetes元件。但我們並不會把各元件的參數寫死在裡面，而是會用參數的方式代入values.yaml定義這個Chart的所有參數，這些參數都會被代入在templates中的元件。例如我們會在這邊定義nodePorts給service.yaml、定義replicaCount給deployment.yaml、定義hosts給ingress.yaml等等從上面的檔案結構可以看到，我們透過編輯values.yaml，就可以對所有的yaml設定檔做到版本控制與管理。並透過install/delete的方式一鍵部署/刪除。如何建立自己的Chart了解了Chart大致上是如何運作後，我們就可以來實際建立一個簡單的Chart。我們的目標是要透過deployment、service、ingress來讓使用者在輸入blue.demo.com時可以得到一隻小鯨魚。而首先，我們一樣輸入指令helmcreatehelm-demo之後我們就先借看一下在ingress章節有使用過的yaml檔們。deployment.yamlapiVersion:extensions/v1beta1kind:Deploymentmetadata:name:blue-nginxspec:replicas:2template:metadata:labels:app:blue-nginxspec:containers:-name:nginximage:hcwxd/blue-whaleports:-containerPort:3000service.yamlapiVersion:v1kind:Servicemetadata:name:blue-servicespec:type:NodePortselector:app:blue-nginxports:-protocol:TCPport:80targetPort:3000ingress.yamlapiVersion:extensions/v1beta1kind:Ingressmetadata:name:webspec:rules:-host:blue.demo.comhttp:paths:-backend:serviceName:blue-serviceservicePort:80然後我們就可以嘗試來把上述yaml檔中可以作為參數的部分抽取出來，在這邊為了降低複雜度，我們只簡單挑幾個參數出來，然後我們就可以把這些參數寫到values.yaml中。values.yamlreplicaCount:2​image:repository:hcwxd/blue-whale​service:type:NodePortport:80​ingress:enabled:true​hosts:-host:blue.demo.compaths:[/]把參數提取出來後，我們就來依樣畫葫蘆地把template中其他三個yaml檔寫成可以接受參數的方式：deployment.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:{{include&quot;value-helm-demo.fullname&quot;.}}spec:replicas:{{.Values.replicaCount}}selector:matchLabels:app:{{include&quot;value-helm-demo.fullname&quot;.}}template:metadata:labels:app:{{include&quot;value-helm-demo.fullname&quot;.}}spec:containers:-name:{{.Chart.Name}}image:'{{.Values.image.repository}}'ports:-containerPort:3000service.yamlapiVersion:v1kind:Servicemetadata:name:{{include&quot;value-helm-demo.fullname&quot;.}}spec:type:{{.Values.service.type}}ports:-port:{{.Values.service.port}}targetPort:3000protocol:TCPselector:app:{{include&quot;value-helm-demo.fullname&quot;.}}ingress.yaml{{-if.Values.ingress.enabled-}}{{-$fullName:=include&quot;value-helm-demo.fullname&quot;.-}}apiVersion:extensions/v1beta1kind:Ingressmetadata:name:{{$fullName}}spec:rules:{{-range.Values.ingress.hosts}}-host:{{.host|quote}}http:paths:{{-range.paths}}-backend:serviceName:{{$fullName}}servicePort:80{{-end}}{{-end}}{{-end}}寫好後，我們就可以來一鍵部署我們的這三份檔案囉。我們可以直接在/helm-demo資料夾下輸入指令helminstall.NAME:gilded-peacockLASTDEPLOYED:MonMay616:31:272019NAMESPACE:defaultSTATUS:DEPLOYED部署成功後顯示的NAME:gilded-peacock就是這個Chart部署後的名稱囉（在Helm中稱部署出去的這個實體為release）。我們可以再透過指令helmlist列出我們目前所有的releases。接下來我們可以用kubectlgetall來看到我們目前的kubernetes狀況NAMEREADYSTATUSRESTARTSpod/gilded-peacock-helm-demo-5fc59647591/1Running0pod/gilded-peacock-helm-demo-5fc59647591/1Running0​NAMETYPECLUSTER-IPEXTERNAL-IPPORT(S)service/gilded-peacock-helm-demoNodePort10.106.164.53&lt;none&gt;80:30333/TCP​NAMEREADYUP-TO-DATEAVAILABLEdeployment.apps/gilded-peacock-helm-demo2/222​NAMEDESIREDCURRENTreplicaset.apps/gilded-peacock-helm-demo-5fc596475922這邊就可以看到我們所指定的資源都有按照chart的配置建立起來囉，所以打開blue.demo.com就可以看到一隻我們透過Helm實際部署出的小鯨魚。以上，我們就完成Helm的實際部署囉！而其他常用的Helm指令還有helmdelete--purgeRELEASE_NAME刪除一個release（--purge這個flag可以把該RELEASE_NAME釋放出來供之後重複使用）。helmupgradeRELEASE_NAMECHART_PATH如果有更新Chart的檔案時，可以透過upgrade去更新對應的Release。helmlintCHART_PATH檢查你的Chart檔案有沒有錯誤的語法。helmpackageCHART_PATH打包並壓縮整個Chart資料夾的檔案。kubectl額外補充簡寫覺得每次下指令都要打kubectl很花時間的話，可以透過alias來節省時間，例如設定aliaskbs=kubectl。kubectl中的各項資源的名稱其實也都有內建的簡寫，可以透過指令kubectlapi-resources去看到各個資源的簡寫，例如deployments可以簡寫成deploy、services簡寫成svc等。auto-complete覺得kubectl的指令沒有auto-complete很痛苦的話可以參考官網的教學。像是如果使用zsh的話，就可以透過指令echo&quot;if[$commands[kubectl]];thensource&lt;(kubectlcompletionzsh);fi&quot;&gt;&gt;~/.zshrc來啟用kubectl的auto-completecreatevsapply在之前提到透過yaml建立資源時，我們都用了kubectlcreate-f。但其實也可以使用kubectlapply-f達成建立與更新資源，雖然在單純建立的使用情景上沒有差別，但其它用法上的差別可見kubectlapplyvskubectlcreate。小結呼！希望這三篇Kubernetes基礎教學能對你有幫助！再次溫故知新一下：在系列文的第一篇文章中，我們了解了構成Kubernetes的四個重要元素：Pod、Node、Master、Cluster，並安裝好了我們要實際動手玩Kubernetes前需要的套件與工具。而在第二篇文章中，我們實際動手操作了Kubernetes中的Pod、Service、Deployment、Ingress。在最後的這篇文章中，我們了解了讓Kubernetes部署變得好輕鬆好棒棒的工具Helm，並實際透過Helm部署了一個Helm。在學會了上述的各個基礎後，如果想要實際在各個雲端服務上操作Kubernetes的話，歡迎接下來可以進一步去了解Kops！原文链接參考資料：HelmOfficialDocumenthttps://helm.sh/docs/KubernetesOfficialDocumenthttps://kubernetes.io/docs/home/Kubernetes30天學習筆記byzxcvbniushttps://ithelp.ithome.com.tw/articles/10192401五分鐘Kubernetes有感https://medium.com/@evenchange4/%E4%BA%94%E5%88%86%E9%90%98-kubernetes-%E6%9C%89%E6%84%9F-e51f093cb10b十分鐘帶你理解Kubernetes核心概念http://dockone.io/article/932Kubernetes學習筆記https://gcpug-tw.gitbook.io/kuberbetes-in-action/[教學]用Drone,Kubernetes跟Helm，以及RBAC來建置你的CI/CD流程https://medium.com/@cloudsanchen/ci-cd-with-drone-kubernetes-and-helm-part-1-69c147046ffa","link":"https://chriswsq.github.io/post/kubernetes-ji-chu-jiao-xue-san-helm-jie-shao-yu-jian-li-chart/"},{"title":"Kubernetes 基础教学（二）实作范例：Pod、Service、Deployment、Ingress","content":"Kubernetes（K8S）是一個可以幫助我們管理微服務（microservices）的系統，他可以自動化地部署及管理多台機器上的多個容器（Container）。簡單來說，他可以做到：Kubernetes（K8S）是一个可以帮助我们管理微服务（microservices）的系统，他可以自动化地部署及管理多台机器上的多个容器（Container）。简单来说，他可以做到：同时部署多个容器到多台机器上（Deployment）服务的乘载量有变化时，可以对容器做自动扩展（Scaling）管理多个容器的状态，自动侦测并重启故障的容器（Management）在系列文的上一篇文章中，我们了解了构成Kubernetes的四个重要元素：Pod、Node、Master、Cluster，并安装好了我们要实际动手玩Kubernetes前需要的套件与工具。接下来在这篇文章中，我们会透过例子来实际建立那些在Kubernetes中常见的元件们。如何建立一个Pod撰写Pod的身分证还记得我们在介绍Kubernetes时有提到，每个Pod都有一个身分证，也就是属于这个Pod的.yaml档。我们透过撰写下面的这个.yaml档就可以建立出Pod。kubernetes-demo.yamlapiVersion:v1kind:Podmetadata:name:kubernetes-demo-podlabels:app:demoAppspec:containers:-name:kubernetes-demo-containerimage:hcwxd/kubernetes-demoports:-containerPort:3000apiVersion该元件版本号kind该元件是什么属性，常见有Pod、Node、Service、Namespace、ReplicationController等metadataname指定该Pod的名称labels指定该Pod的标签，这里我们暂时帮它上标签为app:demoAppspeccontainer.name指定运行出的Container的名称container.image指定Container要使用哪个Image，这里会从DockerHub上搜寻container.ports指定该Container有哪些portnumber是允许外部资源存取透过kubectl建立Pod有了身份证后，我们就可以透过kubectl指令来建立Podkubectlcreate-fkubernetes-demo.yaml看到pod/kubernetes-demo-podcreated的字样就代表我们建立成功我们的第一个Pod了。我们可以再透过指令kubectlgetpods看到我们运行中的Pod：NAMEREADYSTATUSRESTARTSAGEkubernetes-demo-pod1/1Running060s连线到我们Pod的服务资源建立好我们的Pod之后，打开浏览器的localhost:3000我们会发现怎么什么都看不到。这是因为在Pod中所指定的port，跟我们本机端的port是不相通的。因此，我们必须还要透过kubectlport-forward，把我们两端的port做mapping。kubectlport-forwardkubernetes-demo-pod3000:3000做好mapping后，再打开浏览器的localhost:3000，我们就可以迎接一只可爱的小鲸鱼啰！Kubernetes进阶三元件了解完如何从无到有建立一个KubernetesCluster并产生一个Pod后，接下来我们要认识在现实应用中，我们还会搭配到哪些Kubernetes的进阶元件。其中最重要的三个进阶元件就是：Service、Ingress、Deployment。Service还记得上面提到我们在连线到一个Pod的服务资源时，会使用到port-forward的指令。但如果我们有多个Pods想要同时被连线时，我们就可以用到Service这个进阶元件。简单来说，Service就是Kubernetes中用来定义「一群Pod要如何被连线及存取」的元件。要建立一个Service，一样要撰写属于他的身分证。service.yamlapiVersion:v1kind:Servicemetadata:name:my-servicespec:selector:app:demoApptype:NodePortports:-protocol:TCPport:3001targetPort:3000nodePort:30390apiVersion该元件的版本号kind该元件是什么属性，常见有Pod、Node、Service、Namespace、ReplicationController等metadataname指定该Pod的名称specselector该Service的连线规则适用在哪一群Pods，还记得我们在建立Pod的时候，会帮它上label，这时就可以透过app:demoApp，去找到那群label的app属性是demoApp的Pods们portstargetPort指定我们Pod上允许外部资源存取PortNumberport指定我们Pod上的targetPort要mapping到Service中ClusterIP中的哪个portnodePort指定我们Pod上的targetPort要mapping到Node上的哪个port接下来我们先重新建立我们的Podkubectlcreate-fkubernetes-demo.yaml接下来我们透过service.yaml来建立我们的Service元件kubectlcreate-fservice.yaml然后我们可以透过kubectlgetservices取得我们新建立Service的资料NAMETYPECLUSTER-IPEXTERNAL-IPPORT(S)AGEmy-serviceNodePort10.110.237.205&lt;none&gt;3001:30391/TCP60s有了建立好的Service后，我们可以透过两种方式连线我们的Pod的服务资源。首先，要从外部连线到我们的Pod资源服务，我们必须要先有我们的KubernetesCluster（在这边是minikube）对外开放的IP。我们先透过指令minikubeip得到我们minikube的ip192.168.99.100接着打开我们的浏览器，输入上面的ip加上我们在yaml档指定的nodePort，在这边是192.168.99.100:30390，就会得到我们的小鲸鱼了。而如果不从浏览器，而是直接从minikube里面连线到我们的Pod则要先透过指令minikubesshssh进入我们的minikubecluster，接着输入指令curl&lt;CLUSTER-IP&gt;:&lt;port&gt;其中CLUSTER-IP就是我们用kubectlgetservices得到我们Service的IP，而port就是我们在yaml档指定的port，在这边合起来就是10.110.237.205:3001，于是我们curl10.110.237.205:3001就可以在minikube里面得到我们的小鲸鱼啰！Deployment了解了Service后，接下来要来了解第二个进阶元件：Deployment。今天当我们同时要把一个Pod做横向扩展，也就是复制多个相同的Pod在Cluster中同时提供服务，并监控如果有Pod当机我们就要重新把它启动时，如果我们要一个Pod一个Pod透过指令建立并监控是很花时间的。因此，我们可以透过Deployment这个特殊元件帮我们达成上述的要求。同样要建立一个Deployment，要先撰写属于他的身分证。deployment.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:my-deploymentspec:replicas:3template:metadata:labels:app:demoAppspec:containers:-name:kubernetes-demo-containerimage:hcwxd/kubernetes-demoports:-containerPort:3000selector:matchLabels:app:demoAppapiVersion该元件的版本号kind该元件是什么属性，常见有Pod、Node、Service、Namespace、ReplicationController等metadataname指定该Pod的名称specreplicas指定要建立多少个相同的Pod，在这边给的数字是所谓的DesireState，当Cluster运行时如果Pod数量低于此数字，Kubernetes就会自动帮我们增加pod，反之就会帮我们关掉Podtemplate指定这个Deployment建立的Pod们统一的设定，包括metadata以及这些Pod的Containers，这边我们就沿用之前建立Pod的设定selector指定这个Deployment的规则要适用到哪些Pod，在这边就是指定我们在template中指定的labels接下来我们就可以透过指令kubectlcreate-fdeployment.yaml建立好我们的Deployment，这时我们可以查看我们的Deployment有没有被建立好kubectlgetdeployNAMEREADYUP-TO-DATEAVAILABLEAGEmy-deployment3/33360s接着我们在看Pod们有没有乖乖按照Deployment建立kubectlgetpodsNAMEREADYSTATUSRESTARTSAGEmy-deployment-5454f687cd-bxjfz1/1Running060smy-deployment-5454f687cd-gszbr1/1Running060smy-deployment-5454f687cd-k6zfv1/1Running060s这边我们可以看到三个Pod都被建立好了，我们就成功做到了Pod的横向扩展。而除了Pod的横向扩展外，Deployment的另外一个好处就是可以帮我们做到无停机的系统升级（ZeroDowntimeRollout）。也就是说，当我们要更新我们的Pod时，Kubernetes并不会直接砍掉我们所有的Pod，而是会建立新的Pod，等新的Pod开始正常运行后，再来取代旧的Pod。举例来说，假设我们现在想要更新我们Pod对外的Port，我们可以先透过指令kubectleditdeploymentsmy-deployment接着我们会看到我们的Yaml档apiVersion:extensions/v1beta1kind:Deploymentmetadata:annotations:deployment.kubernetes.io/revision:'2'creationTimestamp:'2019-04-26T04:18:26Z'generation:2labels:app:demoAppname:my-deploymentnamespace:defaultresourceVersion:'328692'selfLink:/apis/extensions/v1beta1/namespaces/default/deployments/my-deploymentuid:56608fb5-67da-11e9-933f-08002789461fspec:progressDeadlineSeconds:600replicas:3revisionHistoryLimit:10selector:matchLabels:app:demoAppstrategy:rollingUpdate:maxSurge:25%maxUnavailable:25%type:RollingUpdatetemplate:metadata:creationTimestamp:nulllabels:app:demoAppspec:containers:-image:hcwxd/kubernetes-demoimagePullPolicy:Alwaysname:kubernetes-demo-containerports:-containerPort:3000protocol:TCPresources:{}terminationMessagePath:/dev/termination-logterminationMessagePolicy:FilednsPolicy:ClusterFirstrestartPolicy:AlwaysschedulerName:default-schedulersecurityContext:{}terminationGracePeriodSeconds:30我们把其中containerPort:3000改成3001后储存，Kubernetes就会开始帮我们进行更新。这时我们继续用指令kubectlgetpods就会看到NAMEREADYSTATUSRESTARTSAGEmy-deployment-5454f687cd-bxjf1/1Running060smy-deployment-5454f687cd-gszb1/1Terminating060smy-deployment-5454f687cd-k6zf1/1Running060smy-deployment-78dc8dcb89-59270/1ContainerCreating01smy-deployment-78dc8dcb89-dwtl1/1Running05s从上面可以看到，Kubernetes会永远保持有3个Pods在正常运作，如果有新的Pod还在ContainerCreating的阶段时，他还不会关掉对应要被取代的Pod。而在过一段时间我们输入同样指令可以看到kubectlrollouthistorydeploymentmy-deployment看到我们目前更改过的版本deployment.extensions/my-deploymentREVISIONCHANGE-CAUSE1&lt;none&gt;2&lt;none&gt;从上面可以看出来，我们目前有两个版本，如果我们发现版本2的程式有问题，想要先让服务先恢复成版本1的程式（Rollback）时，我们还可以透过指令kubectlrolloutundodeploymy-deployment让我们的Pod都恢复成版本1。甚至之后如果版本变的较多后，我们也可以指定要Rollback到的版本kubectlrolloutundodeploymy-deployment--to-revision=2Ingress了解完了Service跟Deployment後，接下來就輪到概ㄓ念稍微複雜的Ingress元件了。在上面有提到Service就是Kubernetes中用來定義「一群Pod要如何被連線及存取」的元件。但在Service中，我們是將每個Service元件對外的portnumber跟Node上的portnumber做mapping，這樣在我們的Service變多時，portnumber以及分流規則的管理變得相當困難。而Ingress可以透過HTTP/HTTPS，在我們眾多的Service前搭建一個reverse-proxy。這樣Ingress可以幫助我們統一一個對外的portnumber，並且根據hostname或是pathname決定封包要轉發到哪個Service上，如同下圖的比較：在Kubernetes中，Ingress这项服务其实是由IngressResources、IngressServer、IngressController构成。其中IngressResources就是定义Ingress的身分证，而IngressServer则是实体化用来接收HTTP/HTTPS连线的网路伺服器。但实际上，IngressServer有各式各样的实作，就如同市面上的WebServer琳琅满目一样。因此，IngressController就是一个可以把定义好的IngressResources设定转换成特定IngressServer实作的角色。举例来说，Kubernetes由官方维护的两种IngressController就有ingress-gce跟ingress-nginx，分别可以对应转换成GCE与Nginx。也有其他非官方在维护的Controller，详细的列表可见官网的additional-controllers。接下来我们要来试着建立一个Ingress物件去根据hostname转发封包到不同的Pod上面。所以第一步，我们要用Deployment建立好几个不同的Pod。在这边我们直接透过准备好的两个Image来建立其中的Container，blue-whale这个Image里的程式会监听3000port然后在浏览器上被存取时会吐出蓝色的鲸鱼，purple-whale则会吐出紫色的鲸鱼。deployment.yamlapiVersion:extensions/v1beta1kind:Deploymentmetadata:name:blue-nginxspec:replicas:2template:metadata:labels:app:blue-nginxspec:containers:-name:nginximage:hcwxd/blue-whaleports:-containerPort:3000---apiVersion:extensions/v1beta1kind:Deploymentmetadata:name:purple-nginxspec:replicas:2template:metadata:labels:app:purple-nginxspec:containers:-name:nginximage:hcwxd/purple-whaleports:-containerPort:3000接着我们就可以透过kubectlcreate-fdeployment.yaml建立好我们的Pod。AMEREADYSTATUSRESTARTSAGEblue-nginx-6b68c797c7-28tkz1/1Running060sblue-nginx-6b68c797c7-8ww8l1/1Running060spurple-nginx-84854fd7c-8g4nl1/1Running060spurple-nginx-84854fd7c-tmrbs1/1Running060s建立好了Pod们后，接下来我们就要建立这些Pod对外的各自Service，在这边我们会把各至Container上的3000port全部都转到80port上。service.yamlapiVersion:v1kind:Servicemetadata:name:blue-servicespec:type:NodePortselector:app:blue-nginxports:-protocol:TCPport:80targetPort:3000---apiVersion:v1kind:Servicemetadata:name:purple-servicespec:type:NodePortselector:app:purple-nginxports:-protocol:TCPport:80targetPort:3000透过kubectlcreate-fservice.yaml建立好我们的service。NAMETYPECLUSTER-IPEXTERNAL-IPPORT(S)blue-serviceNodePort10.111.192.164&lt;none&gt;80:30492/TCPpurple-serviceNodePort0.107.21.77&lt;none&gt;80:32086/TCP最后，我们就可以来建立我们的主角Ingress了！在这边我们的Ingress只有很简单的规则，他会把所有发送到blue.demo.com的封包交给serviceblue-service负责，而根据上面service.yaml的定义，他会再转交给blue-nginx这个Pod。而发送给purple.demo.com则会转交给purple-nginx。在这边，我们要先记得使用指令minikubeaddonsenableingress来启用minikube的ingress功能。接着，我们就来撰写ingress的身分证。ingress.yamlapiVersion:extensions/v1beta1kind:Ingressmetadata:name:webspec:rules:-host:blue.demo.comhttp:paths:-backend:serviceName:blue-serviceservicePort:80-host:purple.demo.comhttp:paths:-backend:serviceName:purple-serviceservicePort:80我们一样透过kubectlcreate-fingress.yaml来建立我们的ingress物件。并使用kubectlgetingress来查看我们的ingress状况：NAMEHOSTSADDRESSPORTSAGEwebblue.demo.com,purple.demo.com10.0.2.158060s接下来我们要来测试ingress有没有乖乖帮我们转发。因为我们的Cluster实际上对外的ip都是我们透过指令minikubeip会看到的192.168.99.100，这样我们要怎么同时让这个ip可以是我们设定规则中的blue.demo.com以及purple.demo.com呢？因为我们知道在DNS解析网址时，会先查找本机上/etc/hosts后才会到其他DNSServer上寻找。所以我们可以透过一个小技巧，在本机上把blue.demo.com以及purple.demo.com都指向192.168.99.100。透过指令echo192.168.99.100blue.demo.com&gt;&gt;/etc/hostsecho192.168.99.100purple.demo.com&gt;&gt;/etc/hosts或是透过sudovim/etc/hosts手动加上这两条规则，我们就成功搞定DNS可以来测试了。接下来我们打开浏览器，输入blue.demo.com就可以得到熟悉的蓝色小鲸鱼然后输入purple.demo.com就可以得到紫色小鲸鱼啰！在实际建立过Pod、Service、Deployment还有Ingress后，在接下来的文章，我们要来介绍一个可以让这个建立流程变得更简单的工具，也就是Kubernetes中的PackageManager：Helm！","link":"https://chriswsq.github.io/post/kubernetes-ji-chu-jiao-xue-er-shi-zuo-fan-li-podservicedeploymentingress/"},{"title":"kubernetes基础教学（一）原理","content":"Kubernetes（K8S）是一个可以帮助我们管理微服务（microservices）的系统，他可以自动化地部署及管理多台机器上的多个容器（Container）。更进一步地说，Kubernetes想解决的问题是：「手动部署多个容器到多台机器上并监测管理这些容器的状态非常麻烦。」而Kubernetes要提供的解法：提供一个平台以较高层次的抽象化去自动化操作与管理容器们。打开Kubernetes的官网，我们可以看到关于Kubernetes服务的描述为：Automatedcontainerdeployment,scaling,andmanagement而白话来说，上面的描述表示他可以做到：同时部署多个容器到多台机器上（Deployment）服务的乘载量有变化时，可以对容器做自动扩展（Scaling）管理多个容器的状态，自动侦测并重启故障的容器（Management）Kubernetes四元件在了解Kubernetes如何帮助我们管理容器们前，我们先要由小到大依序了解组成Kubernetes的四种最基本的元件：Pod、WorkerNode、MasterNode、Cluster。PodKubernetes运作的最小单位，一个Pod对应到一个应用服务（Application），举例来说一个Pod可能会对应到一个APIServer。每个Pod都有一个身分证，也就是属于这个Pod的yaml档一个Pod里面可以有一个或是多个Container，但一般情况一个Pod最好只有一个Container同一个Pod中的Containers共享相同资源及网路，彼此透过localportnumber沟通WorkerNodeKubernetes运作的最小硬体单位，一个WorkerNode（简称Node）对应到一台机器，可以是实体机如你的笔电、或是虚拟机如AWS上的一台EC2或GCP上的一台ComputerEngine。每个Node中都有三个组件：kubelet、kube-proxy、ContainerRuntime。小提醒：在WorkerNode与MasterNode的组件部分，因为Kubernetes本身其实都抽象的很好，所以在Kubernetes「基础的」使用上如何不了解这些组建也不会有非常大的影响。kubelet该Node的管理员，负责管理该Node上的所有Pods的状态并负责与Master沟通kube-proxy该Node的传讯员，负责更新Node的iptables，让Kubernetes中不在该Node的其他物件可以得知该Node上所有Pods的最新状态kube-apiserver管理整个Kubernetes所需API的接口（Endpoint），例如从CommandLine下kubectl指令就会把指令送到这里负责Node之间的沟通桥梁，每个Node彼此不能直接沟通，必须要透过apiserver转介负责Kubernetes中的请求的身份认证与授权etcd用来存放KubernetesCluster的资料作为备份，当Master因为某些原因而故障时，我们可以透过etcd帮我们还原Kubernetes的状态kube-controller-manager负责管理并运行Kubernetescontroller的组件，简单来说controller就是Kubernetes里一个个负责监视Cluster状态的Process，例如：NodeController、ReplicationController这些Process会在Cluster与预期状态（desirestate）不符时尝试更新现有状态（currentstate）。例如：现在要多开一台机器以应付突然增加的流量，那我的预期状态就会更新成N+1，现有状态为N，这时相对应的controller就会想办法多开一台机器controller-manager的监视与尝试更新也都需要透过访问kube-apiserver达成kube-scheduler整个Kubernetes的Pods调度员，scheduler会监视新建立但还没有被指定要跑在哪个Node上的Pod，并根据每个Node上面资源规定、硬体限制等条件去协调出一个最适合放置的Node让该Pod跑ClusterKubernetes中多个Node与Master的集合。基本上可以想成在同一个环境里所有Node集合在一起的单位。基本运作与安装因原文为外网才能访问所以转载原文链接","link":"https://chriswsq.github.io/post/kubernetes-ji-chu-xue-xi-yi-yuan-li/"},{"title":"k8s——apiVersion对照表","content":"不同的控制器选用不同的apiVersion对照表kindapiVersionCertificateSigningRequestcertificates.k8s.io/v1beta1ClusterRoleBindingrbac.authorization.k8s.io/v1ClusterRolerbac.authorization.k8s.io/v1ComponentStatusv1ConfigMapv1ControllerRevisionapps/v1CronJobbatch/v1beta1DaemonSetextensions/v1beta1Deploymentextensions/v1beta1Endpointsv1Eventv1HorizontalPodAutoscalerautoscaling/v1Ingressextensions/v1beta1Jobbatch/v1LimitRangev1Namespacev1NetworkPolicyextensions/v1beta1Nodev1PersistentVolumeClaimv1PersistentVolumev1PodDisruptionBudgetpolicy/v1beta1Podv1PodSecurityPolicyextensions/v1beta1PodTemplatev1ReplicaSetextensions/v1beta1ReplicationControllerv1ResourceQuotav1RoleBindingrbac.authorization.k8s.io/v1Rolerbac.authorization.k8s.io/v1Secretv1ServiceAccountv1Servicev1StatefulSetapps/v1apiVersion分别是什么意思？名称中带有'alpha'的alphaAPI版本是Kubernetes中新功能的早期候选者。这些可能包含错误，并且不能保证将来能够正常工作。测试在API版本名称意味着测试已取得进展过去的阿尔法水平，并且该功能最终将被列入Kubernetes“测试”。尽管其工作方式可能会发生变化，并且对象的定义方式可能会发生完全变化，但功能本身很有可能以某种形式将其纳入Kubernetes。稳定名称中不包含“alpha”或“beta”。它们是安全使用的。v1这是KubernetesAPI的第一个稳定版本。它包含许多核心对象。apps/v1apps是Kubernetes中最常见的API组，其中许多核心对象均来自v1。它包括与在Kubernetes上运行应用程序相关的功能，例如Deployments，RollingUpdates和ReplicaSets。autoscaling/v1此API版本允许根据不同的资源使用量指标对pod进行自动缩放。此稳定版本仅支持CPU扩展，但是将来的alpha和beta版本将允许您根据内存使用情况和自定义指标进行扩展。batch/v1的batchAPI组包含与批处理和作业样的任务（而不是应用类一样无限期运行Web服务器的任务）的对象。该apiVersion是这些API对象的第一个稳定版本。batch/v1beta1Beta版本，用于Kubernetes中批处理对象的新功能，特别是包括CronJobs，它使您可以在特定的时间或周期性地运行Jobs。certificate.k8s.io/v1beta1此API版本增加了用于验证网络证书以在群集中进行安全通信的功能。您可以阅读更多有关官方文档的信息。extensions/v1beta1此版本的API包括Kubernetes的许多常用新功能。在此版本中，部署，DaemonSet，副本集和Ingress都进行了重大更改。请注意，在Kubernetes1.6中，其中一些对象已从重新定位extensions到特定的API组（例如apps）。当这些对象移出测试版时，应将它们归入特定的API组，例如apps/v1。使用extensions/v1beta1正变得过时，尝试用在可能情况下，根据您的Kubernetes集群版本的特定API组。policy/v1beta1此apiVersion增加了设置容器中断预算和有关容器安全性的新规则的功能。rbac.authorization.k8s.io/v1此apiVersion包含用于Kubernetes基于角色的访问控制的附加功能。这可以帮助您保护群集。查看官方博客文章。","link":"https://chriswsq.github.io/post/k8s-apiversion-dui-zhao-biao/"},{"title":"NFS网络文件系统部署","content":"FS（NetworkFileSystem），即网络文件系统。NFS服务可以将远程Linux系统上的文件共享资源挂载到本地主机的目录上，从而使用本地主机（Linux客户端）像使用本地资源那样读写远程Linux系统上的共享资源。系统环境主机名称操作系统IP地址NFS服务器Centos764位192.168.40.6NFS客户端Centos764位192.168.40.7NFS服务配置文件的参数参数作用rorw读写root_squash当NFS客户端以root管理员访问时，映射为NFS服务器的匿名用户no_root_squash当NFS客户端以root管理员访问时，映射为NFS服务器的root管理员all_squash无论NFS客户端使用什么账户访问，均映射为NFS服务器的匿名用户sync同时将数据写入到内存与硬盘中，保证不丢失数据async优先将数据写入到内存，然后再写入硬盘；这样效率更高，但可能会丢失数据NFS服务器操作1、安装NFSyum-yinstallnfs-utils2、创建用于NFS共享的目录mkdir/testchown-Rnfsnobody/nfsdata3、编辑NFS的配置文件，添加如下内容注：NFS的配置文件默认是没有内容的vim/etc/exports/nfsdata192.168.40.*(rw,sync,root_squash)4、启动NFS服务，并加入开机启动项NFS服务需要使用RPC（RemoteProcedureCall，远程过程调用）服务将NFS服务器的IP地址和端口号等信息发送给客户端，因此，在启动NFS服务之前，还需要顺带启动rpcbind服务。systemctlstartrpcbindsystemctlenablerpcbindsystemctlstartnfs-serversystemctlenablenfs-server5、查看nfs向rpc注册的端口信息rpcinfo-plocalhost注：下图中用红框括起来的端口号需要防火墙允许6、配置firewalld防火墙，允许nfs和rpc端口firewall-cmd--permanent--add-service=nfsfirewall-cmd--permanent--add-service=mountdfirewall-cmd--permanent--add-port=111/tcpfirewall-cmd--permanent--add-port=111/udpfirewall-cmd--reloadNFS客户端操作1、使用showmount命令查询NFS服务器的远程共享信息表3：showmount命令可用的参数以及作用参数作用-e显示NFS服务器的共享列表-a显示本机挂载的文件资源的情况-v显示版本号查看能否连接到nfs服务showmount-e192.168.40.62、创建挂载目录，并挂载mkdir/nfsdatamount-tnfs192.168.40.6:/nfsdata/nfsdatadf-h3、将挂载信息写入/etc/fstab文件中，以便开机自动挂载vim/etc/fstab192.168.40.6:/nfsdata/nfsdatanfsdefaults004、测试往/nfsdata目录下写入一个文件echo&quot;welcometoxuad.com&quot;&gt;/nfsdata/xuad.txt在NFS服务器上查看/nfsdata目录下是否生成了此文件","link":"https://chriswsq.github.io/post/nfs-wang-luo-wen-jian-xi-tong-bu-shu/"},{"title":"sed&awk笔记","content":"sed&amp;awk提升工作效率的小工具删除行首空格或者tabsed-i's/^[\\t]*//g'file删除行尾空格或者tabsed-i's/[\\t]*$//g'file注释特定行sed-i'/swapfile/s/^/#/'/etc/fstabsed-i'/xvdb/s/^/#/g'/etc/fstabsed-i'/vdb/s/^/#/g'/etc/fstab取消注释sed-i'/swapfile/s/^#//'/etc/fstabsed-i'/xvdb/s/^#//g'/etc/fstabsed-i'/vdb/s/^#//g'/etc/fstab注释未注释行sed-i's/^[^#]/#&amp;/'/var/spool/cron/root首字母大写sed's/\\b[a-z]/\\U&amp;/g'file首字母小写sed's/\\b[a-Z]/\\L&amp;/g'file在包含某个字符的上一行或者下一行插入内容[root@RedHattest]#cattestfilehello[root@RedHattest]#sed-i'/hello/i\\\\up'testfile[root@RedHattest]#cattestfileuphello[root@RedHattest]#sed-i'/hello/a\\\\down'testfile[root@RedHattest]#cattestfileuphellodown[root@RedHattest]#假如有两个关键字hello，那么在每一行上面或者下面都插入内容","link":"https://chriswsq.github.io/post/sedandawk-bi-ji/"},{"title":"xuperchain添加旷工节点","content":"xuperchain通过提案动态添加旷工节点两个方案1.提案替换节点为共识，退出共识，转出金额，退出群组（或不退出）就可以下节点了2.拷贝公私钥和数据，停止就节点，启动新节点因为创建链是一次性的操作，那么后续需要改动关于xuper.json里面的参数则要通过进行提案来进行修改一共两个步骤发起提案投票编写提案文件首先查看当前块高度，因为提案文件的投票截止高度和生效高度是根据目前高度来写的./xchain-clistatus-H192.168.40.6:37101|jq'.blockchains[]|{&quot;name&quot;:.name,&quot;height&quot;:.ledger.trunkHeight}'{&quot;name&quot;:&quot;xuper&quot;,&quot;height&quot;:26505}首先需要准备一个提案的文件，json格式proposal.json{&quot;module&quot;:&quot;proposal&quot;,&quot;method&quot;:&quot;Propose&quot;,&quot;args&quot;:{&quot;min_vote_percent&quot;:51,&quot;stop_vote_height&quot;:26600},&quot;trigger&quot;:{&quot;height&quot;:26630,&quot;module&quot;:&quot;consensus&quot;,&quot;method&quot;:&quot;update_consensus&quot;,&quot;args&quot;:{&quot;name&quot;:&quot;tdpos&quot;,&quot;config&quot;:{&quot;version&quot;:&quot;21&quot;,&quot;proposer_num&quot;:&quot;3&quot;,&quot;period&quot;:&quot;3000&quot;,&quot;alternate_interval&quot;:&quot;6000&quot;,&quot;term_interval&quot;:&quot;9000&quot;,&quot;block_num&quot;:&quot;20&quot;,&quot;vote_unit_price&quot;:&quot;1&quot;,&quot;init_proposer&quot;:{&quot;1&quot;:[&quot;2B1rDQhq7W4TStSHoD88N1SUYXrCDV821v&quot;,&quot;rwGpYwpkcpMgxdGJ9KX9xSvJPiCyPsFVQ&quot;,&quot;262G4VuXBmFg6W486XqY4bj2iMotHG5ypb&quot;]}}}}}注意：提案文件里不能有注释需要注意的是当前的区块高度，来设置合理的截至计票高度和生效高度转账然后在矿工节点下，执行给自己转账的操作，并在--desc参数里传入提案./xchain-clitransfer--todpzuVdosQrF2kmzumhVeFQZa1aYcdgFpN--descproposal.json--amount1运行后会得到本次提案的交易id，需要记录下来供投票使用转账地址写当前天的节点就可以，这里的交易金额写1也行，官方是100投票对提案进行投票操作由如下命令执行：查看当前总金额./xchain-clistatus|grep-A10xuper|greputxoTotal10000000523#查看本节点账户余额./xchain-cliaccountbalance--keysdata/keys-H127.0.0.1:37101#查看本节点账户余额./xchain-cliaccountbalance--keysdata/keys-H127.0.0.1:37101./xchain-clivotef26d670b695d9fd5da503a34d130ef19e738b35e031b18b70ad4cbbf6dfe2656--frozen26650--amount100002825031900000000注意：--frozen参数的冻结高度大于提案生效的高度，也就是大于26630。这里需要注意进行投票的节点需要有矿工账号的密钥对因为最终通过的规则是投票资源大于总资源的51%，所以需要初始token量最多的矿工账号来进行投票，并保证token数符合要求。--amount金额为大于总金额的51%查看状态待到当前块到生效高度时查看当前tdpos算法的状态[root@test-1output]#./xchain-clitdposstatus{&quot;term&quot;:21,&quot;block_num&quot;:4,&quot;proposer&quot;:&quot;rwGpYwpkcpMgxdGJ9KX9xSvJPiCyPsFVQ&quot;,&quot;proposer_num&quot;:3,&quot;checkResult&quot;:[&quot;2B1rDQhq7W4TStSHoD88N1SUYXrCDV821v&quot;,&quot;rwGpYwpkcpMgxdGJ9KX9xSvJPiCyPsFVQ&quot;,&quot;262G4VuXBmFg6W486XqY4bj2iMotHG5ypb&quot;]}注意：刚到投票生效的时间执行此命令查看的时候可能会有报错情况[root@test-1output]#./xchain-clitdposstatusrpcerror:code=Unknowndesc=leveldb:notfound等一会就好了","link":"https://chriswsq.github.io/post/xuperchain-tian-jia-kuang-gong-jie-dian/"},{"title":"生产xuperchain部署文档","content":"百度区块链部署xuperchain部署信息**版本：**3.9.0**加密方式：**国密**共识：**tdpos**链类型：**联盟链1.环境准备XuperChain主要由Golang开发，需要首先准备编译运行的环境安装go语言编译环境，版本为1.11或更高，此次此时版本go为1.12下载地址：golang安装git下载地址：git1.1.编译XuperChain使用git下载源码到本地gitclone-bv3.9https://github.com/xuperchain/xuperchain.git执行命令gitclone-bv3.8https://github.com/xuperchain/xuperchain.gitcdxuperchainmake以下为初始化建链前的一次性操作（建链后再修改需要进提案），修改完后创建xuper链，并将ouput目录拷贝至其他节点，后续其他节点根据情况修改个别需要节点唯一的内容即可（keys的三个文件,netURL地址）修改/output/conf/plugins.conf文件(将default的加密方式改为国密方式)修改/output/conf/xchain.yaml文件#在utxo区域中添加`nonUtxo:true`参数utxo:nonUtxo:true##wasm合约配置wasm:driver:&quot;xvm&quot;enableUpgrade:true#能够创建平行链的节点kernel:#minNewChainAmount设置创建平行链时最少要转多少钱到同链名addressminNewChainAmount:&quot;10&quot;newChainWhiteList:-25nbSZeSjMs8GT4TiSmgofnRusg1rxxggV:true#是否开启默认的XEndorser背书服务enableXEndorser:true修改/output/data/config/xuper.json文件，拷贝至其他节点（选择挖矿节点、挖矿节点数量、address地址为主节点即可、更改时间戳、挖矿节点netURL)注意：此模板文件节点之间必须一致，否则会出现块高不一致的情况maxblocksize、proposer_num、period、alternate_interval、term_interval、block_num参数根据实际情况修改，我这里是测试环境的配置拷贝至其他节点后的操作拷贝时确保主节点模板文件没有问题，不会再更改，这样拷贝过去后，不需要重新建链，如改动/output/data/config/xuper.json文件后，需要重新生成链，重新生成neturl地址(生成后的地址IP替换为实际IP)./xchain-clinetURLgen重新生成address地址（生产环境为开发提供每个节点的address地址）./xchain-cliaccountnewkeys-f修改xchain.yaml文件p2p区域的netURL地址，连接主节点（主节点开始先别连接其他节点，待环境部署好后可停止刚开始的主节点然后添加其他节点，使全部节点互相备份）启动服务#删除data/blockchain/*文件rm-rfdata/blockchain/*#创建链./xchain-clicreateChain#启动服务节点nohup./xchain&amp;注如果是单台机器启动多节点需修改三个端口（RPC、metricPort、p2p）先启动bootnode节点先修改插件的默认加密方式为国密后，手动生成的私有公钥和秘钥都会以国密的加密方式生成。创建合约账号如何要支持群组，需要在xuper链部署一个系统合约：GroupChain（一个网络有且仅有一个）#在xuper链部署GroupChain合约#拷贝group_chain.wasm文件到outpue目录(group_chain.wasm文件由开发提供)./xchain-cliaccountnew--account1111111111111111./xchain-cliwasmdeploy--accountXC1111111111111111@xuper--cnamegroup_chain./group_chain.wasm./xchain-cliaccountcontracts--accountXC1111111111111111@xuper-H127.0.0.1:37101创建平行链此步骤在测试阶段可手动进行测试，正式环境集成在程序内创建平行链应的节点应和xuper.json文件中指定地址、xchain.conf文件中允许创建平行链的地址一致#创建群组./xchain-cliwasminvokegroup_chain--methodaddChain-a'{&quot;bcname&quot;:&quot;xchain_chriswang&quot;}'#添加节点(可在网上https://www.bejson.com中检查；可添加多个节点)./xchain-cliwasminvokegroup_chain--methodaddNode-a'{&quot;bcname&quot;:&quot;baidu_zhengqi3&quot;,&quot;ip&quot;:&quot;/ip4/192.168.52.4/tcp/40001/p2p/QmRUVTKqdYVE49VzQKEuGsr2afiwDkj1RMMtdcVdaaqLUX&quot;,&quot;address&quot;:&quot;qaXhH7gJcdfpapmWkbHdLNqUFq3Vst6Am&quot;}'#查看node节点，检查是否添加成功./xchain-cliwasmquerygroup_chain--methodlistNode-a'{&quot;bcname&quot;:&quot;xchain_chriswang&quot;}'#创建平行链#创建平行链的json文件（模版），如下：{&quot;Module&quot;:&quot;kernel&quot;,&quot;Method&quot;:&quot;CreateBlockChain&quot;,&quot;Args&quot;:{&quot;name&quot;:&quot;xchain_chriswsq&quot;,&quot;data&quot;:&quot;{\\&quot;version\\&quot;:\\&quot;1\\&quot;,\\&quot;predistribution\\&quot;:[{\\&quot;address\\&quot;:\\&quot;266L6fw9rBXSm4uBciLwRWTkonb2HCqz5a\\&quot;,\\&quot;quota\\&quot;:\\&quot;100000000000000000000\\&quot;}],\\&quot;maxblocksize\\&quot;:\\&quot;128\\&quot;,\\&quot;award\\&quot;:\\&quot;1000000\\&quot;,\\&quot;decimals\\&quot;:\\&quot;8\\&quot;,\\&quot;award_decay\\&quot;:{\\&quot;height_gap\\&quot;:31536000,\\&quot;ratio\\&quot;:1},\\&quot;gas_price\\&quot;:{\\&quot;cpu_rate\\&quot;:1000,\\&quot;mem_rate\\&quot;:1000000,\\&quot;disk_rate\\&quot;:1,\\&quot;xfee_rate\\&quot;:1},\\&quot;new_account_resource_amount\\&quot;:1000,\\&quot;crypto\\&quot;:\\&quot;gm\\&quot;,\\&quot;genesis_consensus\\&quot;:{\\&quot;name\\&quot;:\\&quot;tdpos\\&quot;,\\&quot;config\\&quot;:{\\&quot;timestamp\\&quot;:\\&quot;1559021720000000000\\&quot;,\\&quot;proposer_num\\&quot;:\\&quot;1\\&quot;,\\&quot;period\\&quot;:\\&quot;3000\\&quot;,\\&quot;alternate_interval\\&quot;:\\&quot;3000\\&quot;,\\&quot;term_interval\\&quot;:\\&quot;6000\\&quot;,\\&quot;block_num\\&quot;:\\&quot;20\\&quot;,\\&quot;vote_unit_price\\&quot;:\\&quot;1\\&quot;,\\&quot;init_proposer\\&quot;:{\\&quot;1\\&quot;:[\\&quot;266L6fw9rBXSm4uBciLwRWTkonb2HCqz5a\\&quot;]},\\&quot;init_proposer_neturl\\&quot;:{\\&quot;1\\&quot;:[\\&quot;/ip4/127.0.0.1/tcp/47101/p2p/QmP38zphQmThRkK5DtiR7iN2ERDiZ1YZF5aXgJHjszj79t\\&quot;]}}}}&quot;}}使用如下指令即可创建平行链,转了100个主链的token到平行链同名的address，作为创建链的代价：./xchain-clitransfer--tobaidu_zhengqi3--amount100--descxchain_chriswang.json02a19467620d01cea01242c94442a39e550e8ae30bc92eaffb341aeaf5f0fa66基本命令#创建普通用户,包含地址，公钥，私钥./xchain-cliaccountnewkeys-f#获取本地netURL地址./xchain-clinetURLget-H127.0.0.1:37101#重新生成本地节点的网络私钥./xchain-clinetURLgen#显示本地节点的p2p地址./xchain-clinetURLpreview#创建xuper链./xchain-clicreateChain#check服务运行状况./xchain-clistatus-H127.0.0.1:37101#查看块高确保每个节点一致变化./xchain-clistatus-H127.0.0.1:37101|jq'.blockchains[]|{&quot;name&quot;:.name,&quot;height&quot;:.ledger.trunkHeight}'#查看本节点账户余额./xchain-cliaccountbalance--keysdata/keys-H127.0.0.1:37101#查看链总账户余额（要注意如果多个链可能会有多个utxoTotal）./xchain-clistatus|greputxoTotal#查看tdpos共识的状态./xchain-clitdposstatus#--keys从此地址转给--to地址--amount钱./xchain-clitransfer--toczojZcZ6cHSiDVJ4jFoZMB1PjKnfUiuFQ--amount10--keysdata/keys/-H127.0.0.1:37101#可查询上一步生成的txid的交易信息./xchain-clitxquerycbbda2606837c950160e99480049e2aec3e60689a280b68a2d253fdd8a6ce931-H127.0.0.1:37101#查询踹节点信息./xchain-cliblock-N22#可查询上一步交易所在的blockid信息./xchain-cliblock0354240c8335e10d8b48d76c0584e29ab604cfdb7b421d973f01a2a49bb67fee-H127.0.0.1:37101#看余额./xchain-cliaccountbalance转账账户地址-H127.0.0.1:37101#看冻结金额./xchain-cliaccountbalance-Z转账账户地址-H127.0.0.1:37101#看utxo列表./xchain-cliutxolist-A转账账户地址-N10-H127.0.0.1:37101所有xchain-cli命令默认都是只xuper链上执行操作，如需操作其他平行链需加--name参数","link":"https://chriswsq.github.io/post/sheng-chan-xuperchain-bu-shu-wen-dang/"},{"title":"ansible学习","content":"⏰ansible自动化运维工具，可以同时对多台主机进行管理，提升工作效率。在此记录工作中用到的东西ansible基本小知识组件inventoryansible的hosts文件是存放被管理主机的，被管理主机比较少的情况下，直接在hosts中定义即可，但是以后很定会管理多台主机，而ansible可管理的主机集合就叫做inventory。在ansible中，描述你主机的默认方法是将它们列在一个文本文件中,这个文件叫inventory文件。此时可以把不同分类的机器放在不同的inventory中，达到清晰管理的目的；配置文件：/etc/ansible/ansible.cfg配置参数：inventory=目录相关inventory文件都放在/etc/ansible/inventory目录下配置实例：[root@test-1~]#grep&quot;^inventory&quot;/etc/ansible/ansible.cfginventory=/etc/ansible/inventory[root@test-1~]#ls/etc/ansible/inventory/cacjqcontext.xmlfiscosigngateway_Mguangdongpeer1peer2peer3webasenodeyc注：inventory配置的是目录，此目录所有文件都会生效ventory（清单）的使用规则（定义主机和组）#“#”开头的行表示该行为注释行，即当时行的配置不生效。#Inventory（清单）可以直接为IP地址192.168.1.7#Inventory（清单）同样支持Hostname（主机名）的方式，后跟冒号加数字表示端口号，默认22号端口ntp.magede.com：22nfs.magede.com：22#中括号内的内容表示一个分组的开始，紧随其后的主机均属于该组成员，空行后的主机亦属于该组，即web2.magedu.com这台主机也属于[webservers]组。[webservers]web1.magedu.comweb[10:20].magedu.com#[10:20]表示10~20之间的所有数字（包括10和20），即表示web10.magedu.com、web11.magedu.com.................web20.magedu.com的所有主机。web2.magedu.com[dbservers]db-a.magedu.comdb-[b:f].magedu.com#[b:f]表示b到f之间的所有数字（包括b和f），即表示db-b.magedu.com、db-c.magedu.com..........db-f.magedu.com的所有主机。定义主机变量在平时工作中，通常会遇到非标准化的需求配置，如考虑到安全性问题，业务人员通常将企业内部的web服务80端口修改为其他端口号，而该功能可以直接通过修改Inventory（清单）配置来实现，在定义主机时为其添加主机变量，以便在Playbook中使用针对某一主机的个性化要求。例如：[webservers]web1.magedu.comhttp_port=808ansible_host=11.111.111.111ansible_ssh_port=22name=shanghai#自定义http_port的端口号为808、主机ip为11.111.111.111、ssh端口为22、定义变量name为shanghai定义组变量Ansible支持定义组变量，主要是针对大量机器的变量定义需求，赋予指定组内所有主机在Playbook中可用的变量，等同于逐一给该组下的所有主机赋予同一变量。例如：[groupservers]web1.magedu.comweb2.magedu.com[groupservers:vars]ntp_server=ntp.magedu.com#定义groupservers组中所有主机ntp_server值为ntp.magedu.comnfs_server=nfs.magedu.com#定义groupservers组中的所有主机nfs_server值为nfs.magedu.com其他Inventory（清单）参数列表除了支持如上的功能外，Ansible基于SSH连接Inventory（清单）中指定的远程主机时，还内置了很多其他参数，用于指定其交互方式。下面列举了部分重要参数：ansible_ssh_host：指定连接主机ansible_ssh_port，指定SSH连接端口，默认22ansible_ssh_user：指定SSH连接用户ansible_ssh_pass：指定SSH连接密码ansible_sudo_pass：指定SSH连接时sudo密码ansible_ssh_private_key_file：指定特有私钥文件","link":"https://chriswsq.github.io/post/ansible-xue-xi/"}]}