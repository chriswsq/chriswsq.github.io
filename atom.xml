<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://chriswsq.github.io</id>
    <title>chris&apos;wang</title>
    <updated>2020-09-24T11:39:11.637Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://chriswsq.github.io"/>
    <link rel="self" href="https://chriswsq.github.io/atom.xml"/>
    <subtitle>当你觉得无所事事时，那你就是在虚度光阴</subtitle>
    <logo>https://chriswsq.github.io/images/avatar.png</logo>
    <icon>https://chriswsq.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, chris&apos;wang</rights>
    <entry>
        <title type="html"><![CDATA[xuperchian  建链未成功手动修复]]></title>
        <id>https://chriswsq.github.io/post/xuperchian change Common node/</id>
        <link href="https://chriswsq.github.io/post/xuperchian change Common node/">
        </link>
        <updated>2020-09-18T03:04:41.000Z</updated>
        <content type="html"><![CDATA[<!-- more -->
<p>从idc服务器日志中找到 链名的日志  会有创建该链的json文件</p>
<p>#手动修复</p>
<pre><code>#step 1. 创建群组
./xchain-cli wasm invoke group_chain --method addChain -a '{&quot;bcname&quot;:&quot;CounterChain1&quot;}'

#step 2. 添加节点
./xchain-cli wasm invoke group_chain --method addNode -a '{&quot;bcname&quot;:&quot;CounterChain1&quot;, &quot;ip&quot;:&quot;/ip4/127.0.0.1/tcp/47101/p2p/QmVxeNubpg1ZQjQT8W5yZC9fD7ZB1ViArwvyGUB53sqf8e&quot;, 

#step 3. 创建平行链
./xchain-cli transfer --to CounterChain1 --amount 100 --desc createCounterChain1.json


</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[xuperchian 增减共识节点、添加群组操作步骤]]></title>
        <id>https://chriswsq.github.io/post/xuperchian-change-Common-node/</id>
        <link href="https://chriswsq.github.io/post/xuperchian-change-Common-node/">
        </link>
        <updated>2020-09-15T11:19:03.000Z</updated>
        <content type="html"><![CDATA[<h1 id="加入群组">加入群组</h1>
<h2 id="查看当前群组节点">查看当前群组节点</h2>
<pre><code class="language-bash">./xchain-cli wasm query group_chain --method listNode -a '{&quot;bcname&quot;:&quot;app000120200824165811835422&quot;}'
</code></pre>
<h2 id="节点加入群组">节点加入群组</h2>
<pre><code class="language-bash">./xchain-cli wasm invoke group_chain --method addNode -a '{&quot;bcname&quot;:&quot;app000120200824165811835422&quot;, &quot;ip&quot;:&quot;/ip4/192.168.40.6/tcp/40001/p2p/QmY7UxwcLpD8QK8p6gAk99xuz3Q16jcaCoU8iMokVWYa83&quot;, &quot;address&quot;:&quot;2B1rDQhq7W4TStSHoD88N1SUYXrCDV821v&quot;}'
</code></pre>
<blockquote>
<p>将bcname 、ip 、 address改为实际的参数即可</p>
</blockquote>
<h1 id="更改共识节点">更改共识节点</h1>
<h2 id="查询金额">查询金额</h2>
<pre><code class="language-bash">./xchain-cli account balance zALW2YRp55LFuro1uAjgfoGTqCjgz3nHc --name app000120200824165811835422
</code></pre>
<h2 id="查看块高">查看块高</h2>
<pre><code class="language-bash">./xchain-cli status -H 127.0.0.1:37101 | jq '.blockchains[] | {&quot;name&quot;:.name,&quot;height&quot;:.ledger.trunkHeight}' | grep -A 1 app000120200824165811835422
</code></pre>
<h2 id="提案">提案</h2>
<pre><code class="language-bash">./xchain-cli transfer --to scocy5ZTaFykhRGGYxN9KSEkpxCD1cd72 --desc proposal_app000120200824165811835422.json  --amount 1  --name app000120200824165811835422

ae074dc967e0a03b81346a8e4796b01fe15b51472d3ba34f191d6b08e24a1918
</code></pre>
<h2 id="投票">投票</h2>
<pre><code class="language-bash">./xchain-cli vote ae074dc967e0a03b81346a8e4796b01fe15b51472d3ba34f191d6b08e24a1918 --frozen 138160 --amount 52000000071795360000   --name app000120200824165811835422

74217bf729af8fc1b6acfbc5f953fbda029fb9078360ec301dfc113e690ac016
</code></pre>
<h2 id="查看交易内容">查看交易内容</h2>
<pre><code class="language-bash">./xchain-cli tx query  74217bf729af8fc1b6acfbc5f953fbda029fb9078360ec301dfc113e690ac016
</code></pre>
<h2 id="查看共识节点">查看共识节点</h2>
<pre><code class="language-bash">./xchain-cli tdpos status --name   app000120200824165811835422
</code></pre>
<h1 id="提案内容">提案内容</h1>
<pre><code class="language-json">{
    &quot;module&quot;: &quot;proposal&quot;,
    &quot;method&quot;: &quot;Propose&quot;,
    &quot;args&quot; : {
        &quot;min_vote_percent&quot;: 51,
        &quot;stop_vote_height&quot;: 688970
    },
    &quot;trigger&quot;: {
        &quot;height&quot;: 688980,
        &quot;module&quot;: &quot;consensus&quot;,
        &quot;method&quot;: &quot;update_consensus&quot;,
        &quot;args&quot; : {
            &quot;name&quot;: &quot;tdpos&quot;,
            &quot;config&quot;: {
                &quot;version&quot;:&quot;20&quot;,
                &quot;proposer_num&quot;:&quot;2&quot;,
                &quot;period&quot;:&quot;5000&quot;,
                &quot;alternate_interval&quot;:&quot;5000&quot;,
                &quot;term_interval&quot;:&quot;10000&quot;,
                &quot;block_num&quot;:&quot;720&quot;,
                &quot;vote_unit_price&quot;:&quot;1&quot;,
                &quot;init_proposer&quot;: {
                    &quot;1&quot;:[&quot;scocy5ZTaFykhRGGYxN9KSEkpxCD1cd72&quot;, &quot;qaXhH7gJcdfpapmWkbHdLNqUFq3Vst6Am&quot;]
                }
            }
        }
    }
}
</code></pre>
<blockquote>
<p>注意： 以上将 address、p2p、proposer_num等改为实际数据即可<br>
命令添加群组，一次性添加两个以上群组 不生效,须一个一个添加<br>
用命令行创建平行链，如果命令写错不报命令的错误，而会报其他的问题    例如 地址不在白名单<br>
后续加入群组的节点，需要重启才会同步块数据<br>
在进行提案的时候经常会导致xuper链或其他调整的链快高不一致的情况，这时候重启解决问题<br>
如果同一个链既要增加节点又要删减节点，那么是需要分开提案来做的</p>
</blockquote>
<pre><code class="language-bash">new
pJsfQecriScf4ZA6MHhjEpMMAiBbct5Sv
24rqLhCMozBJrmsXhtrR68wLAr72zDfuEL
qJV7qfGdf2GAZUcrx6v71ahQ69nYYWGpa


huainan2
app0001202007302056480751681 
app0001202007310000590617583
app0001202008011331415497000

2A8cTP6dFjKoZPXyCx7T5APbQKCUzbgBsx
pecX9eVDd368J3GBfdPNtVUgnTu4nvD7b
28HsYtaS1p7DqZ3QqXzCSsEk47dX4seU6o


</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[docker容器保持后台运行的两种方式]]></title>
        <id>https://chriswsq.github.io/post/docker-rong-qi-bao-chi-hou-tai-yun-xing-de-liang-chong-fang-shi/</id>
        <link href="https://chriswsq.github.io/post/docker-rong-qi-bao-chi-hou-tai-yun-xing-de-liang-chong-fang-shi/">
        </link>
        <updated>2020-09-14T08:39:50.000Z</updated>
        <content type="html"><![CDATA[<!-- more -->
<p>有的服务支持服务后台运行，那么在用docker启动服务时可以使用前台运行的命令，除此之外一些服务器是不支持前台运行，那么就要靠其他方式使用进程挂在前台。</p>
<ul>
<li>方法1 ，run一个容器：</li>
</ul>
<pre><code class="language-bash">[root@localhost ~]# docker run -dit --hostname centos --name centos --restart always a8493f5f50ff /bin/bash
 
-dit 是后台运行、交互模式、分配终端，容器启动后不会退出
 如果没有it参数，run 一个容器以后，docker ps -a 容器状态就exited了
--restart always 容器可以随docker服务启动而启动
</code></pre>
<ul>
<li>方法2， run 一个容器，并一直发送ping包</li>
</ul>
<pre><code class="language-bash">[root@localhost ~]# docker run -d --name test --hostname test a8493f5f50ff ping 127.0.0.1
</code></pre>
<p>这样容器就一直在发送ping包，不会退出。<br>
意思就是然容器一直运行一个程序，不退出，容器就不会停止</p>
<p>本文链接地址 https://www.rootop.org/pages/3736.html</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[记一次服务器内存过高问题]]></title>
        <id>https://chriswsq.github.io/post/ji-yi-ci-fu-wu-qi-nei-cun-guo-gao-wen-ti/</id>
        <link href="https://chriswsq.github.io/post/ji-yi-ci-fu-wu-qi-nei-cun-guo-gao-wen-ti/">
        </link>
        <updated>2020-09-10T09:40:31.000Z</updated>
        <content type="html"><![CDATA[<!-- more -->
<p>监控系统发邮件告警，一台服务器内存过高，已达 90% 以上，之前这台机器内存就高到85%以上，上去看了下服务正常 top 指令看了下没什么太高的进程占用，就重启了下上面的服务（前提是上面的服务重启是没什么影响的）。</p>
<p>今天有来告警了，内存占用率已经到90%，照常上去看了下<br>
使用 free -m 查看内存，一共是16G  已经用了 13G<br>
top  查看进程占用资源情况，发现最高的应用占用也不过百分之五左右</p>
<p>怀疑是被攻击了，看了下进程  ps  aux<br>
发现进程很多   ps aux |wc  -l    进程都6000多了，大部分的进程都是 sshd: root@notty  的进程，网上查了下这是被 ssh 暴力破解了，很奇怪，端口号改了不是22 了 ，攻击者怎么知道的呢。。。。</p>
<p>先解决问题吧</p>
<pre><code class="language-bash"> for i in  `ps  aux | grep root@notty | more | awk  '{print $2}'` ;do kill $i ;done
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[docker部署Prometheus监控服务器及容器并发送告警]]></title>
        <id>https://chriswsq.github.io/post/docker-bu-shu-prometheus-jian-kong-fu-wu-qi-ji-rong-qi-bing-fa-song-gao-jing/</id>
        <link href="https://chriswsq.github.io/post/docker-bu-shu-prometheus-jian-kong-fu-wu-qi-ji-rong-qi-bing-fa-song-gao-jing/">
        </link>
        <updated>2020-09-03T02:44:39.000Z</updated>
        <summary type="html"><![CDATA[<p>记录一下利用prometheus监控服务器信息和容器服务并发送邮件告警</p>
]]></summary>
        <content type="html"><![CDATA[<p>记录一下利用prometheus监控服务器信息和容器服务并发送邮件告警</p>
<!-- more -->
<h2 id="基本原理">基本原理</h2>
<p>Prometheus的基本原理是通过HTTP协议周期性抓取被监控组件的状态，任意组件只要提供对应的HTTP接口就可以接入监控。不需要任何SDK或者其他的集成过程。这样做非常适合做虚拟化环境监控系统，比如VM、Docker、Kubernetes等。输出被监控组件信息的HTTP接口被叫做exporter 。目前互联网公司常用的组件大部分都有exporter可以直接使用，比如Varnish、Haproxy、Nginx、MySQL、Linux系统信息(包括磁盘、内存、CPU、网络等等)。</p>
<h2 id="相关组件">相关组件</h2>
<ul>
<li>Prometheus: Prometheus Daemon负责定时去目标上抓取metrics(指标)数据，每个抓取目标需要暴露一个http服务的接口给它定时抓取。</li>
<li>Grafana: 接入prometheus数据，图形化展示监控信息</li>
<li>Node-exporter: 负责收集 host 硬件和操作系统数据。它将以容器方式运行在所有 host 上。</li>
<li>Cadvisor: 负责收集容器数据。它将以容器方式运行在所有 host 上。</li>
<li>Alertmanager: 警告管理器，用来进行报警。</li>
</ul>
<table>
<thead>
<tr>
<th>主机名</th>
<th>ip</th>
<th>服务</th>
</tr>
</thead>
<tbody>
<tr>
<td>test-1</td>
<td>192.168.40.6</td>
<td>cadvisor、node-exporter、grafana、prometheus、alertmanager.yml</td>
</tr>
<tr>
<td>test-2</td>
<td>192.168.40.7</td>
<td>node-exporter、cadvisor</td>
</tr>
<tr>
<td>test-3</td>
<td>192.168.40.8</td>
<td>node-exporter、cadvisor</td>
</tr>
</tbody>
</table>
<h2 id="安装docker-docker-compose">安装docker、docker-compose</h2>
<h3 id="安装docker">安装docker</h3>
<pre><code class="language-bash"># 安装依赖包
yum install -y yum-utils device-mapper-persistent-data lvm2
# 添加Docker软件包源
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
# 安装Docker CE
yum install docker-ce -y
# 启动
systemctl start docker
# 开机启动
systemctl enable docker
# 查看Docker信息
docker info
</code></pre>
<h3 id="安装docker-compose">安装docker-compose</h3>
<pre><code class="language-bash">curl -L https://github.com/docker/compose/releases/download/1.23.2/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose
chmod +x /usr/local/bin/docker-compose
</code></pre>
<h2 id="添加配置文件">添加配置文件</h2>
<pre><code class="language-bash">mkdir -p /usr/local/src/config
cd /usr/local/src/config
</code></pre>
<h3 id="添加prometheusyml配置文件">添加prometheus.yml配置文件</h3>
<p>vim prometheus.yml</p>
<pre><code class="language-bash"># my global config
global:
  scrape_interval: 15s
  evaluation_interval: 15s

alerting:
  alertmanagers:
  - static_configs:
    - targets:
       - 192.168.40.6:9093

rule_files:
  - &quot;/etc/prometheus/config/rule/*rule.yml&quot;

scrape_configs:
  - job_name: 'prometheus'
    scrape_interval: 5s
    static_configs:
        - targets: ['192.168.40.6:9090']

  - job_name: 'cadvisor'
    scrape_interval: 5s
    static_configs:
        - targets: ['192.168.40.6:8080', '192.168.40.7:8080', '192.168.40.8']

  - job_name: 'node-exporter'
    scrape_interval: 5s
    static_configs:
        - targets: ['192.168.40.6:9100']
          labels:
            instance: test-1 - 192.168.40.6
            service: node-service
        - targets: ['192.168.40.7:9100']
          labels:
            instance: test-2 - 192.168.40.7
            service: node-service
        - targets: ['192.168.40.8:9100']
          labels:
            instance: test-3 - 192.168.40.8
            service: node-service
</code></pre>
<h3 id="添加邮件告警配置文件">添加邮件告警配置文件</h3>
<p>添加配置文件alertmanager.yml，配置收发邮件邮箱<br>
vim alertmanager.yml</p>
<pre><code class="language-bash">global:
  # The smarthost and SMTP sender used for mail notifications.  用于邮件通知的智能主机和SMTP发件人。
  smtp_smarthost: 'smtp.163.com:25'
  smtp_from: 'xxxxxxxxx@163.com'
  smtp_auth_username: 'xxxxxxxxxxxxxxx@163.com'
  smtp_auth_password: 'xxxxxxxxxxxxx'
  # The auth token for Hipchat.    Hipchat的身份验证令牌。

templates:
  - '/etc/alertmanager/default-monitor.tmpl'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 5m
  receiver: 'mail'


receivers:
- name: 'mail'
  email_configs:
  - to: 'xxxxxxxxxxxxxxxx@163.com, xxxxxxxxxxxxxx@qq.com'
    send_resolved: true #告警恢复通知
    html: '{{ template &quot;default-monitor.html&quot; . }}'  #应用那个模板
    headers: { Subject: &quot;[WARN] 报警邮件&quot; } #邮件主题信息 如果不写headers也可以在模板中定义默认加载email.default.subject这个模板
</code></pre>
<h3 id="添加报警规则">添加报警规则</h3>
<pre><code class="language-bash">mkdir -p /usr/local/src/config/rule
cd /usr/local/src/config/rule
</code></pre>
<p>创建两个文件</p>
<p>node-exporter-record-rule.yml<br>
node-exporter-alert-rule.yml</p>
<p>第一个文件用于记录规则，第二个是报警规则。<br>
由于之前我们在prometheus.yml中已经引用了所有已rule结尾的文件，所以我们不用在修改prometheus.yml配置文件。</p>
<p>创建node-exporter-record-rule.yml</p>
<pre><code class="language-bash">groups:
  - name: node-exporter-record
    rules:
    - expr: up{job=~&quot;node-exporter&quot;}
      record: node_exporter:up
      labels:
        desc: &quot;节点是否在线, 在线1,不在线0&quot;
        unit: &quot; &quot;
        job: &quot;node-exporter&quot;
    - expr: time() - node_boot_time_seconds{}
      record: node_exporter:node_uptime
      labels:
        desc: &quot;节点的运行时间&quot;
        unit: &quot;s&quot;
        job: &quot;node-exporter&quot;
##############################################################################################
#                              cpu                                                           #
    - expr: (1 - avg by (environment,instance) (irate(node_cpu_seconds_total{job=&quot;node-exporter&quot;,mode=&quot;idle&quot;}[5m])))  * 100
      record: node_exporter:cpu:total:percent
      labels:
        desc: &quot;节点的cpu总消耗百分比&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;

    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total{job=&quot;node-exporter&quot;,mode=&quot;idle&quot;}[5m])))  * 100
      record: node_exporter:cpu:idle:percent
      labels:
        desc: &quot;节点的cpu idle百分比&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;

    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total{job=&quot;node-exporter&quot;,mode=&quot;iowait&quot;}[5m])))  * 100
      record: node_exporter:cpu:iowait:percent
      labels:
        desc: &quot;节点的cpu iowait百分比&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;


    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total{job=&quot;node-exporter&quot;,mode=&quot;system&quot;}[5m])))  * 100
      record: node_exporter:cpu:system:percent
      labels:
        desc: &quot;节点的cpu system百分比&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;

    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total{job=&quot;node-exporter&quot;,mode=&quot;user&quot;}[5m])))  * 100
      record: node_exporter:cpu:user:percent
      labels:
        desc: &quot;节点的cpu user百分比&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;

    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total{job=&quot;node-exporter&quot;,mode=~&quot;softirq|nice|irq|steal&quot;}[5m])))  * 100
      record: node_exporter:cpu:other:percent
      labels:
        desc: &quot;节点的cpu 其他的百分比&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;
##############################################################################################


##############################################################################################
#                                    memory                                                  #
    - expr: node_memory_MemTotal_bytes{job=&quot;node-exporter&quot;}
      record: node_exporter:memory:total
      labels:
        desc: &quot;节点的内存总量&quot;
        unit: byte
        job: &quot;node-exporter&quot;

    - expr: node_memory_MemFree_bytes{job=&quot;node-exporter&quot;}
      record: node_exporter:memory:free
      labels:
        desc: &quot;节点的剩余内存量&quot;
        unit: byte
        job: &quot;node-exporter&quot;

    - expr: node_memory_MemTotal_bytes{job=&quot;node-exporter&quot;} - node_memory_MemFree_bytes{job=&quot;node-exporter&quot;}
      record: node_exporter:memory:used
      labels:
        desc: &quot;节点的已使用内存量&quot;
        unit: byte
        job: &quot;node-exporter&quot;

    - expr: node_memory_MemTotal_bytes{job=&quot;node-exporter&quot;} - node_memory_MemAvailable_bytes{job=&quot;node-exporter&quot;}
      record: node_exporter:memory:actualused
      labels:
        desc: &quot;节点用户实际使用的内存量&quot;
        unit: byte
        job: &quot;node-exporter&quot;

    - expr: (1-(node_memory_MemAvailable_bytes{job=&quot;node-exporter&quot;} / (node_memory_MemTotal_bytes{job=&quot;node-exporter&quot;})))* 100
      record: node_exporter:memory:used:percent
      labels:
        desc: &quot;节点的内存使用百分比&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;

    - expr: ((node_memory_MemAvailable_bytes{job=&quot;node-exporter&quot;} / (node_memory_MemTotal_bytes{job=&quot;node-exporter&quot;})))* 100
      record: node_exporter:memory:free:percent
      labels:
        desc: &quot;节点的内存剩余百分比&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;
##############################################################################################
#                                   load                                                     #
    - expr: sum by (instance) (node_load1{job=&quot;node-exporter&quot;})
      record: node_exporter:load:load1
      labels:
        desc: &quot;系统1分钟负载&quot;
        unit: &quot; &quot;
        job: &quot;node-exporter&quot;

    - expr: sum by (instance) (node_load5{job=&quot;node-exporter&quot;})
      record: node_exporter:load:load5
      labels:
        desc: &quot;系统5分钟负载&quot;
        unit: &quot; &quot;
        job: &quot;node-exporter&quot;

    - expr: sum by (instance) (node_load15{job=&quot;node-exporter&quot;})
      record: node_exporter:load:load15
      labels:
        desc: &quot;系统15分钟负载&quot;
        unit: &quot; &quot;
        job: &quot;node-exporter&quot;

##############################################################################################
#                                 disk                                                       #
    - expr: node_filesystem_size_bytes{job=&quot;node-exporter&quot; ,fstype=~&quot;ext4|xfs&quot;}
      record: node_exporter:disk:usage:total
      labels:
        desc: &quot;节点的磁盘总量&quot;
        unit: byte
        job: &quot;node-exporter&quot;

    - expr: node_filesystem_avail_bytes{job=&quot;node-exporter&quot;,fstype=~&quot;ext4|xfs&quot;}
      record: node_exporter:disk:usage:free
      labels:
        desc: &quot;节点的磁盘剩余空间&quot;
        unit: byte
        job: &quot;node-exporter&quot;

    - expr: node_filesystem_size_bytes{job=&quot;node-exporter&quot;,fstype=~&quot;ext4|xfs&quot;} - node_filesystem_avail_bytes{job=&quot;node-exporter&quot;,fstype=~&quot;ext4|xfs&quot;}
      record: node_exporter:disk:usage:used
      labels:
        desc: &quot;节点的磁盘使用的空间&quot;
        unit: byte
        job: &quot;node-exporter&quot;

    - expr:  (1 - node_filesystem_avail_bytes{job=&quot;node-exporter&quot;,fstype=~&quot;ext4|xfs&quot;} / node_filesystem_size_bytes{job=&quot;node-exporter&quot;,fstype=~&quot;ext4|xfs&quot;}) * 100
      record: node_exporter:disk:used:percent
      labels:
        desc: &quot;节点的磁盘的使用百分比&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;

    - expr: irate(node_disk_reads_completed_total{job=&quot;node-exporter&quot;}[1m])
      record: node_exporter:disk:read:count:rate
      labels:
        desc: &quot;节点的磁盘读取速率&quot;
        unit: &quot;次/秒&quot;
        job: &quot;node-exporter&quot;

    - expr: irate(node_disk_writes_completed_total{job=&quot;node-exporter&quot;}[1m])
      record: node_exporter:disk:write:count:rate
      labels:
        desc: &quot;节点的磁盘写入速率&quot;
        unit: &quot;次/秒&quot;
        job: &quot;node-exporter&quot;

    - expr: (irate(node_disk_written_bytes_total{job=&quot;node-exporter&quot;}[1m]))/1024/1024
      record: node_exporter:disk:read:mb:rate
      labels:
        desc: &quot;节点的设备读取MB速率&quot;
        unit: &quot;MB/s&quot;
        job: &quot;node-exporter&quot;

    - expr: (irate(node_disk_read_bytes_total{job=&quot;node-exporter&quot;}[1m]))/1024/1024
      record: node_exporter:disk:write:mb:rate
      labels:
        desc: &quot;节点的设备写入MB速率&quot;
        unit: &quot;MB/s&quot;
        job: &quot;node-exporter&quot;

##############################################################################################
#                                filesystem                                                  #
    - expr:   (1 -node_filesystem_files_free{job=&quot;node-exporter&quot;,fstype=~&quot;ext4|xfs&quot;} / node_filesystem_files{job=&quot;node-exporter&quot;,fstype=~&quot;ext4|xfs&quot;}) * 100
      record: node_exporter:filesystem:used:percent
      labels:
        desc: &quot;节点的inode的剩余可用的百分比&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;
#############################################################################################
#                                filefd                                                     #
    - expr: node_filefd_allocated{job=&quot;node-exporter&quot;}
      record: node_exporter:filefd_allocated:count
      labels:
        desc: &quot;节点的文件描述符打开个数&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;

    - expr: node_filefd_allocated{job=&quot;node-exporter&quot;}/node_filefd_maximum{job=&quot;node-exporter&quot;} * 100
      record: node_exporter:filefd_allocated:percent
      labels:
        desc: &quot;节点的文件描述符打开百分比&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;

#############################################################################################
#                                network                                                    #
    - expr: avg by (environment,instance,device) (irate(node_network_receive_bytes_total{device=~&quot;eth0|eth1|ens33|ens37&quot;}[1m]))
      record: node_exporter:network:netin:bit:rate
      labels:
        desc: &quot;节点网卡eth0每秒接收的比特数&quot;
        unit: &quot;bit/s&quot;
        job: &quot;node-exporter&quot;

    - expr: avg by (environment,instance,device) (irate(node_network_transmit_bytes_total{device=~&quot;eth0|eth1|ens33|ens37&quot;}[1m]))
      record: node_exporter:network:netout:bit:rate
      labels:
        desc: &quot;节点网卡eth0每秒发送的比特数&quot;
        unit: &quot;bit/s&quot;
        job: &quot;node-exporter&quot;

    - expr: avg by (environment,instance,device) (irate(node_network_receive_packets_total{device=~&quot;eth0|eth1|ens33|ens37&quot;}[1m]))
      record: node_exporter:network:netin:packet:rate
      labels:
        desc: &quot;节点网卡每秒接收的数据包个数&quot;
        unit: &quot;个/秒&quot;
        job: &quot;node-exporter&quot;

    - expr: avg by (environment,instance,device) (irate(node_network_transmit_packets_total{device=~&quot;eth0|eth1|ens33|ens37&quot;}[1m]))
      record: node_exporter:network:netout:packet:rate
      labels:
        desc: &quot;节点网卡发送的数据包个数&quot;
        unit: &quot;个/秒&quot;
        job: &quot;node-exporter&quot;

    - expr: avg by (environment,instance,device) (irate(node_network_receive_errs_total{device=~&quot;eth0|eth1|ens33|ens37&quot;}[1m]))
      record: node_exporter:network:netin:error:rate
      labels:
        desc: &quot;节点设备驱动器检测到的接收错误包的数量&quot;
        unit: &quot;个/秒&quot;
        job: &quot;node-exporter&quot;

    - expr: avg by (environment,instance,device) (irate(node_network_transmit_errs_total{device=~&quot;eth0|eth1|ens33|ens37&quot;}[1m]))
      record: node_exporter:network:netout:error:rate
      labels:
        desc: &quot;节点设备驱动器检测到的发送错误包的数量&quot;
        unit: &quot;个/秒&quot;
        job: &quot;node-exporter&quot;

    - expr: node_tcp_connection_states{job=&quot;node-exporter&quot;, state=&quot;established&quot;}
      record: node_exporter:network:tcp:established:count
      labels:
        desc: &quot;节点当前established的个数&quot;
        unit: &quot;个&quot;
        job: &quot;node-exporter&quot;

    - expr: node_tcp_connection_states{job=&quot;node-exporter&quot;, state=&quot;time_wait&quot;}
      record: node_exporter:network:tcp:timewait:count
      labels:
        desc: &quot;节点timewait的连接数&quot;
        unit: &quot;个&quot;
        job: &quot;node-exporter&quot;

    - expr: sum by (environment,instance) (node_tcp_connection_states{job=&quot;node-exporter&quot;})
      record: node_exporter:network:tcp:total:count
      labels:
        desc: &quot;节点tcp连接总数&quot;
        unit: &quot;个&quot;
        job: &quot;node-exporter&quot;

#############################################################################################
#                                process                                                    #
    - expr: node_processes_state{state=&quot;Z&quot;}
      record: node_exporter:process:zoom:total:count
      labels:
        desc: &quot;节点当前状态为zoom的个数&quot;
        unit: &quot;个&quot;
        job: &quot;node-exporter&quot;
#############################################################################################
#                                other                                                    #
    - expr: abs(node_timex_offset_seconds{job=&quot;node-exporter&quot;})
      record: node_exporter:time:offset
      labels:
        desc: &quot;节点的时间偏差&quot;
        unit: &quot;s&quot;
        job: &quot;node-exporter&quot;

#############################################################################################

    - expr: count by (instance) ( count by (instance,cpu) (node_cpu_seconds_total{ mode='system'}) )
      record: node_exporter:cpu:count
</code></pre>
<p>创建node-exporter-alert-rule.yml</p>
<pre><code class="language-bash">groups:
  - name: node-exporter-alert
    rules:
    - alert: node-exporter-down
      expr: node_exporter:up == 0
      for: 1m
      labels:
        severity: 'critical'
      annotations:
        summary: &quot;instance: {{ $labels.instance }} 宕机了&quot;
        description: &quot;instance: {{ $labels.instance }} \n- job: {{ $labels.job }} 关机了， 时间已经1分钟了。&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;



    - alert: node-exporter-cpu-high
      expr:  node_exporter:cpu:total:percent &gt; 80
      for: 3m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} cpu 使用率高于 {{ $value }}&quot;
        description: &quot;instance: {{ $labels.instance }} \n- job: {{ $labels.job }} CPU使用率已经持续三分钟高过80% 。&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;

    - alert: node-exporter-cpu-iowait-high
      expr:  node_exporter:cpu:iowait:percent &gt;= 12
      for: 3m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} cpu iowait 使用率高于 {{ $value }}&quot;
        description: &quot;instance: {{ $labels.instance }} \n- job: {{ $labels.job }} cpu iowait使用率已经持续三分钟高过12%&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;


    - alert: node-exporter-load-load1-high
      expr:  (node_exporter:load:load1) &gt; (node_exporter:cpu:count) * 1.2
      for: 3m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} load1 使用率高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;


    - alert: node-exporter-memory-high
      expr:  node_exporter:memory:used:percent &gt; 85
      for: 3m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} memory 使用率高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;


    - alert: node-exporter-disk-high
      expr:  node_exporter:disk:used:percent &gt; 88
      for: 10m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} disk 使用率高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;


    - alert: node-exporter-disk-read:count-high
      expr:  node_exporter:disk:read:count:rate &gt; 3000
      for: 2m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} iops read 使用率高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;


    - alert: node-exporter-disk-write-count-high
      expr:  node_exporter:disk:write:count:rate &gt; 3000
      for: 2m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} iops write 使用率高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;





    - alert: node-exporter-disk-read-mb-high
      expr:  node_exporter:disk:read:mb:rate &gt; 60
      for: 2m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} 读取字节数 高于 {{ $value }}&quot;
        description: &quot;&quot;
        instance: &quot;{{ $labels.instance }}&quot;
        value: &quot;{{ $value }}&quot;


    - alert: node-exporter-disk-write-mb-high
      expr:  node_exporter:disk:write:mb:rate &gt; 60
      for: 2m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} 写入字节数 高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;


    - alert: node-exporter-filefd-allocated-percent-high
      expr:  node_exporter:filefd_allocated:percent &gt; 80
      for: 10m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} 打开文件描述符 高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;


    - alert: node-exporter-network-netin-error-rate-high
      expr:  node_exporter:network:netin:error:rate &gt; 4
      for: 1m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} 包进入的错误速率 高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;

    - alert: node-exporter-network-netin-packet-rate-high
      expr:  node_exporter:network:netin:packet:rate &gt; 35000
      for: 1m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} 包进入速率 高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;


    - alert: node-exporter-network-netout-packet-rate-high
      expr:  node_exporter:network:netout:packet:rate &gt; 35000
      for: 1m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} 包流出速率 高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;


    - alert: node-exporter-network-tcp-total-count-high
      expr:  node_exporter:network:tcp:total:count &gt; 40000
      for: 1m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} tcp连接数量 高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;


    - alert: node-exporter-process-zoom-total-count-high
      expr:  node_exporter:process:zoom:total:count &gt; 10
      for: 10m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} 僵死进程数量 高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;


    - alert: node-exporter-time-offset-high
      expr:  node_exporter:time:offset &gt; 0.03
      for: 2m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} {{ $labels.desc }}  {{ $value }} {{ $labels.unit }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;
</code></pre>
<h3 id="添加告警模板">添加告警模板</h3>
<pre><code class="language-bash">mkdir  template
vim template/default-monitor.tmpl
</code></pre>
<pre><code class="language-tmpl">{{ define &quot;default-monitor.html&quot; }}
{{ range .Alerts }}
=========start==========&lt;br&gt;
告警程序: prometheus_alert &lt;br&gt;
告警级别: {{ .Labels.severity }} 级 &lt;br&gt;
告警类型: {{ .Labels.alertname }} &lt;br&gt;
故障主机: {{ .Labels.instance }} &lt;br&gt;
告警主题: {{ .Annotations.summary }} &lt;br&gt;
告警详情: {{ .Annotations.description }} &lt;br&gt;
触发时间: {{ .StartsAt.Format &quot;2019-08-04 16:58:15&quot; }} &lt;br&gt;
=========end==========&lt;br&gt;
{{ end }}
{{ end }}
</code></pre>
<h2 id="编写docker-compose文件">编写docker-compose文件</h2>
<p>vim    docker-compose-monitor.yml</p>
<pre><code class="language-yaml">version: '2'

networks:
    monitor:
        driver: bridge

services:
  prometheus:
    image: prom/prometheus:v2.16.0
    container_name: prometheus
    restart: always
    ports:
     - 9090:9090
    volumes:
     - /bsn/prometheus/prometheus:/prometheus
     - /bsn/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
     - /bsn/prometheus/alert/alert.rules:/usr/local/prometheus/rules/alert.rules
     - /etc/localtime:/etc/localtime
    command:
     - '--config.file=/etc/prometheus/prometheus.yml'
     - '--storage.tsdb.path=/prometheus/'
     - '--storage.tsdb.retention.time=90d'
    depends_on:
     - alertmanager
  grafana:
    image: grafana/grafana:6.4.2
    container_name: grafana
    restart: always
    volumes:
     - /bsn/prometheus/grafana:/var/lib/grafana
     - /bsn/prometheus/grafana/grafana.ini:/etc/grafana/grafana.ini
     - /etc/localtime:/etc/localtime
    ports:
     - 3000:3000
    depends_on:
     - prometheus
  alertmanager:
    image: prom/alertmanager:v0.21.0-rc.0
    container_name: alertmanager
    volumes:
      - /bsn/prometheus/alert/alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - /bsn/prometheus/alert/email.tmpl:/etc/alertmanager/template/email.tmpl
      - /etc/localtime:/etc/localtime
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
    ports:
      - 9093:9093
    restart: always

    node-exporter:
        image: quay.io/prometheus/node-exporter
        container_name: node-exporter
        hostname: $HOSTNAME
        restart: always
        ports:
            - &quot;9100:9100&quot;
        volumes:
          - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro
          - /proc:/host/proc:ro
          - /sys:/host/sys:ro
          - /:/rootfs:ro
        restart: always
        command:
          - '--path.procfs=/host/proc'
          - '--path.sysfs=/host/sys'
          - '--path.rootfs=/rootfs'

    cadvisor:
        image: google/cadvisor:latest
        container_name: cadvisor
        hostname: cadvisor
        restart: always
        volumes:
            - /:/rootfs:ro
            - /var/run:/var/run:rw
            - /sys:/sys:ro
            - /var/lib/docker/:/var/lib/docker:ro
        ports:
            - &quot;8080:8080&quot;

</code></pre>
<h2 id="启动docker-compose">启动docker-compose</h2>
<pre><code class="language-bash">#启动容器：
docker-compose -f /usr/local/src/config/docker-compose-monitor.yml up -d
#删除容器：
docker-compose -f /usr/local/src/config/docker-compose-monitor.yml down
#重启容器：
docker restart id
</code></pre>
<p>在其他节点分别启动cadvisor和node-exporter容器</p>
<pre><code class="language-yaml">version: '3'

services:
    node-exporter:
        image: quay.io/prometheus/node-exporter
        container_name: node-exporter
        hostname: $HOSTNAME
        restart: always
        ports:
            - &quot;9100:9100&quot;
        volumes:
          - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro
          - /proc:/host/proc:ro
          - /sys:/host/sys:ro
          - /:/rootfs:ro
        restart: always
        command:
          - '--path.procfs=/host/proc'
          - '--path.sysfs=/host/sys'
          - '--path.rootfs=/rootfs'
    cadvisor:
        image: google/cadvisor:latest
        container_name: cadvisor
        hostname: cadvisor
        restart: always
        volumes:
            - /:/rootfs:ro
            - /var/run:/var/run:rw
            - /sys:/sys:ro
            - /var/lib/docker/:/var/lib/docker:ro
        ports:
            - &quot;8080:8080&quot;
</code></pre>
<p><strong>容器启动如下：</strong><br>
<img src="https://chriswsq.github.io/post-images/1599121054830.png" alt="" loading="lazy"></p>
<p><strong>prometheus targets界面如下：</strong><br>
<img src="https://chriswsq.github.io/post-images/1599121262766.png" alt="" loading="lazy"></p>
<blockquote>
<p>备注：如果State为Down，应该是防火墙问题，参考下面防火墙配置。</p>
</blockquote>
<p><strong>prometheus targets界面如下：</strong><br>
<img src="https://chriswsq.github.io/post-images/1599121408543.png" alt="" loading="lazy"></p>
<blockquote>
<p>备注：如果没有数据，同步下时间。</p>
</blockquote>
<h2 id="配置grafana">配置grafana</h2>
<h3 id="添加prometheus数据源">添加Prometheus数据源</h3>
<figure data-type="image" tabindex="1"><img src="https://chriswsq.github.io/post-images/1599121507210.png" alt="" loading="lazy"></figure>
<h3 id="配置dashboards">配置dashboards</h3>
<p><strong>说明：可以用自带模板，也可以去https://grafana.com/dashboards，下载对应的模板。</strong></p>
<p>添加监控服务器模板 此处用的模板id是   8919   也可使用（1860）</p>
<p><img src="https://chriswsq.github.io/post-images/1599121616143.png" alt="" loading="lazy"><br>
<img src="https://chriswsq.github.io/post-images/1599121699226.png" alt="" loading="lazy"><br>
<img src="https://chriswsq.github.io/post-images/1599121761143.png" alt="" loading="lazy"></p>
<p>添加监控容器模板   此处用  893   也可用（8321）</p>
<figure data-type="image" tabindex="2"><img src="https://chriswsq.github.io/post-images/1599122084044.png" alt="" loading="lazy"></figure>
<p>也可用综合的 9276</p>
<h2 id="告警">告警</h2>
<p>停止192.168.40.7 的 cadvisor和node-exporter容器<br>
<img src="https://chriswsq.github.io/post-images/1599122289244.png" alt="" loading="lazy"></p>
<p>收到告警邮件<br>
<img src="https://chriswsq.github.io/post-images/1599122330270.png" alt="" loading="lazy"></p>
<blockquote>
<p>注： 如果日期有问题则将告警模板的触发时间参数改为 <code>{{ .StartsAt.Format &quot;2019-08-04 16:58:15&quot; }} &lt;br&gt;</code> 即可。</p>
</blockquote>
<p>参考博客：<br>
https://juejin.im/post/6844903809517371406<br>
https://blog.csdn.net/w342164796/article/details/105079231/<br>
https://blog.csdn.net/aixiaoyang168/article/details/98474494</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[k8s 如何关联pvc到特定的pv?]]></title>
        <id>https://chriswsq.github.io/post/k8s-ru-he-guan-lian-pvc-dao-te-ding-de-pv/</id>
        <link href="https://chriswsq.github.io/post/k8s-ru-he-guan-lian-pvc-dao-te-ding-de-pv/">
        </link>
        <updated>2020-08-27T05:59:30.000Z</updated>
        <summary type="html"><![CDATA[<p>部署有状态的应用时需要挂载相应的配置文件，了解下最常用的pv（nfs）和pvc绑定关系</p>
]]></summary>
        <content type="html"><![CDATA[<p>部署有状态的应用时需要挂载相应的配置文件，了解下最常用的pv（nfs）和pvc绑定关系</p>
<!-- more -->
<p>首先肯定要在放置配置文件的地方配置nfs服务</p>
<p>如何关联pvc到特定的pv?</p>
<p>我们可以使用对 pv 打 label 的方式，具体如下：</p>
<p>创建 pv，指定 label</p>
<p><code>cat nfs-test-pv.yaml</code></p>
<pre><code class="language-yaml">apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-test-pv
#  namespace: test-pv-test
  labels:
    pv: nfs-test-pv
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteMany
  nfs:
    # FIXME: use the right IP
    server: 192.168.40.6
    path: &quot;/test/&quot;
</code></pre>
<p>然后创建 pvc，使用 matchLabel 来关联刚创建的 pv:nfs-test-pv</p>
<p><code>cat nfs-test-pvc.yaml</code></p>
<pre><code class="language-yaml">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-test-pvc
#  namespace: test-pv-test
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: &quot;&quot;
  resources:
    requests:
      storage: 90Mi
  selector:
    matchLabels:
      pv: nfs-test-pv
</code></pre>
<p>下面开始测试：</p>
<p><strong>创建pv</strong></p>
<p><code>kubectl apply -f nfs-test-pv.yaml</code></p>
<p><strong>查看pv</strong></p>
<pre><code>kubectl get pv
[root@test-1 ~]# kubectl get pv
NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS   REASON   AGE
ca-service   100Mi      RWX            Retain           Bound    default/ca-service-pvc                           30m

</code></pre>
<p><strong>然后创建 pvc</strong><br>
<code>kubectl apply -f nfs-test-pvc.yaml</code></p>
<p><strong>查看pvc</strong></p>
<pre><code>[root@test-1 ~]# kubectl get pvc
NAME             STATUS   VOLUME       CAPACITY   ACCESS MODES   STORAGECLASS   AGE
ca-service-pvc   Bound    ca-service   100Mi      RWX                           30m
</code></pre>
<p>已都正确绑定</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[k8s pv,pvc无法删除问题]]></title>
        <id>https://chriswsq.github.io/post/k8s-pvpvc-wu-fa-shan-chu-wen-ti/</id>
        <link href="https://chriswsq.github.io/post/k8s-pvpvc-wu-fa-shan-chu-wen-ti/">
        </link>
        <updated>2020-08-27T05:58:37.000Z</updated>
        <summary type="html"><![CDATA[<p>解决k8s pv,pvc无法删除问题</p>
]]></summary>
        <content type="html"><![CDATA[<p>解决k8s pv,pvc无法删除问题</p>
<!-- more -->
<p>一般删除步骤为：先删pod再删pvc最后删pv</p>
<p>但是遇到pv始终处于“Terminating”状态，而且delete不掉。如下：</p>
<pre><code class="language-bash">[root@test-1 pv]# kubectl get pv
NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS        CLAIM                    STORAGECLASS   REASON   AGE
ca-service   100Mi      RWX            Retain           Terminating   default/ca-service-pvc                           17h

</code></pre>
<p>解决方法：</p>
<p>直接删除k8s中的记录：</p>
<pre><code class="language-bash">kubectl patch pv ca-service  -p '{&quot;metadata&quot;:{&quot;finalizers&quot;:null}}'
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubeadm部署k8s]]></title>
        <id>https://chriswsq.github.io/post/kubeadm-bu-shu-k8s/</id>
        <link href="https://chriswsq.github.io/post/kubeadm-bu-shu-k8s/">
        </link>
        <updated>2020-08-27T05:56:13.000Z</updated>
        <content type="html"><![CDATA[<!-- more -->
<h1 id="部署环境">部署环境</h1>
<table>
<thead>
<tr>
<th>主机名</th>
<th>centos版本</th>
<th>IP</th>
<th>docker-version</th>
<th>flanel-version</th>
<th>keepalived-version</th>
<th>主机配置</th>
</tr>
</thead>
<tbody>
<tr>
<td>k8s-master1</td>
<td>7.4</td>
<td>192.168.40.6</td>
<td>18.09.9</td>
<td>v0.11.0</td>
<td>v1.3.5</td>
<td>2C4G</td>
</tr>
<tr>
<td>k8s-node1</td>
<td>7.4</td>
<td>192.168.40.7</td>
<td>18.09.9</td>
<td>v0.11.0</td>
<td>v1.3.5</td>
<td>2C4G</td>
</tr>
<tr>
<td>k8s-node2</td>
<td>7.4</td>
<td>192.168.40.8</td>
<td>18.09.9</td>
<td>v0.11.0</td>
<td>v1.3.5</td>
<td>2C4G</td>
</tr>
</tbody>
</table>
<p>二、高可用架构</p>
<figure data-type="image" tabindex="1"><img src="https://chriswsq.github.io/post-images/1598507860493.jpg" alt="" loading="lazy"></figure>
<p>注：此图为负载为loadbanacer为云上的负载方案</p>
<p>在此就说下本人在这里犯的小错误</p>
<p>1.拉镜像时各个节点都要拉取</p>
<p>2.部署flannel时因为网络的原因可以先down下载，在执行   kubectl apply -f   kube-flannel.yml<br>
wget https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml</p>
<p>3.部署后会发现主节点Ready 而从节点都是NotReady，这是因为master节点的这个文件 /etc/cni/net.d/10-flannel.conflist 是因为安装flannel生成的，而node节点是通过master分配生成的所以少一个参数<br>
&quot;cniVersion&quot;: &quot;0.3.1&quot;,</p>
<p>直接把maser推过去到node节点，或者手动改下就可以了</p>
<h2 id=""></h2>
<p>kubeadm部署</p>
<p>kubeadm join 192.168.154.6:6443 --token hum394.uf5ehv9ye6w661bz <br>
--discovery-token-ca-cert-hash sha256:6bd3388a191ae0afb2ca86102e655a2ced4c6e6b6bef82afebbdef8154a57b61</p>
<p>node</p>
<p>{<br>
&quot;name&quot;: &quot;cbr0&quot;,<br>
&quot;plugins&quot;: [<br>
{<br>
&quot;type&quot;: &quot;flannel&quot;,<br>
&quot;delegate&quot;: {<br>
&quot;hairpinMode&quot;: true,<br>
&quot;isDefaultGateway&quot;: true<br>
}<br>
},<br>
{<br>
&quot;type&quot;: &quot;portmap&quot;,<br>
&quot;capabilities&quot;: {<br>
&quot;portMappings&quot;: true<br>
}<br>
}<br>
]<br>
}</p>
<p>master</p>
<p>{<br>
&quot;name&quot;: &quot;cbr0&quot;,<br>
&quot;cniVersion&quot;: &quot;0.3.1&quot;,<br>
&quot;plugins&quot;: [<br>
{<br>
&quot;type&quot;: &quot;flannel&quot;,<br>
&quot;delegate&quot;: {<br>
&quot;hairpinMode&quot;: true,<br>
&quot;isDefaultGateway&quot;: true<br>
}<br>
},<br>
{<br>
&quot;type&quot;: &quot;portmap&quot;,<br>
&quot;capabilities&quot;: {<br>
&quot;portMappings&quot;: true<br>
}<br>
}<br>
]<br>
}</p>
<p>参考<br>
https://www.kubernetes.org.cn/6632.html</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[logstash配置]]></title>
        <id>https://chriswsq.github.io/post/logstash-pei-zhi/</id>
        <link href="https://chriswsq.github.io/post/logstash-pei-zhi/">
        </link>
        <updated>2020-08-27T05:54:19.000Z</updated>
        <summary type="html"><![CDATA[<p>补上一份自己在工作中用的一份logstash的配置文件，万变不离其宗，可以参看此内容</p>
]]></summary>
        <content type="html"><![CDATA[<p>补上一份自己在工作中用的一份logstash的配置文件，万变不离其宗，可以参看此内容</p>
<!-- more -->
<h2 id="这一份匹配的日志是前部分是字符串后一部分是json格式的日志">这一份匹配的日志是，前部分是字符串，后一部分是json格式的日志</h2>
<pre><code class="language-yml">input {
    kafka {
        bootstrap_servers =&gt; &quot;kafka1:9092, kafka2:9092, kafka3:9092&quot;
        topics =&gt; [&quot;tomcat&quot;]
        type =&gt; &quot;tomcat&quot;
        codec =&gt; json {
           charset =&gt; &quot;UTF-8&quot;
        }
    }
}

filter {     
   if [type] == &quot;tomcat&quot; {
   grok {
            match =&gt; { 'message' =&gt; '%{TIMESTAMP_ISO8601:access_time}  %{LOGLEVEL:loglevel} %{INT:number} --- \[%{DATA:thread_name}\] %{URIHOST:
type_name}  %{SPACE}*  : %{HOSTNAME:status} %{HOSTNAME:action} %{URIHOST:usetime}((?&lt;request&gt;(.*)(?=Creating)/?)|)(%{GREEDYDATA:sql}|)'}
}
   json {
        source =&gt; &quot;request&quot;
        #target =&gt; &quot;parsedJson&quot;
        remove_field=&gt;[&quot;request&quot;]
}      
    mutate {
        remove_field =&gt; [&quot;agent&quot;]
        remove_field =&gt; [&quot;[ecs][version]&quot;,&quot;[fields][log_topics]&quot;,&quot;[fields][registry_file]&quot;,&quot;[host][name]&quot;,&quot;[input][type]&quot;,&quot;[@version]&quot;,&quot;[tag]&quot;,&quot;[id]&quot;,&quot;[score]&quot;]
          }
     }
}
output {
  if [type] == &quot;tomcat&quot; {
  elasticsearch {
    hosts =&gt; [&quot;elasticsearch1:9200&quot;,&quot;elasticsearch2:9200&quot;,&quot;elasticsearch3:9200&quot;]
    index =&gt; &quot;tomcat-%{+YYYY.MM.dd}&quot;
    user =&gt; 'elastic'
    password =&gt; 'd27DuDQjTChIdv3sE8oI'
   }
 }
}
</code></pre>
<h2 id="这一份是整个的日志都是json格式">这一份是整个的日志都是json格式</h2>
<pre><code class="language-yml">input {
    kafka {
        bootstrap_servers =&gt; &quot;kafka1:9092, kafka2:9092, kafka3:9092&quot;
        topics =&gt; [&quot;gateway&quot;]
        type =&gt; &quot;gateway&quot;
        codec =&gt; json {
           charset =&gt; &quot;UTF-8&quot;
        }
    }
}
filter {
   if [type] == &quot;gateway&quot; {
    json {
        source =&gt; &quot;message&quot;
        #target =&gt; &quot;doc&quot;
        remove_field =&gt; [&quot;message&quot;]
    }        
    mutate {
        remove_field =&gt; [&quot;agent&quot;]
        remove_field =&gt; [&quot;[ecs][version]&quot;,&quot;[fields][log_topics]&quot;,&quot;[fields][registry_file]&quot;,&quot;[host][name]&quot;,&quot;[input][type]&quot;,&quot;[@version]&quot;,&quot;[tag]&quot;,&quot;[id]&quot;,&quot;[score]&quot;]
      } 
   }
}
output {
   if [type] == &quot;gateway&quot; {
    elasticsearch {
    hosts =&gt; [&quot;elasticsearch1:9200&quot;,&quot;elasticsearch2:9200&quot;,&quot;elasticsearch3:9200&quot;]
    index =&gt; &quot;gateway-%{+YYYY.MM.dd}&quot;
    user =&gt; 'elastic'
    password =&gt; 'd27DuDQjTChIdv3sE8oI'
        } 
    }
}
</code></pre>
<h2 id="这是一份整个日志都是字符串的格式">这是一份整个日志都是字符串的格式</h2>
<pre><code class="language-yml">input {
    kafka {
        bootstrap_servers =&gt; &quot;kafka1:9092, kafka2:9092, kafka3:9092&quot;
        topics =&gt; [&quot;gb_core_ngx&quot;]
        type =&gt; &quot;gb_core_ngx&quot;
        codec =&gt; json {
           charset =&gt; &quot;UTF-8&quot;
        }
    }
}
filter {
    if [type] == &quot;gb_core_ngx&quot; {

    grok {
       match =&gt; {
              &quot;message&quot; =&gt; &quot;%{URIHOST:hostname}: \[%{TIMESTAMP_ISO8601:local_time}\] (%{NUMBER:status}|-) %{NUMBER:siteid}-%{WORD:idc_num}-%{HOSTNAME:Random_value} (%{HOSTNAME:X_system}|-) *(%{URIHOST:remote_hostname}|-),%{URIHOST:outsite_hostname} \[(%{URIHOST:upstream_addr}|-)] %{NUMBER:request_time} (%{NUMBER:upstraem_response_time}|-) (%{WORD:request_method}|-) (%{HOSTNAME:http_host}|-) (%{GREEDYDATA:uri}|-) (%{NUMBER:body_bytes_sent}|-) \| (%{IPORHOST:client_ip}|-)&quot;
       }
}
    mutate {
        remove_field =&gt; &quot;tags&quot;
        remove_field =&gt; &quot;port&quot;
        remove_field =&gt; &quot;prospector&quot;
        remove_field =&gt; &quot;beat&quot;
        remove_field =&gt; &quot;source&quot;
        remove_field =&gt; &quot;offset&quot;
        remove_field =&gt; &quot;fields&quot;
        remove_field =&gt; &quot;host&quot;
        remove_field =&gt; &quot;@version&quot;
        remove_field =&gt; &quot;message&quot;
        remove_field =&gt; &quot;input&quot;
        convert =&gt; [&quot;body_bytes_sent&quot;, &quot;float&quot;]
        convert =&gt; [&quot;request_time&quot;, &quot;float&quot;]
        convert =&gt; [&quot;upstream_response_time&quot;, &quot;float&quot;]
        add_field =&gt; { &quot;uuid&quot; =&gt; &quot;%{siteid}-%{idc_num}-%{Random_value}&quot; }
    }

#    date {
#        match =&gt; [&quot;local__time&quot;, &quot;yyyy-MM-dd HH:mm:ss&quot;]
#}
   
   geoip {
        source =&gt; &quot;client_ip&quot;
        target =&gt; &quot;geoip&quot;
        database =&gt; &quot;/opt/GeoLite2-City.mmdb&quot;
        add_field =&gt; [&quot;[geoip][coordinates]&quot;,&quot;%{[geoip][longitude]}&quot;]
        add_field =&gt; [&quot;[geoip][coordinates]&quot;,&quot;%{[geoip][latitude]}&quot;]
                }
   mutate {
                 convert =&gt; [ &quot;[geoip][coordinates]&quot;, &quot;float&quot;]
        }
   }
}
output {
       if [type] == &quot;gb_core_ngx&quot; {
       elasticsearch { 
       hosts =&gt; [&quot;elasticsearch1:9200&quot; ,&quot;elasticsearch2:9200&quot;, &quot;elasticsearch3:9200&quot;]
       index =&gt; &quot;logstash-gb_core_ngx-%{+YYYY.MM.dd}&quot;
       user =&gt; 'elastic'
       password =&gt; 'd27DuDQjTChIdv3sE8oI'
   }
 }
}
</code></pre>
<h2 id="添加组合字段">添加&amp;组合字段</h2>
<pre><code class="language-yml">filter {
 grok {
    match =&gt; { &quot;message&quot; =&gt; '&quot;(%{GREEDYDATA:cust_date})&quot;,&quot;(%{TIME:cust_time})&quot;,&quot;(%{NUMBER:author})&quot;'}
    add_field =&gt; {
            &quot;date_time&quot; =&gt; &quot;%{cust_date};%{cust_time}&quot;
    }
}

date {
  match =&gt; [&quot;date_time&quot;, &quot;yyyy-MM-dd;hh:mm:ss&quot;]
  target =&gt; &quot;@timestamp&quot;
  add_field =&gt; { &quot;debug&quot; =&gt; &quot;timestampMatched&quot;}
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[elk用户认证问题]]></title>
        <id>https://chriswsq.github.io/post/elk-yong-hu-ren-zheng-wen-ti/</id>
        <link href="https://chriswsq.github.io/post/elk-yong-hu-ren-zheng-wen-ti/">
        </link>
        <updated>2020-08-27T05:52:58.000Z</updated>
        <summary type="html"><![CDATA[<p>elk7+的用户认证免费了，不过没有集成在安装包里，还需要自己再进行一些操作，在此记录下自己再配置用户认证时遇到的问题</p>
]]></summary>
        <content type="html"><![CDATA[<p>elk7+的用户认证免费了，不过没有集成在安装包里，还需要自己再进行一些操作，在此记录下自己再配置用户认证时遇到的问题</p>
<!-- more -->
<h2 id="开启安全功能">开启安全功能</h2>
<p>首先在es配置文件中开启安全功能</p>
<pre><code>.....

xpack.security.enabled: true
xpack.security.transport.ssl.enabled
</code></pre>
<p>然后重启es服务</p>
<h2 id="生成证书配置node间ssl通信">生成证书，配置Node间SSL通信</h2>
<h3 id="创建ca证书">创建ca证书</h3>
<pre><code>$ bin/elasticsearch-certutil ca -v
</code></pre>
<p>一路回车即可</p>
<p>默认的CA证书存放在$ES_HOME 目录中</p>
<p>这个命令生成格式为PKCS#12名称为 elastic-stack-ca.p12 的keystore文件，包含CA证书和私钥。</p>
<h3 id="创建节点间认证用的证书">创建节点间认证用的证书</h3>
<pre><code>$ ./bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12 
</code></pre>
<p>一路回车即可</p>
<h3 id="配置es节点使用这个证书">配置ES节点使用这个证书</h3>
<pre><code>$ mkdir config/certs
$ mv elastic-* config/certs/
ll config/certs/

-rw------- 1 elasticsearch root 3443 Jun  9 15:22 elastic-certificates.p12
-rw------- 1 elasticsearch root 2527 Jun  9 15:22 elastic-stack-ca.p12
</code></pre>
<p>拷贝这个目录到所有的ES节点中<br>
config/certs 目录中不需要拷贝CA证书文件，只拷贝cert文件即可</p>
<p>配置elasticsearch.yml配置文件，注意所有的node节点都需要配置，这里的配置是使用PKCS#12格式的证书。</p>
<pre><code>$ vim  config/elasticsearch.yml
xpack.security.enabled: true
xpack.security.transport.ssl.enabled: true
xpack.security.transport.ssl.verification_mode: certificate #认证方式使用证书
xpack.security.transport.ssl.keystore.path: certs/elastic-certificates.p12
xpack.security.transport.ssl.truststore.path: certs/elastic-certificates.p12
</code></pre>
<h2 id="测试能够正常启动了-好了我们再来继续之前的生成密码在随意一台节点即可">测试能够正常启动了。好了，我们再来继续之前的生成密码：在随意一台节点即可。</h2>
<p>生成密码有两个命令，一个为自动生成，一个为手动生成</p>
<p>自动<br>
<code>bin/elasticsearch-setup-passwords auto</code><br>
手动<br>
<code>bin/elasticsearch-setup-passwords interactive</code></p>
<p>这里我们选择自动</p>
<pre><code>[root@elasticsearch1 elasticsearch]# bin/elasticsearch-setup-passwords auto
Initiating the setup of passwords for reserved users elastic,apm_system,kibana,logstash_system,beats_system,remote_monitoring_user.
The passwords will be randomly generated and printed to the console.
Please confirm that you would like to continue [y/N]y


Changed password for user apm_system
PASSWORD apm_system = EKPbqziKu4v3P0MYoYIJ

Changed password for user kibana
PASSWORD kibana = 48Sf7tIFArn4rmyMW2x4

Changed password for user logstash_system
PASSWORD logstash_system = O09XtxP495uNRxHRZxtC

Changed password for user beats_system
PASSWORD beats_system = ULC810uP0sVCAKhbwxvE

Changed password for user remote_monitoring_user
PASSWORD remote_monitoring_user = zqnO9YBFsoRu5QIsgOi6

Changed password for user elastic
PASSWORD elastic = d27DuDQjTChIdv3sE8oI

</code></pre>
<p>查看集群节点数量：</p>
<pre><code>[root@test-1 conf]# curl -u elastic 192.168.154.6:9200/_cat/nodes
Enter host password for user 'elastic':
10.0.0.67 32 88 1 0.12 0.08 0.43 ilmr   - node-1
10.0.0.92 14 80 1 0.12 0.07 0.22 dilmrt * node-2
10.0.0.93 12 79 1 0.00 0.03 0.24 dilmrt - node-3
</code></pre>
<h2 id="kibana的安装配置">kibana的安装配置</h2>
<h3 id="编辑配置文件">编辑配置文件</h3>
<pre><code>$ cat kibana/config/kibana.yml |grep -Ev &quot;^$|^#&quot;
server.port: 5601
server.host: &quot;0.0.0.0&quot;
server.name: &quot;mykibana&quot;
elasticsearch.hosts: [&quot;http://localhost:9200&quot;]
kibana.index: &quot;.kibana&quot;
elasticsearch.username: &quot;kibana&quot;
elasticsearch.password: &quot;UKuHceHWudloJk9NvHlX&quot;
# i18n.locale: &quot;en&quot;
i18n.locale: &quot;zh-CN&quot;
xpack.security.encryptionKey: Hz*9yFFaPejHvCkhT*ddNx%WsBgxVSCQ
</code></pre>
<h3 id="页面访问kibana">页面访问kibana</h3>
<figure data-type="image" tabindex="1"><img src="/images/elk%E7%94%A8%E6%88%B7%E8%AE%A4%E8%AF%81/kibana-login.jpg" alt="" loading="lazy"></figure>
<p>输入上面生成的管理员elastic的用户和密码，就可以登陆了，我们查看一下license许可吧：<br>
<img src="/images/elk%E7%94%A8%E6%88%B7%E8%AE%A4%E8%AF%81/kibana-basic.jpg" alt="" loading="lazy"></p>
<p>一个使用永不过期的Basic许可的免费License，开启了基本的Auth认证和集群间SSL/TLS 认证的Elasticsearch集群就创建完毕了。</p>
<p>等等，你有没有想过Kibana的配置文件中使用着明文的用户名密码，这里只能通过LInux的权限进行控制了，有没有更安全的方式呢，有的，就是keystore。</p>
<h3 id="kibana-keystore-安全配置">kibana keystore 安全配置</h3>
<p><a href="https://www.elastic.co/guide/en/kibana/7.7/secure-settings.html">参考官方</a></p>
<p>查看<code>kibana-keystore</code>命令帮助：</p>
<pre><code>$ ./bin/kibana-keystore --help
Usage: bin/kibana-keystore [options] [command]

A tool for managing settings stored in the Kibana keystore

Options:
  -V, --version           output the version number
  -h, --help              output usage information

Commands:
  create [options]        Creates a new Kibana keystore
  list [options]          List entries in the keystore
  add [options] &lt;key&gt;     Add a string setting to the keystore
  remove [options] &lt;key&gt;  Remove a setting from the keystore

</code></pre>
<p>首先我们创建keystore：</p>
<pre><code>$ bin/kibana-keystore create
Created Kibana keystore in /opt/elk74/kibana-7.4.2-linux-x86_64/data/kibana.keystore # 默认存放位置
</code></pre>
<p>增加配置：</p>
<p>我们要吧kibana.yml 配置文件中的敏感信息，比如：elasticsearch.username 和 elasticsearch.password，给隐藏掉，或者直接去掉；</p>
<p>所以这里我们增加两个配置：分别是elasticsearch.password 和 elasticsearch.username:</p>
<pre><code># 查看add的命令帮助：
$ ./bin/kibana-keystore add --help
Usage: add [options] &lt;key&gt;

Add a string setting to the keystore

Options:
  -f, --force   overwrite existing setting without prompting
  -x, --stdin   read setting value from stdin
  -s, --silent  prevent all logging
  -h, --help    output usage information

# 创建elasticsearch.username这个key：注意名字必须是kibana.yml中的key
$ ./bin/kibana-keystore add elasticsearch.username
Enter value for elasticsearch.username: ******  # 输入key对应的value，这里是kibana连接es的账号：kibana

# 创建elasticsearch.password这个key
$ ./bin/kibana-keystore add elasticsearch.password
Enter value for elasticsearch.password: ******************** # 输入对应的密码：UKuHceHWudloJk9NvHlX
</code></pre>
<p>好了，我们把kibana.yml配置文件中的这两项配置删除即可，然后直接启动kibana，kibana会自动已用这两个配置的。</p>
<p>最终的kibana.yml配置如下：</p>
<pre><code>server.port: 5601
server.host: &quot;0.0.0.0&quot;
server.name: &quot;mykibana&quot;
elasticsearch.hosts: [&quot;http://localhost:9200&quot;]
kibana.index: &quot;.kibana&quot;
# i18n.locale: &quot;en&quot;
i18n.locale: &quot;zh-CN&quot;
xpack.security.encryptionKey: Hz*9yFFaPejHvCkhT*ddNx%WsBgxVSCQ
</code></pre>
<p>这样配置文件中就不会出现敏感信息了，达到了更高的安全性。</p>
<p>类似的Keystore方式不只是Kibana支持，ELK的产品都是支持的。</p>
<p><strong>注意：两个证书文件属主须是启动用户，certs权限为744即可，权限不够会报错,其他节点权限也一样</strong></p>
<pre><code>elk_elasticsearch1.1.rqtnwnm1sqzw@test-1    | uncaught exception in thread [main]
elk_elasticsearch1.1.rqtnwnm1sqzw@test-1    | ElasticsearchSecurityException[failed to load SSL configuration [xpack.security.transport.ssl]]; nested: ElasticsearchException[failed to initialize SSL TrustManager - not permitted to read truststore file [/usr/share/elasticsearch/config/certs/elastic-certificates.p12]]; nested: AccessDeniedException[/usr/share/elasticsearch/config/certs/elastic-certificates.p12];
elk_elasticsearch1.1.jp9k598g34hq@test-1    | &quot;... 6 more&quot;] }
elk_elasticsearch1.1.yk71h6aom47z@test-1    | &quot;at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:481) ~[elasticsearch-7.7.0.jar:7.7.0]&quot;,
elk_elasticsearch1.1.w26656n32a86@test-1    | &quot;... 6 more&quot;] }
elk_elasticsearch1.1.w26656n32a86@test-1    | ElasticsearchSecurityException[failed to load SSL configuration [xpack.security.transport.ssl]]; nested: ElasticsearchException[failed to initialize SSL TrustManager - not permitted to read truststore file [/usr/share/elasticsearch/config/certs/elastic-certificates.p12]]; nested: AccessDeniedException[/usr/share/elasticsearch/config/certs/elastic-certificates.p12];
elk_elasticsearch1.1.rqtnwnm1sqzw@test-1    | Likely root cause: java.nio.file.AccessDeniedException: /usr/share/elasticsearch/config/certs/elastic-certificates.p12
elk_elasticsearch1.1.jp9k598g34hq@test-1    | ElasticsearchSecurityException[failed to load SSL configuration [xpack.security.transport.ssl]]; nested: ElasticsearchException[failed to initialize SSL TrustManager - not permitted to read truststore file [/usr/share/elasticsearch/config/certs/elastic-certificates.p12]]; nested: AccessDeniedException[/usr/share/elasticsearch/config/certs/elastic-certificates.p12];
</code></pre>
<p><a href="https://knner.wang/2019/11/26/install-elasticsearch-cluster-7-4.html">参考自</a></p>
<h2 id="logstash配置">logstash配置</h2>
<p><a href="https://www.elastic.co/cn/blog/configuring-ssl-tls-and-https-to-secure-elasticsearch-kibana-beats-and-logstash">参考</a></p>
]]></content>
    </entry>
</feed>