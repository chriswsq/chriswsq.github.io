<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://chriswsq.github.io</id>
    <title>chris&apos;wang</title>
    <updated>2020-08-20T10:33:47.238Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://chriswsq.github.io"/>
    <link rel="self" href="https://chriswsq.github.io/atom.xml"/>
    <subtitle>当你觉得无所事事时，那你就是在虚度光阴</subtitle>
    <logo>https://chriswsq.github.io/images/avatar.png</logo>
    <icon>https://chriswsq.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, chris&apos;wang</rights>
    <entry>
        <title type="html"><![CDATA[Kubernetes 基础教学（二）实作范例：Pod、Service、Deployment、Ingress]]></title>
        <id>https://chriswsq.github.io/post/kubernetes-ji-chu-jiao-xue-er-shi-zuo-fan-li-podservicedeploymentingress/</id>
        <link href="https://chriswsq.github.io/post/kubernetes-ji-chu-jiao-xue-er-shi-zuo-fan-li-podservicedeploymentingress/">
        </link>
        <updated>2020-08-20T10:02:28.000Z</updated>
        <summary type="html"><![CDATA[<p>Kubernetes（K8S）是一個可以幫助我們管理微服務（microservices）的系統，他可以自動化地部署及管理多台機器上的多個容器（Container）。簡單來說，他可以做到：</p>
]]></summary>
        <content type="html"><![CDATA[<p>Kubernetes（K8S）是一個可以幫助我們管理微服務（microservices）的系統，他可以自動化地部署及管理多台機器上的多個容器（Container）。簡單來說，他可以做到：</p>
<!-- more -->
<p>Kubernetes（K8S）是一个可以帮助我们管理微服务（microservices）的系统，他可以自动化地部署及管理多台机器上的多个容器（Container）。简单来说，他可以做到：</p>
<ul>
<li>同时部署多个容器到多台机器上（Deployment）</li>
<li>服务的乘载量有变化时，可以对容器做自动扩展（Scaling）</li>
<li>管理多个容器的状态，自动侦测并重启故障的容器（Management）</li>
</ul>
<p>在系列文的上一篇文章中，我们了解了构成 Kubernetes 的四个重要元素：Pod、Node、Master、Cluster，并安装好了我们要实际动手玩 Kubernetes 前需要的套件与工具。接下来在这篇文章中，我们会透过例子来实际建立那些在 Kubernetes 中常见的元件们。</p>
<h2 id="如何建立一个-pod">如何建立一个 Pod</h2>
<h2 id="撰写-pod-的身分证">撰写 Pod 的身分证</h2>
<p>还记得我们在介绍 Kubernetes 时有提到，每个 Pod 都有一个身分证，也就是属于这个 Pod 的 .yaml 档。我们透过撰写下面的这个 .yaml 档就可以建立出 Pod。</p>
<p>kubernetes-demo.yaml</p>
<pre><code class="language-yaml">apiVersion: v1
 kind: Pod
 metadata:
   name: kubernetes-demo-pod
   labels:
     app: demoApp
 spec:
   containers:
     - name: kubernetes-demo-container
       image: hcwxd/kubernetes-demo
       ports:
         - containerPort: 3000
</code></pre>
<p>apiVersion</p>
<ul>
<li>该元件版本号</li>
</ul>
<p>kind</p>
<ul>
<li>该元件是什么属性，常见有 Pod、Node、Service、Namespace、ReplicationController 等</li>
</ul>
<p>metadata</p>
<p>name</p>
<ul>
<li>指定该 Pod 的名称<br>
labels</li>
<li>指定该 Pod 的标签，这里我们暂时帮它上标签为 app: demoApp</li>
</ul>
<p>spec</p>
<p>container.name</p>
<ul>
<li>指定运行出的 Container 的名称</li>
</ul>
<p>container.image</p>
<ul>
<li>指定 Container 要使用哪个 Image，这里会从 DockerHub 上搜寻</li>
</ul>
<p>container.ports</p>
<ul>
<li>指定该 Container 有哪些 port number 是允许外部资源存取</li>
</ul>
<h2 id="透过-kubectl-建立-pod">透过 kubectl 建立 Pod</h2>
<p>有了身份证后，我们就可以透过 kubectl 指令来建立 Pod</p>
<p><code>kubectl create -f kubernetes-demo.yaml</code></p>
<p>看到 <code>pod/kubernetes-demo-pod created</code> 的字样就代表我们建立成功我们的第一个 Pod 了。我们可以再透过指令</p>
<p><code>kubectl get pods</code></p>
<p>看到我们运行中的 Pod：</p>
<pre><code class="language-bash">NAME                  READY   STATUS    RESTARTS   AGE
kubernetes-demo-pod   1/1     Running   0          60s
</code></pre>
<h2 id="连线到我们-pod-的服务资源">连线到我们 Pod 的服务资源</h2>
<p>建立好我们的 Pod 之后，打开浏览器的 <code>localhost:3000</code> 我们会发现怎么什么都看不到。这是因为在 Pod 中所指定的 port，跟我们本机端的 port 是不相通的。因此，我们必须还要透过 <code>kubectl port-forward</code>，把我们两端的 port 做 mapping。</p>
<p><code>kubectl port-forward kubernetes-demo-pod 3000:3000</code></p>
<p>做好 mapping 后，再打开浏览器的 localhost:3000 ，我们就可以迎接一只可爱的小鲸鱼啰！</p>
<figure data-type="image" tabindex="1"><img src="https://chriswsq.github.io/post-images/1597918423758.png" alt="" loading="lazy"></figure>
<h2 id="kubernetes-进阶三元件">Kubernetes 进阶三元件</h2>
<p>了解完如何从无到有建立一个 Kubernetes Cluster 并产生一个 Pod 后，接下来我们要认识在现实应用中，我们还会搭配到哪些 Kubernetes 的进阶元件。其中最重要的三个进阶元件就是：Service、Ingress、Deployment。</p>
<p><strong>Service</strong></p>
<p>还记得上面提到我们在连线到一个 Pod 的服务资源时，会使用到 port-forward 的指令。但如果我们有多个 Pods 想要同时被连线时，我们就可以用到 Service 这个进阶元件。简单来说，Service 就是 Kubernetes 中用来定义「一群 Pod 要如何被连线及存取」的元件。</p>
<p>要建立一个 Service，一样要撰写属于他的身分证。</p>
<p>service.yaml</p>
<pre><code class="language-yaml">apiVersion: v1
 kind: Service
 metadata:
   name: my-service
 spec:
   selector:
     app: demoApp
   type: NodePort
   ports:
     - protocol: TCP
       port: 3001
       targetPort: 3000
       nodePort: 30390
</code></pre>
<p>apiVersion</p>
<ul>
<li>该元件的版本号</li>
</ul>
<p>kind</p>
<ul>
<li>该元件是什么属性，常见有 Pod、Node、Service、Namespace、ReplicationController 等</li>
</ul>
<p>metadata</p>
<ul>
<li>name<br>
指定该 Pod 的名称</li>
</ul>
<p>spec</p>
<ul>
<li>
<p>selector<br>
该 Service 的连线规则适用在哪一群 Pods，还记得我们在建立 Pod 的时候，会帮它上 label，这时就可以透过 app: demoApp，去找到那群 label 的 app 属性是 demoApp 的 Pods 们</p>
</li>
<li>
<p>ports</p>
<ul>
<li>targetPort<br>
指定我们 Pod 上允许外部资源存取 Port Number</li>
<li>port<br>
指定我们 Pod 上的 targetPort 要 mapping 到 Service 中 ClusterIP 中的哪个 port</li>
<li>nodePort<br>
指定我们 Pod 上的 targetPort 要 mapping 到 Node 上的哪个 port</li>
</ul>
</li>
</ul>
<p>接下来我们先重新建立我们的 Pod</p>
<p><code>kubectl create -f kubernetes-demo.yaml</code></p>
<p>接下来我们透过 service.yaml 来建立我们的 Service 元件</p>
<p><code>kubectl create -f service.yaml</code></p>
<p>然后我们可以透过</p>
<p><code>kubectl get services</code></p>
<p>取得我们新建立 Service 的资料</p>
<pre><code class="language-bash">NAME       TYPE      CLUSTER-IP     EXTERNAL-IP PORT(S)          AGE
my-service NodePort  10.110.237.205 &lt;none&gt;      3001:30391/TCP   60s
</code></pre>
<p>有了建立好的 Service 后，我们可以透过两种方式连线我们的 Pod 的服务资源。首先，要从外部连线到我们的 Pod 资源服务，我们必须要先有我们的 Kubernetes Cluster（在这边是 minikube）对外开放的 IP。我们先透过指令</p>
<p><code>minikube ip</code></p>
<p>得到我们 minikube 的 ip</p>
<p><code>192.168.99.100</code></p>
<p>接着打开我们的浏览器，输入上面的 ip 加上我们在 yaml 档指定的 nodePort，在这边是 192.168.99.100:30390，就会得到我们的小鲸鱼了。</p>
<p>而如果不从浏览器，而是直接从 minikube 里面连线到我们的 Pod 则要先透过指令</p>
<p><code>minikube ssh</code></p>
<p>ssh 进入我们的 minikube cluster，接着输入指令</p>
<p><code>curl &lt;CLUSTER-IP&gt;:&lt;port&gt;</code></p>
<p>其中 CLUSTER-IP 就是我们用 kubectl get services 得到我们 Service 的 IP，而 port 就是我们在 yaml 档指定的 port，在这边合起来就是 10.110.237.205:3001，于是我们</p>
<p><code>curl 10.110.237.205:3001</code></p>
<p>就可以在 minikube 里面得到我们的小鲸鱼啰！</p>
<p><strong>Deployment</strong></p>
<p>了解了 Service 后，接下来要来了解第二个进阶元件：Deployment。今天当我们同时要把一个Pod 做横向扩展，也就是复制多个相同的Pod 在Cluster 中同时提供服务，并监控如果有Pod 当机我们就要重新把它启动时，如果我们要一个Pod 一个Pod透过指令建立并监控是很花时间的。因此，我们可以透过 Deployment 这个特殊元件帮我们达成上述的要求。</p>
<p>同样要建立一个 Deployment，要先撰写属于他的身分证。</p>
<p>deployment.yaml</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: demoApp
    spec:
      containers:
        - name: kubernetes-demo-container
          image: hcwxd/kubernetes-demo
          ports:
            - containerPort: 3000
  selector:
    matchLabels:
      app: demoApp
</code></pre>
<p>apiVersion</p>
<ul>
<li>该元件的版本号</li>
</ul>
<p>kind</p>
<ul>
<li>该元件是什么属性，常见有 Pod、Node、Service、Namespace、ReplicationController 等</li>
</ul>
<p>metadata</p>
<ul>
<li>name<br>
指定该 Pod 的名称</li>
</ul>
<p>spec</p>
<ul>
<li>
<p>replicas<br>
指定要建立多少个相同的 Pod，在这边给的数字是所谓的 Desire State，当 Cluster 运行时如果 Pod 数量低于此数字，Kubernetes 就会自动帮我们增加 pod，反之就会帮我们关掉 Pod</p>
</li>
<li>
<p>template<br>
指定这个 Deployment 建立的 Pod 们统一的设定，包括 metadata 以及这些 Pod 的 Containers，这边我们就沿用之前建立 Pod 的设定</p>
</li>
<li>
<p>selector<br>
指定这个 Deployment 的规则要适用到哪些 Pod，在这边就是指定我们在 template 中指定的 labels</p>
</li>
</ul>
<p>接下来我们就可以透过指令</p>
<p><code>kubectl create -f deployment.yaml</code></p>
<p>建立好我们的 Deployment，这时我们可以查看我们的 Deployment 有没有被建立好</p>
<pre><code class="language-bash">kubectl get deploy
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
my-deployment   3/3     3            3           60s
</code></pre>
<p>接着我们在看 Pod 们有没有乖乖按照 Deployment 建立</p>
<pre><code class="language-bash">kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
my-deployment-5454f687cd-bxjfz   1/1     Running   0          60s
my-deployment-5454f687cd-gszbr   1/1     Running   0          60s
my-deployment-5454f687cd-k6zfv   1/1     Running   0          60s
</code></pre>
<p>这边我们可以看到三个 Pod 都被建立好了，我们就成功做到了 Pod 的横向扩展。而除了 Pod 的横向扩展外，Deployment 的另外一个好处就是可以帮我们做到无停机的系统升级（Zero Downtime Rollout）。也就是说，当我们要更新我们的 Pod 时，Kubernetes 并不会直接砍掉我们所有的 Pod，而是会建立新的 Pod，等新的 Pod 开始正常运行后，再来取代旧的 Pod。</p>
<p>举例来说，假设我们现在想要更新我们 Pod 对外的 Port，我们可以先透过指令</p>
<p><code>kubectl edit deployments my-deployment</code></p>
<p>接着我们会看到我们的 Yaml 档</p>
<pre><code class="language-yaml">apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: '2'
  creationTimestamp: '2019-04-26T04:18:26Z'
  generation: 2
  labels:
    app: demoApp
  name: my-deployment
  namespace: default
  resourceVersion: '328692'
  selfLink: /apis/extensions/v1beta1/namespaces/default/deployments/my-deployment
  uid: 56608fb5-67da-11e9-933f-08002789461f
spec:
  progressDeadlineSeconds: 600
  replicas: 3
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: demoApp
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: demoApp
    spec:
      containers:
        - image: hcwxd/kubernetes-demo
          imagePullPolicy: Always
          name: kubernetes-demo-container
          ports:
            - containerPort: 3000
              protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
</code></pre>
<p>我们把其中 containerPort: 3000 改成 3001 后储存，Kubernetes 就会开始帮我们进行更新。这时我们继续用指令 kubectl get pods 就会看到</p>
<pre><code class="language-bash">NAME                          READY STATUS            RESTARTS   AGE
my-deployment-5454f687cd-bxjf 1/1   Running           0          60s
my-deployment-5454f687cd-gszb 1/1   Terminating       0          60s
my-deployment-5454f687cd-k6zf 1/1   Running           0          60s
my-deployment-78dc8dcb89-5927 0/1   ContainerCreating 0          1s
my-deployment-78dc8dcb89-dwtl 1/1   Running           0          5s
</code></pre>
<p>从上面可以看到，Kubernetes 会永远保持有 3 个 Pods 在正常运作，如果有新的 Pod 还在 ContainerCreating 的阶段时，他还不会关掉对应要被取代的 Pod。而在过一段时间我们输入同样指令可以看到</p>
<p><code>kubectl rollout history deployment my-deployment</code></p>
<p>看到我们目前更改过的版本</p>
<pre><code class="language-bash">deployment.extensions/my-deployment
REVISION  CHANGE-CAUSE
1         &lt;none&gt;
2         &lt;none&gt;
</code></pre>
<p>从上面可以看出来，我们目前有两个版本，如果我们发现版本 2 的程式有问题，想要先让服务先恢复成版本 1 的程式（Rollback）时，我们还可以透过指令</p>
<p><code>kubectl rollout undo deploy my-deployment</code></p>
<p>让我们的 Pod 都恢复成版本 1。甚至之后如果版本变的较多后，我们也可以指定要 Rollback 到的版本</p>
<p><code>kubectl rollout undo deploy my-deployment --to-revision=2</code></p>
<p><strong>Ingress</strong><br>
了解完了 Service 跟 Deployment 後，接下來就輪到概ㄓ念稍微複雜的 Ingress 元件了。 在上面有提到 Service 就是 Kubernetes 中用來定義「一群 Pod 要如何被連線及存取」的元件。 但在 Service 中，我們是將每個 Service 元件對外的 port number 跟 Node 上的 port number 做 mapping，這樣在我們的 Service 變多時，port number 以及分流規則的管理變得相當困難。</p>
<p>而 Ingress 可以透過 HTTP/HTTPS，在我們眾多的 Service 前搭建一個 reverse-proxy。這樣 Ingress 可以幫助我們統一一個對外的 port number，並且根據 hostname 或是 pathname 決定封包要轉發到哪個 Service 上，如同下圖的比較：</p>
<figure data-type="image" tabindex="2"><img src="https://chriswsq.github.io/post-images/1597919286278.jpg" alt="" loading="lazy"></figure>
<p>在 Kubernetes 中，Ingress 这项服务其实是由 Ingress Resources、Ingress Server、Ingress Controller 构成。其中 Ingress Resources 就是定义 Ingress 的身分证，而 Ingress Server 则是实体化用来接收 HTTP/HTTPS 连线的网路伺服器。但实际上，Ingress Server 有各式各样的实作，就如同市面上的 Web Server 琳琅满目一样。因此，Ingress Controller 就是一个可以把定义好的 Ingress Resources 设定转换成特定 Ingress Server 实作的角色。</p>
<p>举例来说，Kubernetes 由官方维护的两种 Ingress Controller 就有 ingress-gce 跟 ingress-nginx，分别可以对应转换成 GCE 与 Nginx。也有其他非官方在维护的 Controller，详细的列表可见官网的 additional-controllers。</p>
<p>接下来我们要来试着建立一个 Ingress 物件去根据 hostname 转发封包到不同的 Pod 上面。所以第一步，我们要用 Deployment 建立好几个不同的 Pod。在这边我们直接透过准备好的两个Image 来建立其中的Container，blue-whale 这个Image 里的程式会监听3000 port 然后在浏览器上被存取时会吐出蓝色的鲸鱼，purple-whale 则会吐出紫色的鲸鱼。</p>
<p>deployment.yaml</p>
<pre><code class="language-yaml">apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: blue-nginx
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: blue-nginx
    spec:
      containers:
        - name: nginx
          image: hcwxd/blue-whale
          ports:
            - containerPort: 3000
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: purple-nginx
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: purple-nginx
    spec:
      containers:
        - name: nginx
          image: hcwxd/purple-whale
          ports:
            - containerPort: 3000
</code></pre>
<p>接着我们就可以透过 kubectl create -f deployment.yaml 建立好我们的 Pod。</p>
<pre><code class="language-yaml">AME                            READY   STATUS    RESTARTS  AGE
blue-nginx-6b68c797c7-28tkz    1/1     Running   0         60s
blue-nginx-6b68c797c7-8ww8l    1/1     Running   0         60s
purple-nginx-84854fd7c-8g4nl   1/1     Running   0         60s
purple-nginx-84854fd7c-tmrbs   1/1     Running   0         60s
</code></pre>
<p>建立好了 Pod 们后，接下来我们就要建立这些 Pod 对外的各自 Service，在这边我们会把各至 Container 上的 3000 port 全部都转到 80 port 上。</p>
<p>service.yaml</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: blue-service
spec:
  type: NodePort
  selector:
    app: blue-nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 3000
---
apiVersion: v1
kind: Service
metadata:
  name: purple-service
spec:
  type: NodePort
  selector:
    app: purple-nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 3000
</code></pre>
<p>透过 kubectl create -f service.yaml 建立好我们的 Pod。</p>
<pre><code class="language-yaml">NAME            TYPE      CLUSTER-IP      EXTERNAL-IP  PORT(S)        
blue-service    NodePort  10.111.192.164  &lt;none&gt;       80:30492/TCP
purple-service  NodePort  0.107.21.77     &lt;none&gt;       80:32086/TCP
</code></pre>
<p>最后，我们就可以来建立我们的主角 Ingress 了！在这边我们的Ingress 只有很简单的规则，他会把所有发送到blue.demo.com 的封包交给service blue-service 负责，而根据上面service.yaml 的定义，他会再转交给blue-nginx这个Pod。而发送给 purple.demo.com 则会转交给 purple-nginx。<br>
在这边，我们要先记得使用指令 minikube addons enable ingress 来启用 minikube 的 ingress 功能。接着，我们就来撰写 ingress 的身分证。</p>
<p>ingress.yaml</p>
<pre><code class="language-yaml">apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: web
spec:
  rules:
    - host: blue.demo.com
      http:
        paths:
          - backend:
              serviceName: blue-service
              servicePort: 80
    - host: purple.demo.com
      http:
        paths:
          - backend:
              serviceName: purple-service
              servicePort: 80
</code></pre>
<p>我们一样透过 kubectl create -f ingress.yaml 来建立我们的 ingress 物件。并使用 kubectl get ingress 来查看我们的 ingress 状况：</p>
<pre><code class="language-bash">NAME   HOSTS                           ADDRESS     PORTS   AGE
web    blue.demo.com,purple.demo.com   10.0.2.15   80      60s
</code></pre>
<p>接下来我们要来测试 ingress 有没有乖乖帮我们转发。因为我们的Cluster 实际上对外的ip 都是我们透过指令minikube ip 会看到的192.168.99.100，这样我们要怎么同时让这个ip 可以是我们设定规则中的blue.demo.com 以及purple.demo.com呢？</p>
<p>因为我们知道在 DNS 解析网址时，会先查找本机上 /etc/hosts 后才会到其他 DNS Server 上寻找。所以我们可以透过一个小技巧，在本机上把 blue.demo.com 以及 purple.demo.com 都指向 192.168.99.100。透过指令</p>
<pre><code class="language-bash">echo 192.168.99.100   blue.demo.com  &gt;&gt; /etc/hosts
echo 192.168.99.100   purple.demo.com &gt;&gt; /etc/hosts
</code></pre>
<p>或是透过 sudo vim /etc/hosts 手动加上这两条规则，我们就成功搞定 DNS 可以来测试了。接下来我们打开浏览器，输入 blue.demo.com 就可以得到熟悉的蓝色小鲸鱼</p>
<figure data-type="image" tabindex="3"><img src="https://chriswsq.github.io/post-images/1597919547364.png" alt="" loading="lazy"></figure>
<p>然后输入 purple.demo.com 就可以得到紫色小鲸鱼啰！</p>
<figure data-type="image" tabindex="4"><img src="https://chriswsq.github.io/post-images/1597919583300.png" alt="" loading="lazy"></figure>
<p>在实际建立过 Pod、Service、Deployment 还有 Ingress 后，在接下来的文章，我们要来介绍一个可以让这个建立流程变得更简单的工具，也就是 Kubernetes 中的 Package Manager：Helm！</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes基础教学（一）原理]]></title>
        <id>https://chriswsq.github.io/post/kubernetes-ji-chu-xue-xi-yi-yuan-li/</id>
        <link href="https://chriswsq.github.io/post/kubernetes-ji-chu-xue-xi-yi-yuan-li/">
        </link>
        <updated>2020-08-20T09:32:13.000Z</updated>
        <summary type="html"><![CDATA[<p>Kubernetes（K8S）是一个可以帮助我们管理微服务（microservices）的系统，他可以自动化地部署及管理多台机器上的多个容器（Container）。<br>
更进一步地说，Kubernetes 想解决的问题是：「手动部署多个容器到多台机器上并监测管理这些容器的状态非常麻烦。」而Kubernetes 要提供的解法： 提供一个平台以较高层次的抽象化去自动化操作与管理容器们。</p>
]]></summary>
        <content type="html"><![CDATA[<p>Kubernetes（K8S）是一个可以帮助我们管理微服务（microservices）的系统，他可以自动化地部署及管理多台机器上的多个容器（Container）。<br>
更进一步地说，Kubernetes 想解决的问题是：「手动部署多个容器到多台机器上并监测管理这些容器的状态非常麻烦。」而Kubernetes 要提供的解法： 提供一个平台以较高层次的抽象化去自动化操作与管理容器们。</p>
<!-- more -->
<p>打开 Kubernetes 的官网，我们可以看到关于 Kubernetes 服务的描述为：</p>
<blockquote>
<p>Automated container deployment, scaling, and management</p>
</blockquote>
<p>而白话来说，上面的描述表示他可以做到：</p>
<ul>
<li>同时部署多个容器到多台机器上（Deployment）</li>
<li>服务的乘载量有变化时，可以对容器做自动扩展（Scaling）</li>
<li>管理多个容器的状态，自动侦测并重启故障的容器（Management）</li>
</ul>
<h2 id="kubernetes-四元件">Kubernetes 四元件</h2>
<p>在了解 Kubernetes 如何帮助我们管理容器们前，我们先要由小到大依序了解组成 Kubernetes 的四种最基本的元件：Pod、Worker Node、Master Node、Cluster。</p>
<p><strong>Pod</strong></p>
<p>Kubernetes 运作的最小单位，一个 Pod 对应到一个应用服务（Application） ，举例来说一个 Pod 可能会对应到一个 API Server。</p>
<ul>
<li>每个 Pod 都有一个身分证，也就是属于这个 Pod 的 yaml 档</li>
<li>一个 Pod 里面可以有一个或是多个 Container，但一般情况一个 Pod 最好只有一个 Container</li>
<li>同一个 Pod 中的 Containers 共享相同资源及网路，彼此透过 local port number 沟通</li>
</ul>
<p><strong>Worker Node</strong></p>
<p>Kubernetes 运作的最小硬体单位，一个Worker Node（简称Node）对应到一台机器，可以是实体机如你的笔电、或是虚拟机如AWS 上的一台EC2 或GCP 上的一台Computer Engine 。</p>
<p>每个 Node 中都有三个组件：kubelet、kube-proxy、Container Runtime。</p>
<blockquote>
<p>小提醒：在 Worker Node 与 Master Node 的组件部分，因为 Kubernetes 本身其实都抽象的很好，所以在 Kubernetes 「基础的」使用上如何不了解这些组建也不会有非常大的影响。</p>
</blockquote>
<p><strong>kubelet</strong></p>
<ul>
<li>该 Node 的管理员，负责管理该 Node 上的所有 Pods 的状态并负责与 Master 沟通</li>
</ul>
<p><strong>kube-proxy</strong></p>
<ul>
<li>该 Node 的传讯员，负责更新 Node 的 iptables，让 Kubernetes 中不在该 Node 的其他物件可以得知该 Node 上所有 Pods 的最新状态</li>
</ul>
<p><strong>kube-apiserver</strong></p>
<ul>
<li>
<p>管理整个 Kubernetes 所需 API 的接口（Endpoint），例如从 Command Line 下 kubectl 指令就会把指令送到这里</p>
</li>
<li>
<p>负责 Node 之间的沟通桥梁，每个 Node 彼此不能直接沟通，必须要透过 apiserver 转介<br>
负责 Kubernetes 中的请求的身份认证与授权</p>
</li>
</ul>
<p><strong>etcd</strong></p>
<ul>
<li>用来存放 Kubernetes Cluster 的资料作为备份，当 Master 因为某些原因而故障时，我们可以透过 etcd 帮我们还原 Kubernetes 的状态</li>
</ul>
<p><strong>kube-controller-manager</strong></p>
<ul>
<li>负责管理并运行 Kubernetes controller 的组件，简单来说 controller 就是 Kubernetes 里一个个负责监视 Cluster 状态的 Process，例如：Node Controller、Replication Controller</li>
<li>这些 Process 会在 Cluster 与预期状态（desire state）不符时尝试更新现有状态（current state）。例如：现在要多开一台机器以应付突然增加的流量，那我的预期状态就会更新成 N+1，现有状态为 N，这时相对应的 controller 就会想办法多开一台机器</li>
<li>controller-manager 的监视与尝试更新也都需要透过访问 kube-apiserver 达成</li>
</ul>
<p><strong>kube-scheduler</strong></p>
<p>整个Kubernetes 的Pods 调度员，scheduler 会监视新建立但还没有被指定要跑在哪个Node 上的Pod，并根据每个Node 上面资源规定、硬体限制等条件去协调出一个最适合放置的Node 让该Pod 跑</p>
<p><strong>Cluster</strong></p>
<p>Kubernetes 中多个 Node 与 Master 的集合。基本上可以想成在同一个环境里所有 Node 集合在一起的单位。</p>
<h2 id="基本运作与安装">基本运作与安装</h2>
<figure data-type="image" tabindex="1"><img src="https://chriswsq.github.io/post-images/1597917479477.jpg" alt="" loading="lazy"></figure>
<p>因原文为外网才能访问所以转载<br>
<a href="https://medium.com/@C.W.Hu/kubernetes-basic-concept-tutorial-e033e3504ec0">原文链接</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[k8s——apiVersion对照表]]></title>
        <id>https://chriswsq.github.io/post/k8s-apiversion-dui-zhao-biao/</id>
        <link href="https://chriswsq.github.io/post/k8s-apiversion-dui-zhao-biao/">
        </link>
        <updated>2020-08-20T07:10:24.000Z</updated>
        <content type="html"><![CDATA[<!-- more -->
<p>不同的控制器选用不同的apiVersion</p>
<!-- more -->
<h2 id="对照表">对照表</h2>
<table>
<thead>
<tr>
<th>kind</th>
<th>apiVersion</th>
</tr>
</thead>
<tbody>
<tr>
<td>CertificateSigningRequest</td>
<td>certificates.k8s.io/v1beta1</td>
</tr>
<tr>
<td>ClusterRoleBinding</td>
<td>rbac.authorization.k8s.io/v1</td>
</tr>
<tr>
<td>ClusterRole</td>
<td>rbac.authorization.k8s.io/v1</td>
</tr>
<tr>
<td>ComponentStatus</td>
<td>v1</td>
</tr>
<tr>
<td>ConfigMap</td>
<td>v1</td>
</tr>
<tr>
<td>ControllerRevision</td>
<td>apps/v1</td>
</tr>
<tr>
<td>CronJob</td>
<td>batch/v1beta1</td>
</tr>
<tr>
<td>DaemonSet</td>
<td>extensions/v1beta1</td>
</tr>
<tr>
<td>Deployment</td>
<td>extensions/v1beta1</td>
</tr>
<tr>
<td>Endpoints</td>
<td>v1</td>
</tr>
<tr>
<td>Event</td>
<td>v1</td>
</tr>
<tr>
<td>HorizontalPodAutoscaler</td>
<td>autoscaling/v1</td>
</tr>
<tr>
<td>Ingress</td>
<td>extensions/v1beta1</td>
</tr>
<tr>
<td>Job</td>
<td>batch/v1</td>
</tr>
<tr>
<td>LimitRange</td>
<td>v1</td>
</tr>
<tr>
<td>Namespace</td>
<td>v1</td>
</tr>
<tr>
<td>NetworkPolicy</td>
<td>extensions/v1beta1</td>
</tr>
<tr>
<td>Node</td>
<td>v1</td>
</tr>
<tr>
<td>PersistentVolumeClaim</td>
<td>v1</td>
</tr>
<tr>
<td>PersistentVolume</td>
<td>v1</td>
</tr>
<tr>
<td>PodDisruptionBudget</td>
<td>policy/v1beta1</td>
</tr>
<tr>
<td>Pod</td>
<td>v1</td>
</tr>
<tr>
<td>PodSecurityPolicy</td>
<td>extensions/v1beta1</td>
</tr>
<tr>
<td>PodTemplate</td>
<td>v1</td>
</tr>
<tr>
<td>ReplicaSet</td>
<td>extensions/v1beta1</td>
</tr>
<tr>
<td>ReplicationController</td>
<td>v1</td>
</tr>
<tr>
<td>ResourceQuota</td>
<td>v1</td>
</tr>
<tr>
<td>RoleBinding</td>
<td>rbac.authorization.k8s.io/v1</td>
</tr>
<tr>
<td>Role</td>
<td>rbac.authorization.k8s.io/v1</td>
</tr>
<tr>
<td>Secret</td>
<td>v1</td>
</tr>
<tr>
<td>ServiceAccount</td>
<td>v1</td>
</tr>
<tr>
<td>Service</td>
<td>v1</td>
</tr>
<tr>
<td>StatefulSet</td>
<td>apps/v1</td>
</tr>
</tbody>
</table>
<h2 id="apiversion分别是什么意思">apiVersion分别是什么意思？</h2>
<p>名称中带有'alpha'的alpha API版本是Kubernetes中新功能的早期候选者。这些可能包含错误，并且不能保证将来能够正常工作。</p>
<p><strong>测试</strong><br>
在API版本名称意味着测试已取得进展过去的阿尔法水平，并且该功能最终将被列入Kubernetes“测试”。尽管其工作方式可能会发生变化，并且对象的定义方式可能会发生完全变化，但功能本身很有可能以某种形式将其纳入Kubernetes。</p>
<p><strong>稳定名称中</strong><br>
不包含“ alpha”或“ beta”。它们是安全使用的。</p>
<hr>
<p><strong>v1</strong><br>
这是Kubernetes API的第一个稳定版本。它包含许多核心对象。</p>
<p><strong>apps / v1</strong><br>
apps是Kubernetes中最常见的API组，其中许多核心对象均来自v1。它包括与在Kubernetes上运行应用程序相关的功能，例如Deployments，RollingUpdates和ReplicaSets。</p>
<p><strong>autoscaling / v1</strong><br>
此API版本允许根据不同的资源使用量指标对pod进行自动缩放。此稳定版本仅支持CPU扩展，但是将来的alpha和beta版本将允许您根据内存使用情况和自定义指标进行扩展。</p>
<p><strong>batch/v1</strong><br>
的batchAPI组包含与批处理和作业样的任务（而不是应用类一样无限期运行Web服务器的任务）的对象。该apiVersion是这些API对象的第一个稳定版本。</p>
<p><strong>batch / v1beta1</strong><br>
Beta版本，用于Kubernetes中批处理对象的新功能，特别是包括CronJobs，它使您可以在特定的时间或周期性地运行Jobs。</p>
<p><strong>certificate.k8s.io/v1beta1</strong><br>
此API版本增加了用于验证网络证书以在群集中进行安全通信的功能。您可以阅读更多有关官方文档的信息。</p>
<p><strong>extensions / v1beta1</strong><br>
此版本的API包括Kubernetes的许多常用新功能。在此版本中，部署，DaemonSet，副本集和Ingress都进行了重大更改。</p>
<p>请注意，在Kubernetes 1.6中，其中一些对象已从重新定位extensions到特定的API组（例如apps）。当这些对象移出测试版时，应将它们归入特定的API组，例如apps/v1。使用extensions/v1beta1正变得过时，尝试用在可能情况下，根据您的Kubernetes集群版本的特定API组。</p>
<p><strong>policy / v1beta1</strong><br>
此apiVersion增加了设置容器中断预算和有关容器安全性的新规则的功能。</p>
<p><strong>rbac.authorization.k8s.io/v1</strong><br>
此apiVersion包含用于Kubernetes基于角色的访问控制的附加功能。这可以帮助您保护群集。查看官方博客文章。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[NFS网络文件系统部署]]></title>
        <id>https://chriswsq.github.io/post/nfs-wang-luo-wen-jian-xi-tong-bu-shu/</id>
        <link href="https://chriswsq.github.io/post/nfs-wang-luo-wen-jian-xi-tong-bu-shu/">
        </link>
        <updated>2020-08-20T06:58:24.000Z</updated>
        <summary type="html"><![CDATA[<p>FS（Network File System），即网络文件系统。NFS服务可以将远程Linux系统上的文件共享资源挂载到本地主机的目录上，从而使用本地主机（Linux客户端）像使用本地资源那样读写远程Linux系统上的共享资源。</p>
]]></summary>
        <content type="html"><![CDATA[<p>FS（Network File System），即网络文件系统。NFS服务可以将远程Linux系统上的文件共享资源挂载到本地主机的目录上，从而使用本地主机（Linux客户端）像使用本地资源那样读写远程Linux系统上的共享资源。</p>
<!-- more -->
<h2 id="系统环境">系统环境</h2>
<table>
<thead>
<tr>
<th>主机名称</th>
<th>操作系统</th>
<th>IP地址</th>
</tr>
</thead>
<tbody>
<tr>
<td>NFS服务器</td>
<td>Centos 7 64位</td>
<td>192.168.40.6</td>
</tr>
<tr>
<td>NFS客户端</td>
<td>Centos 7 64位</td>
<td>192.168.40.7</td>
</tr>
</tbody>
</table>
<h2 id="nfs服务配置文件的参数">NFS服务配置文件的参数</h2>
<table>
<thead>
<tr>
<th>参数</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>ro</td>
<td></td>
</tr>
<tr>
<td>rw</td>
<td>读写</td>
</tr>
<tr>
<td>root_squash</td>
<td>当NFS客户端以root管理员访问时，映射为NFS服务器的匿名用户</td>
</tr>
<tr>
<td>no_root_squash</td>
<td>当NFS客户端以root管理员访问时，映射为NFS服务器的root管理员</td>
</tr>
<tr>
<td>all_squash</td>
<td>无论NFS客户端使用什么账户访问，均映射为NFS服务器的匿名用户</td>
</tr>
<tr>
<td>sync</td>
<td>同时将数据写入到内存与硬盘中，保证不丢失数据</td>
</tr>
<tr>
<td>async</td>
<td>优先将数据写入到内存，然后再写入硬盘；这样效率更高，但可能会丢失数据</td>
</tr>
</tbody>
</table>
<h2 id="nfs服务器操作">NFS服务器操作</h2>
<h3 id="1-安装nfs">1、安装NFS</h3>
<p><code>yum -y install nfs-utils</code></p>
<h3 id="2-创建用于nfs共享的目录">2、创建用于NFS共享的目录</h3>
<pre><code class="language-bash">mkdir /test
chown -R nfsnobody /nfsdata
</code></pre>
<h3 id="3-编辑nfs的配置文件添加如下内容">3、编辑NFS的配置文件，添加如下内容</h3>
<p><em>注：NFS的配置文件默认是没有内容的</em></p>
<pre><code class="language-conf">vim /etc/exports
/nfsdata 192.168.40.*(rw,sync,root_squash)
</code></pre>
<h3 id="4-启动nfs服务并加入开机启动项">4、启动NFS服务，并加入开机启动项</h3>
<p>NFS服务需要使用RPC（Remote Procedure Call，远程过程调用）服务将NFS服务器的IP地址和端口号等信息发送给客户端，因此，在启动NFS服务之前，还需要顺带启动rpcbind服务。</p>
<pre><code class="language-shell">systemctl start rpcbind
systemctl enable rpcbind
systemctl start nfs-server
systemctl enable nfs-server
</code></pre>
<h3 id="5-查看nfs向rpc注册的端口信息">5、查看nfs向rpc注册的端口信息</h3>
<p><code>rpcinfo -p localhost</code><br>
<em>注：下图中用红框括起来的端口号需要防火墙允许</em><br>
<img src="/images/nfs/nfs.jpg" alt="" loading="lazy"></p>
<h3 id="6-配置firewalld防火墙允许nfs和rpc端口">6、配置firewalld防火墙，允许nfs和rpc端口</h3>
<pre><code class="language-bash">firewall-cmd --permanent --add-service=nfs
firewall-cmd --permanent --add-service=mountd
firewall-cmd --permanent --add-port=111/tcp
firewall-cmd --permanent --add-port=111/udp
firewall-cmd --reload
</code></pre>
<h2 id="nfs客户端操作">NFS客户端操作</h2>
<h3 id="1-使用showmount命令查询nfs服务器的远程共享信息">1、使用showmount命令查询NFS服务器的远程共享信息</h3>
<p><strong>表3：showmount命令可用的参数以及作用</strong></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>-e</td>
<td>显示NFS服务器的共享列表</td>
</tr>
<tr>
<td>-a</td>
<td>显示本机挂载的文件资源的情况</td>
</tr>
<tr>
<td>-v</td>
<td>显示版本号</td>
</tr>
</tbody>
</table>
<p>查看能否连接到nfs服务</p>
<pre><code>showmount -e 192.168.40.6
</code></pre>
<h3 id="2-创建挂载目录并挂载">2、创建挂载目录，并挂载</h3>
<pre><code class="language-bash">mkdir /nfsdata
mount -t nfs 192.168.40.6:/nfsdata /nfsdata
df -h
</code></pre>
<h3 id="3-将挂载信息写入etcfstab文件中以便开机自动挂载">3、将挂载信息写入/etc/fstab文件中，以便开机自动挂载</h3>
<pre><code class="language-bash">vim /etc/fstab
192.168.40.6:/nfsdata /nfsdata nfs defaults 0 0
</code></pre>
<p>4、测试<br>
往/nfsdata目录下写入一个文件</p>
<pre><code>echo &quot;welcome to xuad.com&quot; &gt; /nfsdata/xuad.txt
</code></pre>
<p>在NFS服务器上查看/nfsdata目录下是否生成了此文件</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[sed&awk笔记]]></title>
        <id>https://chriswsq.github.io/post/sedandawk-bi-ji/</id>
        <link href="https://chriswsq.github.io/post/sedandawk-bi-ji/">
        </link>
        <updated>2020-08-20T06:49:30.000Z</updated>
        <summary type="html"><![CDATA[<p>sed&amp;awk 提升工作效率的小工具</p>
]]></summary>
        <content type="html"><![CDATA[<p>sed&amp;awk 提升工作效率的小工具</p>
<!-- more -->
<h2 id="删除行首空格或者tab">删除行首空格或者tab</h2>
<p><code>sed -i 's/^[ \t]*//g' file</code></p>
<p>删除行尾空格或者tab<br>
<code>sed -i 's/[ \t]*$//g' file</code></p>
<h2 id="注释特定行">注释特定行</h2>
<pre><code>sed -i '/swapfile/s/^/#/' /etc/fstab
sed -i '/xvdb/s/^/#/g' /etc/fstab
sed -i '/vdb/s/^/#/g' /etc/fstab
</code></pre>
<h2 id="取消注释">取消注释</h2>
<pre><code>sed -i '/swapfile/s/^#//' /etc/fstab
sed -i '/xvdb/s/^#//g' /etc/fstab
sed -i '/vdb/s/^#//g' /etc/fstab
</code></pre>
<h2 id="注释未注释行">注释未注释行</h2>
<p><code>sed -i 's/^[^#]/#&amp;/' /var/spool/cron/root</code></p>
<h2 id="首字母大写">首字母大写</h2>
<p><code>sed 's/\b[a-z]/\U&amp;/g' file</code></p>
<h2 id="首字母小写">首字母小写</h2>
<p><code>sed 's/\b[a-Z]/\L&amp;/g' file</code></p>
<h2 id="在包含某个字符的上一行或者下一行插入内容">在包含某个字符的上一行或者下一行插入内容</h2>
<pre><code>[root@RedHat test]# cat testfile 
hello
[root@RedHat test]# sed -i '/hello/i\\up' testfile 
[root@RedHat test]# cat testfile 
up
hello
[root@RedHat test]# sed -i '/hello/a\\down' testfile 
[root@RedHat test]# cat testfile 
up
hello
down
[root@RedHat test]#
</code></pre>
<p><strong>假如有两个关键字hello，那么在每一行上面或者下面都插入内容</strong></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[xuperchain添加旷工节点]]></title>
        <id>https://chriswsq.github.io/post/xuperchain-tian-jia-kuang-gong-jie-dian/</id>
        <link href="https://chriswsq.github.io/post/xuperchain-tian-jia-kuang-gong-jie-dian/">
        </link>
        <updated>2020-08-20T06:39:53.000Z</updated>
        <summary type="html"><![CDATA[<p>xuperchain通过提案动态添加旷工节点</p>
]]></summary>
        <content type="html"><![CDATA[<p>xuperchain通过提案动态添加旷工节点</p>
<!-- more -->
<h2 id="因为创建链时一次性的操作那么后续需要改动关于xuperjson里面的参数则要通过进行提案来进行修改">因为创建链时一次性的操作，那么后续需要改动关于xuper.json里面的参数则要通过进行提案来进行修改</h2>
<p>一共两个步骤</p>
<ul>
<li>发起提案</li>
<li>投票</li>
</ul>
<h3 id="编写提案文件">编写提案文件</h3>
<p>首先查看当前块高度，因为提案文件的 投票截止高度和生效高度是根据目前高度来写的</p>
<pre><code class="language-bash">./xchain-cli status -H 192.168.40.6:37101 | jq '.blockchains[] | {&quot;name&quot;:.name,&quot;height&quot;:.ledger.trunkHeight}'

{
  &quot;name&quot;: &quot;xuper&quot;,
  &quot;height&quot;: 26505
}

</code></pre>
<p>首先需要准备一个提案的文件，json格式</p>
<p>proposal.json</p>
<pre><code class="language-json">
{
    &quot;module&quot;: &quot;proposal&quot;,
    &quot;method&quot;: &quot;Propose&quot;,
    &quot;args&quot; : {
        &quot;min_vote_percent&quot;: 51,
        &quot;stop_vote_height&quot;: 26600
    },
    &quot;trigger&quot;: {
        &quot;height&quot;: 26630,
        &quot;module&quot;: &quot;consensus&quot;,
        &quot;method&quot;: &quot;update_consensus&quot;,
        &quot;args&quot; : {
            &quot;name&quot;: &quot;tdpos&quot;,
            &quot;config&quot;: {
                &quot;version&quot;:&quot;21&quot;,
                &quot;proposer_num&quot;:&quot;3&quot;,
                &quot;period&quot;:&quot;3000&quot;,
                &quot;alternate_interval&quot;:&quot;6000&quot;,
                &quot;term_interval&quot;:&quot;9000&quot;,
                &quot;block_num&quot;:&quot;20&quot;,
                &quot;vote_unit_price&quot;:&quot;1&quot;,
                &quot;init_proposer&quot;: {
                    &quot;1&quot;:[&quot;2B1rDQhq7W4TStSHoD88N1SUYXrCDV821v&quot;, &quot;rwGpYwpkcpMgxdGJ9KX9xSvJPiCyPsFVQ&quot;,&quot;262G4VuXBmFg6W486XqY4bj2iMotHG5ypb&quot;]
                }
            }
        }
    }
}
</code></pre>
<figure data-type="image" tabindex="1"><img src="/images/xuperchain/proposal.jpg" alt="" loading="lazy"></figure>
<blockquote>
<p>注意：</p>
<ul>
<li>提案文件里不能有注释</li>
<li>需要注意的是当前的区块高度，来设置合理的截至计票高度和生效高度</li>
</ul>
</blockquote>
<h3 id="转账">转账</h3>
<p>然后在矿工节点下，执行给自己转账的操作，并在 --desc 参数里传入提案</p>
<pre><code class="language-bash">./xchain-cli transfer --to dpzuVdosQrF2kmzumhVeFQZa1aYcdgFpN --desc proposal.json --amount 1

</code></pre>
<p>运行后会得到本次提案的交易id，需要记录下来供投票使用</p>
<blockquote>
<p>转账地址写当前天的节点就可以，这里的交易金额写 1 也行，官方是100</p>
</blockquote>
<h3 id="投票">投票</h3>
<p>对提案进行投票操作由如下命令执行：</p>
<h4 id="查看当前总金额">查看当前总金额</h4>
<pre><code class="language-bash">./xchain-cli  status  | grep utxoTotal
10000000523
</code></pre>
<pre><code class="language-bash">./xchain-cli vote f26d670b695d9fd5da503a34d130ef19e738b35e031b18b70ad4cbbf6dfe2656 --frozen 26650 --amount 100002825031900000000

</code></pre>
<blockquote>
<p>注意：</p>
<ul>
<li>
<p>--frozen 参数的冻结高度大于提案生效的高度，也就是大于26630。</p>
</li>
<li>
<p>这里需要注意进行投票的节点需要有矿工账号的密钥对</p>
</li>
<li>
<p>因为最终通过的规则是投票资源大于总资源的51%，所以需要初始token量最多的矿工账号来进行投票，并保证token数符合要求。</p>
</li>
<li>
<p>--amount 金额为大于总金额的51%</p>
</li>
</ul>
</blockquote>
<h3 id="查看状态">查看状态</h3>
<p>待到当前块到生效高度时查看当前tdpos算法的状态</p>
<pre><code class="language-bash">[root@test-1 output]# ./xchain-cli tdpos status
{
  &quot;term&quot;: 21,
  &quot;block_num&quot;: 4,
  &quot;proposer&quot;: &quot;rwGpYwpkcpMgxdGJ9KX9xSvJPiCyPsFVQ&quot;,
  &quot;proposer_num&quot;: 3,
  &quot;checkResult&quot;: [
    &quot;2B1rDQhq7W4TStSHoD88N1SUYXrCDV821v&quot;,
    &quot;rwGpYwpkcpMgxdGJ9KX9xSvJPiCyPsFVQ&quot;,
    &quot;262G4VuXBmFg6W486XqY4bj2iMotHG5ypb&quot;
  ]
}

</code></pre>
<blockquote>
<p>注意：</p>
<p>刚到投票生效的时间执行此命令查看的时候可能会有报错情况</p>
<p>[root@test-1 output]# ./xchain-cli  tdpos status<br>
rpc error: code = Unknown desc = leveldb: not found</p>
<p>等一会就好了</p>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[生产xuperchain部署文档]]></title>
        <id>https://chriswsq.github.io/post/sheng-chan-xuperchain-bu-shu-wen-dang/</id>
        <link href="https://chriswsq.github.io/post/sheng-chan-xuperchain-bu-shu-wen-dang/">
        </link>
        <updated>2020-08-20T03:02:14.000Z</updated>
        <summary type="html"><![CDATA[<p>百度区块链部署xuperchain</p>
]]></summary>
        <content type="html"><![CDATA[<p>百度区块链部署xuperchain</p>
<!-- more -->
<h2 id="部署信息">部署信息</h2>
<p>**版本：**3.7.0<br>
**加密方式：**国密<br>
**共识：**tdpos<br>
**链类型：**联盟链</p>
<p><strong>以下为初始化 建链前的一次性操作（建链后再修改需要进提案），修改完后创建xuper链，并将ouput目录拷贝至其他节点，后续其他节点根据情况修改个别需要节点唯一的内容即可（keys的三个文件 ,netURL地址）</strong></p>
<h3 id="修改outputconfpluginsconf-文件将default的加密方式改为国密方式">修改/output/conf/plugins.conf 文件(将default的加密方式改为国密方式)</h3>
<figure data-type="image" tabindex="1"><img src="https://chriswsq.github.io/post-images/1597892994286.jpg" alt="" loading="lazy"></figure>
<h3 id="修改-outputconfxchainyaml文件">修改 /output/conf/xchain.yaml文件</h3>
<pre><code class="language-yaml">#在 utxo 区域中添加 `nonUtxo: true`  参数
utxo:
  nonUtxo: true    #

# wasm合约配置
wasm:
  driver: &quot;xvm&quot;
  enableUpgrade: true

#能够创建平行链的节点
kernel:
  # minNewChainAmount 设置创建平行链时最少要转多少钱到同链名address
  minNewChainAmount: &quot;10&quot;
  newChainWhiteList:
    - 25nbSZeSjMs8GT4TiSmgofnRusg1rxxggV: true

# 是否开启默认的XEndorser背书服务
enableXEndorser: true

</code></pre>
<h3 id="修改outputdataconfigxuperjson文件拷贝至其他节点选择挖矿节点-挖矿节点数量-address地址为主节点即可-更改时间戳-挖矿节点neturl">修改/output/data/config/xuper.json文件，拷贝至其他节点（选择挖矿节点、挖矿节点数量、address地址为主节点即可、更改时间戳、挖矿节点netURL)</h3>
<figure data-type="image" tabindex="2"><img src="https://chriswsq.github.io/post-images/1597905583948.jpg" alt="" loading="lazy"></figure>
<blockquote>
<p>注意： 此模板文件节点之间必须一致，否则会出现块高不一致的情况<br>
maxblocksize、proposer_num、period、alternate_interval、term_interval、block_num参数根据实际情况修改，我这里是测试环境的配置</p>
</blockquote>
<h2 id="拷贝至其他节点后的操作">拷贝至其他节点后的操作</h2>
<p>拷贝时确保主节点模板文件没有问题，不会再更改，这样拷贝过去后，不需要重新建链，如改动/output/data/config/xuper.json文件后，需要重新生成链，</p>
<ul>
<li>重新生成 neturl 地址(生成后的地址IP替换为实际IP)<br>
<code>./xchain-cli netURL gen</code></li>
<li>重新生成address地址（生产环境为开发提供每个节点的address地址）<br>
<code>./xchain-cli account newkeys -f</code></li>
<li>修改xchain.yaml文件 p2p 区域的 netURL 地址，连接主节点  （主节点开始先别连接其他节点，待环境部署好后可停止刚开始的主节点然后添加其他节点，使全部节点互相备份）</li>
</ul>
<h2 id="启动服务">启动服务</h2>
<pre><code class="language-bash">#删除 data/blockchain/*文件
rm -rf   data/blockchain/*

#创建链
./xchain-cli createChain

#启动服务节点
nohup ./xchain &amp;
</code></pre>
<h2 id="注">注</h2>
<ul>
<li>如果是单台机器启动多节点需修改三个端口（RPC、metricPort、p2p）</li>
<li>先启动bootnode节点</li>
<li>先修改插件的默认加密方式为国密后，手动生成的私有公钥和秘钥都会以国密的加密方式生成。</li>
</ul>
<h2 id="创建合约账号">创建合约账号</h2>
<p>如何要支持群组，需要在xuper链部署一个系统合约：GroupChain（一个网络有且仅有一个）</p>
<pre><code class="language-bash"># 在xuper链部署GroupChain合约
#拷贝 group_chain.wasm 文件到 outpue 目录(group_chain.wasm文件由开发提供)
./xchain-cli account new --account 1111111111111111
./xchain-cli wasm deploy --account XC1111111111111111@xuper --cname group_chain ./group_chain.wasm
./xchain-cli account contracts --account XC1111111111111111@xuper -H 127.0.0.1:37101
</code></pre>
<h3 id="创建平行链">创建平行链</h3>
<p>此步骤在测试阶段可手动进行测试，正式环境集成在程序内<br>
创建平行链应的节点应和xuper.json文件中指定地址、xchain.conf文件中允许创建平行链的地址一致</p>
<pre><code class="language-bash">#创建群组
./xchain-cli wasm invoke group_chain --method addChain -a '{&quot;bcname&quot;:&quot;xchain_chriswang&quot;}'

#添加节点(可在网上https://www.bejson.com中检查；可添加多个节点)
./xchain-cli wasm invoke group_chain --method addNode -a '{&quot;bcname&quot;:&quot;baidu_zhengqi3&quot;, &quot;ip&quot;:&quot;/ip4/192.168.52.4/tcp/40001/p2p/QmRUVTKqdYVE49VzQKEuGsr2afiwDkj1RMMtdcVdaaqLUX&quot;, &quot;address&quot;:&quot;qaXhH7gJcdfpapmWkbHdLNqUFq3Vst6Am&quot;}'

#查看node节点，检查是否添加成功
./xchain-cli wasm query group_chain --method listNode -a '{&quot;bcname&quot;:&quot;xchain_chriswang&quot;}'
</code></pre>
<pre><code class="language-json"># 创建平行链

#创建平行链的json文件（模版），如下：

{
    &quot;Module&quot;: &quot;kernel&quot;,
    &quot;Method&quot;: &quot;CreateBlockChain&quot;,
    &quot;Args&quot;: {
        &quot;name&quot;: &quot;xchain_chriswsq&quot;,
        &quot;data&quot;: &quot;{\&quot;version\&quot;:\&quot;1\&quot;,\&quot;predistribution\&quot;:[{\&quot;address\&quot;:\&quot;266L6fw9rBXSm4uBciLwRWTkonb2HCqz5a\&quot;,\&quot;quota\&quot;:\&quot;100000000000000000000\&quot;}],\&quot;maxblocksize\&quot;:\&quot;128\&quot;,\&quot;award\&quot;:\&quot;1000000\&quot;,\&quot;decimals\&quot;:\&quot;8\&quot;,\&quot;award_decay\&quot;:{\&quot;height_gap\&quot;:31536000,\&quot;ratio\&quot;:1},\&quot;gas_price\&quot;:{\&quot;cpu_rate\&quot;:1000,\&quot;mem_rate\&quot;:1000000,\&quot;disk_rate\&quot;:1,\&quot;xfee_rate\&quot;:1},\&quot;new_account_resource_amount\&quot;:1000,\&quot;crypto\&quot;:\&quot;gm\&quot;,\&quot;genesis_consensus\&quot;:{\&quot;name\&quot;:\&quot;tdpos\&quot;,\&quot;config\&quot;:{\&quot;timestamp\&quot;:\&quot;1559021720000000000\&quot;,\&quot;proposer_num\&quot;:\&quot;1\&quot;,\&quot;period\&quot;:\&quot;3000\&quot;,\&quot;alternate_interval\&quot;:\&quot;3000\&quot;,\&quot;term_interval\&quot;:\&quot;6000\&quot;,\&quot;block_num\&quot;:\&quot;20\&quot;,\&quot;vote_unit_price\&quot;:\&quot;1\&quot;,\&quot;init_proposer\&quot;:{\&quot;1\&quot;:[\&quot;266L6fw9rBXSm4uBciLwRWTkonb2HCqz5a\&quot;]},\&quot;init_proposer_neturl\&quot;:{\&quot;1\&quot;:[\&quot;/ip4/127.0.0.1/tcp/47101/p2p/QmP38zphQmThRkK5DtiR7iN2ERDiZ1YZF5aXgJHjszj79t\&quot;]}}}}&quot;
    }
}
</code></pre>
<p>使用如下指令即可创建平行链,转了100个主链的token到平行链同名的address，作为创建链的代价：</p>
<pre><code>./xchain-cli transfer --to baidu_zhengqi3 --amount 100 --desc xchain_chriswang.json

02a19467620d01cea01242c94442a39e550e8ae30bc92eaffb341aeaf5f0fa66
</code></pre>
<h2 id="基本命令">基本命令</h2>
<pre><code class="language-bash"># 创建普通用户, 包含地址，公钥，私钥
./xchain-cli account newkeys  -f

# 获取本地netURL地址
./xchain-cli netURL get -H 127.0.0.1:37101

 #重新生成本地节点的网络私钥
./xchain-cli netURL gen

#显示本地节点的p2p地址
./xchain-cli netURL preview

# 创建xuper链
./xchain-cli createChain

# check服务运行状况
./xchain-cli status -H 127.0.0.1:37101

#查看块高 确保每个节点一致变化
./xchain-cli status -H 127.0.0.1:37101 | jq '.blockchains[] | {&quot;name&quot;:.name,&quot;height&quot;:.ledger.trunkHeight}'

#查看本节点账户余额
./xchain-cli account balance --keys data/keys -H 127.0.0.1:37101

#查看链总账户余额（要注意如果多个链可能会有多个utxoTotal）
./xchain-cli status  | grep utxoTotal  

#查看tdpos共识的状态
./xchain-cli tdpos status
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ansible学习]]></title>
        <id>https://chriswsq.github.io/post/ansible-xue-xi/</id>
        <link href="https://chriswsq.github.io/post/ansible-xue-xi/">
        </link>
        <updated>2020-08-19T09:55:33.000Z</updated>
        <summary type="html"><![CDATA[<p>⏰ansible自动化运维工具，可以同时对多台主机进行管理，提升工作效率。在此记录工作中用到的东西</p>
]]></summary>
        <content type="html"><![CDATA[<p>⏰ansible自动化运维工具，可以同时对多台主机进行管理，提升工作效率。在此记录工作中用到的东西</p>
<!--more-->
<h3 id="ansible基本小知识">ansible基本小知识</h3>
<h4 id="组件inventory">组件inventory</h4>
<p>ansible的hosts文件是存放被管理主机的，被管理主机比较少的情况下，直接在hosts中定义即可，但是以后很定会管理多台主机，而ansible可管理的主机集合就叫做inventory。在ansible中，描述你主机的默认方法是将它们列在一个文本文件中,这个文件叫inventory文件。</p>
<p>此时可以把不同分类的机器放在不同的inventory中，达到清晰管理的目的；</p>
<p>配置文件：/etc/ansible/ansible.cfg</p>
<p>配置参数：inventory = 目录</p>
<p>相关inventory文件都放在/etc/ansible/inventory目录下</p>
<p>配置实例：</p>
<pre><code>[root@test-1 ~]# grep &quot;^inventory&quot;  /etc/ansible/ansible.cfg
inventory      = /etc/ansible/inventory

[root@test-1 ~]#  ls /etc/ansible/inventory/
ca  cjq  context.xml  fiscosign  gateway_M  guangdong  peer1  peer2  peer3  webasenode  yc

</code></pre>
<p>注：inventory配置的是目录，此目录所有文件都会生效</p>
<h4 id="ventory清单的使用规则定义主机和组">ventory（清单）的使用规则（定义主机和组）</h4>
<pre><code># “ # ”  开头的行表示该行为注释行，即当时行的配置不生效。

# Inventory（清单）可以直接为IP地址

192.168.1.7

#Inventory（清单）同样支持Hostname（主机名）的方式，后跟冒号加数字表示端口号，默认22号端口

ntp.magede.com：22

nfs.magede.com：22

# 中括号内的内容表示一个分组的开始，紧随其后的主机均属于该组成员，空行后的主机亦属于该组，即web2.magedu.com这台主机也属于[webservers]组。

[webservers]

web1.magedu.com

web[10:20].magedu.com  #[10:20] 表示10 ~ 20 之间的所有数字（包括10和20 ），即表示web10.magedu.com、web11.magedu.com.................web20.magedu.com的所有主机。



web2.magedu.com[dbservers]

db-a.magedu.com

db-[b:f].magedu.com    # [b:f] 表示b到f之间的所有数字（包括b和f），即表示db-b.magedu.com、db-c.magedu.com..........db-f.magedu.com的所有主机。

</code></pre>
<h4 id="定义主机变量">定义主机变量</h4>
<p>在平时工作中，通常会遇到非标准化的需求配置，如考虑到安全性问题，业务人员通常将企业内部的web服务80端口修改为其他端口号，而该功能可以直接通过修改Inventory（清单）配置来实现，在定义主机时为其添加主机变量，以便在Playbook中使用针对某一主机的个性化要求。</p>
<p>例如：</p>
<pre><code>[webservers]
 
web1.magedu.com  http_port=808   ansible_host=11.111.111.111 ansible_ssh_port=22 name=shanghai  #自定义http_port的端口号为808、 主机ip为11.111.111.111、ssh端口为22、定义变量name为shanghai
</code></pre>
<h4 id="定义组变量">定义组变量</h4>
<p>Ansible支持定义组变量，主要是针对大量机器的变量定义需求，赋予指定组内所有主机在Playbook中可用的变量，等同于逐一给该组下的所有主机赋予同一变量。<br>
例如：</p>
<pre><code>
[groupservers]
 
web1.magedu.com
 
web2.magedu.com
 
[groupservers:vars]
 
ntp_server=ntp.magedu.com    #定义groupservers组中所有主机ntp_server值为ntp.magedu.com
 
nfs_server=nfs.magedu.com   #定义groupservers组中的所有主机nfs_server值为nfs.magedu.com

</code></pre>
<h4 id="其他inventory清单参数列表">其他Inventory（清单）参数列表</h4>
<p>除了支持如上的功能外，Ansible基于SSH连接Inventory（清单）中指定的远程主机时，还内置了很多其他参数，用于指定其交互方式。</p>
<p>下面列举了部分重要参数：</p>
<pre><code>
ansible_ssh_host：指定连接主机ansible_ssh_port，指定SSH连接端口，默认22
 
ansible_ssh_user：指定SSH连接用户
 
ansible_ssh_pass：指定SSH连接密码
 
ansible_sudo_pass：指定SSH连接时sudo密码
 
ansible_ssh_private_key_file：指定特有私钥文件
</code></pre>
]]></content>
    </entry>
</feed>