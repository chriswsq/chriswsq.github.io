<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://chriswsq.github.io</id>
    <title>chris&apos;wang</title>
    <updated>2020-10-30T02:06:21.026Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://chriswsq.github.io"/>
    <link rel="self" href="https://chriswsq.github.io/atom.xml"/>
    <subtitle>当你觉得无所事事时，那你就是在虚度光阴</subtitle>
    <logo>https://chriswsq.github.io/images/avatar.png</logo>
    <icon>https://chriswsq.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, chris&apos;wang</rights>
    <entry>
        <title type="html"><![CDATA[cita链管理服务部署]]></title>
        <id>https://chriswsq.github.io/post/cita-lian-guan-li-fu-wu-bu-shu/</id>
        <link href="https://chriswsq.github.io/post/cita-lian-guan-li-fu-wu-bu-shu/">
        </link>
        <updated>2020-10-29T08:32:53.000Z</updated>
        <content type="html"><![CDATA[<h2 id="1链管理服务依赖环境">1.链管理服务依赖环境</h2>
<p>•	python &gt;= 3.7.9<br>
•	docker &gt;= 19.0</p>
<ol>
<li>安装3.7.9版本参考：<br>
https://blog.csdn.net/lkgCSDN/article/details/84403329<br>
https://chriswsq.github.io/post/elk-gao-jing-zhi-elastalert-bu-shu-ji-pei-zhi/</li>
</ol>
<p>yum install sqlite-devel<br>
pip install sqlalchemy<br>
pip install sqlalchemy-migrate<br>
ln -s  /usr/local/python3/bin/pip3  /usr/bin/pip3</p>
<h2 id="2链管理服务部署步骤">2.链管理服务部署步骤</h2>
<ul>
<li>解压源码包并进入目录</li>
</ul>
<pre><code class="language-bash">tar xzvf bsn-cita-chain-manager.tar.gz
cd bsn-cita-chain-manager
</code></pre>
<ul>
<li>
<p>安装依赖包<br>
<code>pip3 install -r requirements.txt -i https://pypi.douban.com/simple</code></p>
</li>
<li>
<p>生成pub_key、pri_key</p>
</li>
</ul>
<p>通过cita-cli工具，执行 ./cita-cli key create --algorithm sm2，将 address 字段写入pub_key，将 private 字段写入pri_key</p>
<ul>
<li>修改配置文件</li>
</ul>
<pre><code class="language-bash">vim conf/config.py

# 链节点密钥对的路径及文件名称
PUBKEY_PATH = '/opt/pub_key'
PRIKEY_PATH = '/opt/pri_key'

# cita 镜像名称，rebirth 镜像名称以及 rebirth 依赖 mysql 的镜像名称（下文详细说明）
CITA_IMAGE_VERSION = 'cita/cita-ee:1.3.1-sm2-sm3'
MYSQL_IMAGE_VERSION = 'cita-for-bsn-mysql:5.7'
REBIRTH_IMAGE_VERSION = 'cita-for-bsn-rebirth:0.1'

# 链管理服务产生链配置的目录
CHAIN_CONFIG_PATH = '/opt/all-chain-config'

# 链节点 TLS 证书所在目录
TLS_PATH = '/opt'

# solc 编译器镜像名称
SOLC_IMAGE_VERSION = 'ethereum/solc:0.4.24'
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[docker-compsoe部署kafka集群]]></title>
        <id>https://chriswsq.github.io/post/docker-compsoe-bu-shu-kafka-ji-qun/</id>
        <link href="https://chriswsq.github.io/post/docker-compsoe-bu-shu-kafka-ji-qun/">
        </link>
        <updated>2020-10-16T10:00:49.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>kafka依赖于zookeeper存放元数据, 所以在创建kafka集群之前需要创建zookeeper; 更多关于zookeeper集群创建见: 使用Docker部署zookeeper集群</p>
<h2 id="拉取镜像">拉取镜像</h2>
<pre><code class="language-bash">docker pull zookeeper:3.4
docker pull zookeeper:3.4 docker pull wurstmeister/kafka_2.13-2.6.0
</code></pre>
<blockquote>
<p>其中kafka版本中的2.12为Scala版本</p>
</blockquote>
<h2 id="创建子网段">创建子网段</h2>
<pre><code class="language-bash">docker network create --subnet 172.30.1.0/16 --gateway 172.30.0.1 kafka
</code></pre>
<pre><code class="language-yaml">version: '3'

services: 
    zoo1:
        image: zookeeper:3.4
        restart: always
        hostname: zoo1
        container_name: zoo1
        ports:
            - 2184:2181
        volumes: 
            - /home/zk/workspace/volumes/zkcluster/zoo1/data:/data
            - /home/zk/workspace/volumes/zkcluster/zoo1/datalog:/datalog
        environment: 
            ZOO_MY_ID: 1
            ZOO_SERVERS: server.1=0.0.0.0:2888:3888 server.2=zoo2:2888:3888 server.3=zoo3:2888:3888
        networks:
            kafka:
                ipv4_address: 172.30.0.11

    zoo2:
        image: zookeeper:3.4
        restart: always
        hostname: zoo2
        container_name: zoo2
        ports:
            - 2185:2181
        volumes: 
            - /home/zk/workspace/volumes/zkcluster/zoo2/data:/data
            - /home/zk/workspace/volumes/zkcluster/zoo2/datalog:/datalog
        environment: 
            ZOO_MY_ID: 2
            ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=0.0.0.0:2888:3888 server.3=zoo3:2888:3888
        networks:
            kafka:
                ipv4_address: 172.30.0.12

    zoo3:
        image: zookeeper:3.4
        restart: always
        hostname: zoo3
        container_name: zoo3
        ports:
            - 2186:2181
        volumes: 
            - /home/zk/workspace/volumes/zkcluster/zoo3/data:/data
            - /home/zk/workspace/volumes/zkcluster/zoo3/datalog:/datalog
        environment: 
            ZOO_MY_ID: 3
            ZOO_SERVERS: server.1=zoo1:2888:3888 server.2=zoo2:2888:3888 server.3=0.0.0.0:2888:3888
        networks:
            kafka:
                ipv4_address: 172.30.0.13

    kafka1:
        image: wurstmeister/kafka
        restart: always
        hostname: kafka1
        container_name: kafka1
        privileged: true
        ports:
            - 9092:9092
        environment:
              KAFKA_ADVERTISED_HOST_NAME: kafka1
              KAFKA_LISTENERS: PLAINTEXT://kafka1:9092
              KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:9092
              KAFKA_ADVERTISED_PORT: 9092
              KAFKA_ZOOKEEPER_CONNECT: zoo1:2181,zoo2:2181,zoo3:2181
        volumes:
            - /home/zk/workspace/volumes/kafkaCluster/kafka1/logs:/kafka
        networks:
            kafka:
                ipv4_address: 172.30.1.11
        extra_hosts: 
            - zoo1:172.30.0.11
            - zoo2:172.30.0.12
            - zoo3:172.30.0.13
        depends_on: 
            - zoo1
            - zoo2
            - zoo3
        external_links: 
            - zoo1
            - zoo2
            - zoo3

    kafka2:
        image: wurstmeister/kafka
        restart: always
        hostname: kafka2
        container_name: kafka2
        privileged: true
        ports:
            - 9093:9093
        environment:
              KAFKA_ADVERTISED_HOST_NAME: kafka2
              KAFKA_LISTENERS: PLAINTEXT://kafka2:9093
              KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka2:9093
              KAFKA_ADVERTISED_PORT: 9093
              KAFKA_ZOOKEEPER_CONNECT: zoo1:2181,zoo2:2181,zoo3:2181
        volumes:
            - /home/zk/workspace/volumes/kafkaCluster/kafka2/logs:/kafka
        networks:
            kafka:
                ipv4_address: 172.30.1.12
        extra_hosts: 
            - zoo1:172.30.0.11
            - zoo2:172.30.0.12
            - zoo3:172.30.0.13                
        depends_on: 
            - zoo1
            - zoo2
            - zoo3                
        external_links: 
            - zoo1
            - zoo2
            - zoo3           

    kafka3:
        image: wurstmeister/kafka
        restart: always
        hostname: kafka3
        container_name: kafka3
        privileged: true
        ports:
            - 9094:9094
        environment:
              KAFKA_ADVERTISED_HOST_NAME: kafka3
              KAFKA_LISTENERS: PLAINTEXT://kafka3:9094
              KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka3:9094
              KAFKA_ADVERTISED_PORT: 9094
              KAFKA_ZOOKEEPER_CONNECT: zoo1:2181,zoo2:2181,zoo3:2181
        volumes:
            - /home/zk/workspace/volumes/kafkaCluster/kafka3/logs:/kafka
        networks:
            kafka:
                ipv4_address: 172.30.1.13
        extra_hosts: 
            - zoo1:172.30.0.11
            - zoo2:172.30.0.12
            - zoo3:172.30.0.13                
        depends_on: 
            - zoo1
            - zoo2
            - zoo3                
        external_links: 
            - zoo1
            - zoo2
            - zoo3

networks: 
  kafka:
     ipam:
       config:
         - subnet: 172.30.0.0/16
</code></pre>
<blockquote>
<p>在kafka服务中声明了depends_on, 所以在所有zookeeper启动之后才会真正启动kafka容器</p>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Loki配置邮件告警]]></title>
        <id>https://chriswsq.github.io/post/loki-pei-zhi-you-jian-gao-jing/</id>
        <link href="https://chriswsq.github.io/post/loki-pei-zhi-you-jian-gao-jing/">
        </link>
        <updated>2020-10-12T09:43:07.000Z</updated>
        <summary type="html"><![CDATA[<p>监控的目的就是及时发现问题去解决处理，那么在告警就是必不可少的。</p>
]]></summary>
        <content type="html"><![CDATA[<p>监控的目的就是及时发现问题去解决处理，那么在告警就是必不可少的。</p>
<!-- more -->
<p>本次告警配置是loki结合grafana来进行邮件告警</p>
<h2 id="grafana配置告警">grafana配置告警</h2>
<p>1.通过标签定位到需要查看的服务,并通过关键字过滤出想要查看的日志内容<br>
<img src="https://chriswsq.github.io/post-images/1602669342426.png" alt="" loading="lazy"></p>
<p>为尽快得到告警邮件这里我们设置带有info字符的日志</p>
<p>2.看下过去一段时间内，日志中带有info字段的趋势<br>
<img src="https://chriswsq.github.io/post-images/1602496164171.png" alt="" loading="lazy"></p>
<p>这里可以看到，日志在不停的打印带有info字段的日志</p>
<p>在数据源中添加一个Prometheus类型，并在地址栏中填写上http://loki:3100/loki ,这样我们就能通过像查询prometheus一样查询日志的走势<br>
<img src="https://chriswsq.github.io/post-images/1602496373242.png" alt="" loading="lazy"></p>
<p>这时候再创建一个pannel来查询info日志的趋势就可以得到如下结果：</p>
<figure data-type="image" tabindex="1"><img src="https://chriswsq.github.io/post-images/1602496436562.png" alt="" loading="lazy"></figure>
<p>接下来的工作，就是在Grafana上添加一个Alert小铃铛，让它每分钟去Loki里面查询有没有出现info字段的日志出现，如果计算出来的结果大于0，就让Grafana通过邮件告警出来。</p>
<figure data-type="image" tabindex="2"><img src="https://chriswsq.github.io/post-images/1602496637970.png" alt="" loading="lazy"></figure>
<p>配置完成后不急保存，我们先Test rule看看rule是否能生效。</p>
<p><img src="https://chriswsq.github.io/post-images/1602496687577.png" alt="" loading="lazy"><br>
看到state:&quot;alerting&quot; 说明现在已经开始报警了</p>
<h2 id="grafana配置邮件">grafana配置邮件</h2>
<pre><code class="language-ini">[smtp]
enabled = true
host = smtp.163.com:25
user = 你的邮箱
# If the password contains # or ; you have to wrap it with triple quotes. Ex &quot;&quot;&quot;#password;&quot;&quot;&quot;
password = 你的密码
;cert_file =
;key_file =
;skip_verify = false
from_address = 你的邮箱
from_name = Grafana
</code></pre>
<p>修改后，保存退出，重启grafana服务</p>
<ul>
<li>登入grafana页面</li>
</ul>
<figure data-type="image" tabindex="3"><img src="https://chriswsq.github.io/post-images/1602496941499.png" alt="" loading="lazy"></figure>
<ul>
<li>创建邮件发送规则</li>
</ul>
<figure data-type="image" tabindex="4"><img src="https://chriswsq.github.io/post-images/1602497009120.png" alt="" loading="lazy"></figure>
<p>可以为不同的业务组创建不同的通知通道，例如，运维通道为Ops，研发为Dev。然后可以为一个Metrics指定多个通知通道。</p>
<p>Name为通道名称</p>
<p>Type为通道类型，此处选Email，也可选择钉钉告警或者Alert Manager等告警类型。</p>
<p>Default(send on all alerts)开启后，表示在所有Graph Panel中配置告警规则时默认都会选用此通道。该选项默认为关闭。</p>
<p>Include image开启后，表示是发送告警图片。</p>
<p>Disable Resove Message开启后，表示发送恢复邮件，默认关闭。</p>
<p>Send reminders开启后，还需设置下方的发送间隔，表示发送告警邮件的间隔，默认关闭表示若某个告警发生后即使持续很长时间也仅发送一次邮件。</p>
<p>多个邮箱地址间用分号&quot;;&quot;隔开。例：xxx@163.com;xxx@qq.com</p>
<p>创建成功后，点击Send Test 按钮，测试邮件是否能发送成功。（如果发送不成功，可在grafana.log中查看日志信息）</p>
<ul>
<li>在告警中添加配置的告警邮件联系人组</li>
</ul>
<figure data-type="image" tabindex="5"><img src="https://chriswsq.github.io/post-images/1602497296964.png" alt="" loading="lazy"></figure>
<ul>
<li>收到告警邮件</li>
</ul>
<figure data-type="image" tabindex="6"><img src="https://chriswsq.github.io/post-images/1602497366975.png" alt="" loading="lazy"></figure>
<p>也可以发送图片 ,根据提示安装插件即可<br>
<img src="https://chriswsq.github.io/post-images/1602497453813.png" alt="" loading="lazy"></p>
<p>安装图片插件 官方建议单独新建一个容器去取加载插件而非直接在grafana容器中安装插件</p>
<pre><code class="language-yaml">version: '2'

services:
  grafana:
    image: grafana/grafana:7.0.0-beta1
    ports:
      - 3000
    environment:
      GF_RENDERING_SERVER_URL: http://renderer:8081/render
      GF_RENDERING_CALLBACK_URL: http://grafana:3000/
      GF_LOG_FILTERS: rendering:debug
  renderer:
    image: grafana/grafana-image-renderer:2.0.0-beta1
    ports:
      - 8081
    environment:
      ENABLE_METRICS: 'true'
</code></pre>
<p>结果为<br>
<img src="https://chriswsq.github.io/post-images/1602668961665.png" alt="" loading="lazy"></p>
<p>参考文档：<br>
https://grafana.com/blog/2020/05/07/grafana-7.0-preview-new-image-renderer-plugin-to-replace-phantomjs/<br>
https://www.jianshu.com/p/0982a8ee204c</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[轻量级日志系统Loki原理简介和使用]]></title>
        <id>https://chriswsq.github.io/post/qing-liang-ji-ri-zhi-xi-tong-loki-yuan-li-jian-jie-he-shi-yong/</id>
        <link href="https://chriswsq.github.io/post/qing-liang-ji-ri-zhi-xi-tong-loki-yuan-li-jian-jie-he-shi-yong/">
        </link>
        <updated>2020-10-12T05:40:24.000Z</updated>
        <content type="html"><![CDATA[<pre><code> 目前公司未采用传统的zabbix对服务器监控，而采用了prometheus。以前监控日志的时候是采用
</code></pre>
<p>elastalert+logstash+kibana+filebeat+zookeeper+kafka 来做日志方面的监控告警。后来了解到有loki这样一个日志系统，上网搜了一番资料做了一些对比</p>
<h2 id="日志监控系统elkloki">日志监控系统elk，loki</h2>
<h3 id="elk">ELK</h3>
<p>优势：<br>
1、功能丰富，允许复杂的操作（增减字段，调整字段书序，数据json化）<br>
劣势：<br>
1、主流的ELK（全文检索）或者EFK比较重<br>
2、ES复杂的搜索功能很多都用不上 规模复杂，资源占用高，操作苦难<br>
大多数查询只关注一定时间范围和一些简单的参数（如host、service等）<br>
3、Kibana和Grafana之间切换，影响用户体验<br>
4、倒排索引的切分和共享的成本较高</p>
<h3 id="loki">Loki</h3>
<p>1、最小化度量和日志的切换成本<br>
有助于减少异常事件的响应时间和提高用户的体验<br>
2、在查询语言的易操作性和复杂性之间可以达到一个权衡<br>
3、更具成本效益</p>
<h2 id="项目主页">项目主页</h2>
<p>https://github.com/grafana/loki</p>
<h2 id="loki组件介绍">loki组件介绍</h2>
<h3 id="promtail">Promtail</h3>
<p>用来将容器日志发送到 Loki 或者 Grafana 服务上的日志收集工具<br>
该工具主要包括发现采集目标以及给日志流添加上 Label 标签 然后发送给 Loki<br>
Promtail 的服务发现是基于 Prometheus 的服务发现机制实现的</p>
<h3 id="loki-2">Loki</h3>
<p>受 Prometheus 启发的可以水平扩展、高可用以及支持多租户的日志聚合系统<br>
使用了和 Prometheus 相同的服务发现机制，将标签添加到日志流中而不是构建全文索引<br>
从 Promtail 接收到的日志和应用的 metrics 指标就具有相同的标签集<br>
不仅提供了更好的日志和指标之间的上下文切换，还避免了对日志进行全文索引</p>
<h3 id="grafana">Grafana</h3>
<p>一个用于监控和可视化观测的开源平台<br>
支持非常丰富的数据源<br>
在 Loki 技术栈中它专门用来展示来自 Prometheus 和 Loki 等数据源的时间序列数据<br>
可进行查询、可视化、报警等操作<br>
可以用于创建、探索和共享数据 Dashboard<br>
鼓励数据驱动</p>
<h2 id="安装loki">安装loki</h2>
<p>这里我使用docker 安装，下面是compose文件</p>
<pre><code class="language-yaml">version: &quot;3&quot;
services:
  loki:
    image: &quot;grafana/loki:1.5.0&quot;
    container_name: &quot;loki&quot;
    restart: &quot;always&quot;
    volumes:
      - &quot;/etc/localtime:/etc/localtime&quot;
      - &quot;/usr/local/src/loki/:/etc/loki&quot;
      - &quot;./loki:/loki&quot;
    ports:
      - &quot;3100:3100&quot;
    command: &quot;-config.file=/etc/loki/local-config.yaml&quot;
</code></pre>
<p>记得要修改loki文件夹的所有者为 10001 不然会提示权限不足</p>
<p><code>chown -Rf 10001:10001 loki</code></p>
<p>之后使用docker-compose启动</p>
<p><code>docker-compose up -d</code></p>
<p>就这样loki的服务端就ok了，下面说下几个坑，第一个是</p>
<pre><code class="language-bash">^[level=warn ts=2020-07-06T06:54:12.754273854Z caller=client.go:242 component=client host=192.179.11.1:3100 msg=&quot;error sending batch, will retry&quot; status=429 error=&quot;server returned HTTP status 429 Too Many Requests (429): Ingestion rate limit exceeded (limit: 6291456 bytes/sec) while attempting to ingest '221' lines totaling '101946' bytes, reduce log volume or contact your Loki administrator to see if the limit can be increased&quot;
</code></pre>
<p>当你搭建完成promtail，并且启动发送日志到loki的时候很有可能会碰到这个错误，因为你要收集的日志太多了，超过了loki的限制，所以会报429，如果你要增加限制可以修改loki的配置文件,在limits_config中添加</p>
<p><code>ingestion_rate_mb: 15</code></p>
<p>如果你是老版本的loki，那么是添加</p>
<p><code>ingestion_rate: 25000</code></p>
<p>详细可以看下面</p>
<p>https://github.com/grafana/loki/pull/1278/files/f468d5d258a42316036290fad1b795c40bec22e4#diff-935fd110763ed3367d3ea740a3d3c072</p>
<p>还有一个坑是<br>
<code>level=error ts=2020-07-06T03:58:02.217480067Z caller=client.go:247 component=client host=192.179.11.1:3100 msg=&quot;final error sending batch&quot; status=400 error=&quot;server returned HTTP status 400 Bad Request (400): entry for stream '{app=\&quot;app_error\&quot;, filename=\&quot;/error.log\&quot;, host=\&quot;192.179.11.12\&quot;}' has timestamp too new: 2020-07-06 03:58:01.175699907 +0000 UTC&quot;</code><br>
这个是两台机器的时间相差太大了，我promtail这台机器的时间没有和ntp服务器同步时间，所以就报了这个错误，只要把时间都同步了就好了</p>
<p><strong>我的最终配置文件</strong></p>
<pre><code class="language-yaml">auth_enabled: false

server:
  http_listen_port: 3100

ingester:
  lifecycler:
    address: 127.0.0.1
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1
    final_sleep: 0s
  chunk_idle_period: 5m
  chunk_retain_period: 30s

schema_config:
  configs:
    - from: 2020-10-12
      store: boltdb
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 168h

storage_config:
  boltdb:
    directory: /loki/index

  filesystem:
    directory: /loki/chunks

limits_config:
  enforce_metric_name: false
  reject_old_samples: true
  reject_old_samples_max_age: 168h
  ingestion_rate_mb: 15

chunk_store_config:
  max_look_back_period: 0s

table_manager:
  retention_deletes_enabled: true
  retention_period: 336h
</code></pre>
<h2 id="安装promtail">安装promtail</h2>
<p>这里我使用docker 安装，下面是compose文件</p>
<pre><code class="language-yaml">version: &quot;3&quot;
services:
  promtail:
    image: grafana/promtail:1.5.0
    container_name: promtail
    restart: always
    volumes:
      - $PWD:/etc/promtail
      - /bsn/xuperchain/output/logs/xchain.log:/bsn/xuperchain/output/logs/xchain.log
    command:
      -config.file=/etc/promtail/promtail-docker-config.yaml
</code></pre>
<p><strong>配置文件如下</strong></p>
<pre><code class="language-yaml">server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: ./positions.yaml

clients:
  - url: http://192.168.40.6:3100/loki/api/v1/push

scrape_configs:
 - job_name: xuperchain
   static_configs:
   - labels:
      app: local_xchain
      host: 192.168.40.7
      env: prod
      __path__: /bsn/xuperchain/output/logs/xchain.log
</code></pre>
<p>之后使用docker-compose启动</p>
<p><code>docker-compose up -d</code></p>
<h2 id="安装grafana">安装grafana</h2>
<pre><code class="language-yaml">version: &quot;3&quot;
services:
  grafana:
    image: grafana/grafana:7.1.5
    container_name: grafana
    restart: always
    volumes:
     - /bsn/prometheus/grafana:/var/lib/grafana
     - /bsn/prometheus/grafana/grafana.ini:/etc/grafana/grafana.ini
     - /etc/localtime:/etc/localtime
    ports:
     - 3000:3000
</code></pre>
<p><strong>配置文件</strong><br>
下载一个grafana的tar包用即可，也可下载镜像后拷出来grafana.ini文件即可</p>
<p><strong>访问grafana界面</strong></p>
<p>http://192.168.40.6:3000/</p>
<figure data-type="image" tabindex="1"><img src="https://chriswsq.github.io/post-images/1602485759409.png" alt="" loading="lazy"></figure>
<p>默认的登陆账号 admin/admin</p>
<p>然后添加loki数据源<br>
<img src="https://chriswsq.github.io/post-images/1602485889822.png" alt="" loading="lazy"></p>
<p>url添加  http://192.168.40.6:3100<br>
<img src="https://chriswsq.github.io/post-images/1602485944668.png" alt="" loading="lazy"></p>
<p>点击Exporter 选择loki选择相应的label</p>
<figure data-type="image" tabindex="2"><img src="https://chriswsq.github.io/post-images/1602486257770.png" alt="" loading="lazy"></figure>
<h2 id="loki-日志查询语言">Loki 日志查询语言</h2>
<p>基本的LogQL查询由两部分组成：log stream selector、filter expression</p>
<h3 id="log-stream-selector">Log stream selector</h3>
<p>它由一个或多个键值对组成，每个键是一个日志标签，值的话是标签的值，例如<br>
<code>{app=&quot;local_xchain&quot;,host=&quot;192.168.40.7&quot;}</code></p>
<p>在这个例子中，记录具有的标签流app，其值是local_xhcain和的一个标签host，它的值192.168.40.7将被包括在查询结果。注意，这将匹配其标签至少 包含192.168.40.7其名称标签的任何日志流；如果有多个包含该标签的流，则所有匹配流的日志将显示在结果中。</p>
<p>支持以下标签匹配运算符：</p>
<p>=：完全相等。<br>
!=：不相等。<br>
=~：正则表达式匹配。<br>
!~：正则表达式不匹配。</p>
<p>适用于Prometheus标签选择器的相同规则也适用 于Loki日志流选择器。</p>
<p>Filter expression<br>
写入日志流选择器后，可以使用搜索表达式进一步过滤生成的日志集。搜索表达式可以只是文本或正则表达式：</p>
<ul>
<li>{job=“mysql”} |= “error”</li>
<li>{name=“kafka”} |~ “tsdb-ops.*io:2003”</li>
<li>{instance=~“kafka-[23]”,name=“kafka”} != kafka.server:type=ReplicaManager</li>
</ul>
<p>运算符说明</p>
<ul>
<li>|=：日志行包含字符串。</li>
<li>!=：日志行不包含字符串。</li>
<li>|~：日志行匹配正则表达式。</li>
<li>!~：日志行与正则表达式不匹配。</li>
</ul>
<h3 id="指标查询">指标查询</h3>
<h4 id="范围向量">范围向量</h4>
<p>LogQL 与Prometheus 具有相同的范围向量概念，不同之处在于所选的样本范围包括每个日志条目的值1。可以在所选范围内应用聚合，以将其转换为实例向量。</p>
<p><strong>注：对于此种查询，需要添加数据源，选择promethes，但是地址为loki的地址，并在最后添加/lok即可</strong></p>
<p>当前支持的操作功能为：</p>
<ul>
<li>rate：计算每秒的条目数</li>
<li>count_over_time：计算给定范围内每个日志流的条目。</li>
</ul>
<p>//对fluent-bit作业在最近五分钟内的所有日志行进行计数。</p>
<p><code>count_over_time({job=&quot;fluent-bit&quot;}[5m])</code></p>
<p>获取fluent-bit作业在过去十秒内所有非超时错误的每秒速率。</p>
<p><code>rate({job=&quot;fluent-bit&quot;} |= &quot;error&quot; != &quot;timeout&quot; [10s]</code></p>
<h4 id="集合运算符">集合运算符</h4>
<p>与PromQL一样，LogQL支持内置聚合运算符的一个子集，可用于聚合单个向量的元素，从而产生具有更少元素但具有集合值的新向量：</p>
<ul>
<li>sum：计算标签上的总和</li>
<li>min：选择最少的标签</li>
<li>max：选择标签上方的最大值</li>
<li>avg：计算标签上的平均值</li>
<li>stddev：计算标签上的总体标准差</li>
<li>stdvar：计算标签上的总体标准方差</li>
<li>count：计算向量中元素的数量</li>
<li>bottomk：通过样本值选择最小的k个元素</li>
<li>topk：通过样本值选择最大的k个元素</li>
</ul>
<p>可以通过包含a without或 by子句，使用聚合运算符聚合所有标签值或一组不同的标签值：</p>
<p><code>&lt;aggr-op&gt;([parameter,] &lt;vector expression&gt;) [without|by (&lt;label list&gt;)]</code></p>
<p>举例：</p>
<p>统计最高日志吞吐量按container排序前十的应用程序<br>
<code>topk(10,sum(rate({job=&quot;fluent-bit&quot;}[5m])) by(container))</code></p>
<p>获取最近五分钟内的日志计数，按级别分组<br>
<code>sum(count_over_time({job=&quot;fluent-bit&quot;}[5m])) by (level)</code></p>
<p>更多内容请参考：https://github.com/grafana/loki/blob/master/docs/logql.md</p>
<p>参考文档：<br>
https://blog.csdn.net/weixin_44267608/article/details/105264432<br>
https://segmentfault.com/a/1190000023379491<br>
https://www.bboy.app/2020/07/08/%E4%BD%BF%E7%94%A8loki%E8%BF%9B%E8%A1%8C%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86/</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[xuperchian  建链未成功手动修复]]></title>
        <id>https://chriswsq.github.io/post/xuperchian change Common node/</id>
        <link href="https://chriswsq.github.io/post/xuperchian change Common node/">
        </link>
        <updated>2020-09-18T03:04:41.000Z</updated>
        <content type="html"><![CDATA[<!-- more -->
<p>从idc服务器日志中找到 链名的日志  会有创建该链的json文件</p>
<p>#手动修复</p>
<pre><code>#step 1. 创建群组
./xchain-cli wasm invoke group_chain --method addChain -a '{&quot;bcname&quot;:&quot;CounterChain1&quot;}'

#step 2. 添加节点
./xchain-cli wasm invoke group_chain --method addNode -a '{&quot;bcname&quot;:&quot;CounterChain1&quot;, &quot;ip&quot;:&quot;/ip4/127.0.0.1/tcp/47101/p2p/QmVxeNubpg1ZQjQT8W5yZC9fD7ZB1ViArwvyGUB53sqf8e&quot;, 

#step 3. 创建平行链
./xchain-cli transfer --to CounterChain1 --amount 100 --desc createCounterChain1.json


</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[xuperchian 增减共识节点、添加群组操作步骤]]></title>
        <id>https://chriswsq.github.io/post/xuperchian-change-Common-node/</id>
        <link href="https://chriswsq.github.io/post/xuperchian-change-Common-node/">
        </link>
        <updated>2020-09-15T11:19:03.000Z</updated>
        <content type="html"><![CDATA[<h1 id="加入群组">加入群组</h1>
<h2 id="查看当前群组节点">查看当前群组节点</h2>
<pre><code class="language-bash">./xchain-cli wasm query group_chain --method listNode -a '{&quot;bcname&quot;:&quot;app000120200824165811835422&quot;}'
</code></pre>
<h2 id="节点加入群组">节点加入群组</h2>
<pre><code class="language-bash">./xchain-cli wasm invoke group_chain --method addNode -a '{&quot;bcname&quot;:&quot;app000120200824165811835422&quot;, &quot;ip&quot;:&quot;/ip4/192.168.40.6/tcp/40001/p2p/QmY7UxwcLpD8QK8p6gAk99xuz3Q16jcaCoU8iMokVWYa83&quot;, &quot;address&quot;:&quot;2B1rDQhq7W4TStSHoD88N1SUYXrCDV821v&quot;}'
</code></pre>
<blockquote>
<p>将bcname 、ip 、 address改为实际的参数即可</p>
</blockquote>
<h1 id="更改共识节点">更改共识节点</h1>
<h2 id="查询金额">查询金额</h2>
<pre><code class="language-bash">./xchain-cli account balance zALW2YRp55LFuro1uAjgfoGTqCjgz3nHc --name app000120200824165811835422
</code></pre>
<h2 id="查看块高">查看块高</h2>
<pre><code class="language-bash">./xchain-cli status -H 127.0.0.1:37101 | jq '.blockchains[] | {&quot;name&quot;:.name,&quot;height&quot;:.ledger.trunkHeight}' | grep -A 1 app000120200824165811835422
</code></pre>
<h2 id="提案">提案</h2>
<pre><code class="language-bash">./xchain-cli transfer --to scocy5ZTaFykhRGGYxN9KSEkpxCD1cd72 --desc proposal_app000120200824165811835422.json  --amount 1  --name app000120200824165811835422

ae074dc967e0a03b81346a8e4796b01fe15b51472d3ba34f191d6b08e24a1918
</code></pre>
<h2 id="投票">投票</h2>
<pre><code class="language-bash">./xchain-cli vote ae074dc967e0a03b81346a8e4796b01fe15b51472d3ba34f191d6b08e24a1918 --frozen 138160 --amount 52000000071795360000   --name app000120200824165811835422

74217bf729af8fc1b6acfbc5f953fbda029fb9078360ec301dfc113e690ac016
</code></pre>
<h2 id="查看交易内容">查看交易内容</h2>
<pre><code class="language-bash">./xchain-cli tx query  74217bf729af8fc1b6acfbc5f953fbda029fb9078360ec301dfc113e690ac016
</code></pre>
<h2 id="查看共识节点">查看共识节点</h2>
<pre><code class="language-bash">./xchain-cli tdpos status --name   app000120200824165811835422
</code></pre>
<h1 id="提案内容">提案内容</h1>
<pre><code class="language-json">{
    &quot;module&quot;: &quot;proposal&quot;,
    &quot;method&quot;: &quot;Propose&quot;,
    &quot;args&quot; : {
        &quot;min_vote_percent&quot;: 51,
        &quot;stop_vote_height&quot;: 688970
    },
    &quot;trigger&quot;: {
        &quot;height&quot;: 688980,
        &quot;module&quot;: &quot;consensus&quot;,
        &quot;method&quot;: &quot;update_consensus&quot;,
        &quot;args&quot; : {
            &quot;name&quot;: &quot;tdpos&quot;,
            &quot;config&quot;: {
                &quot;version&quot;:&quot;20&quot;,
                &quot;proposer_num&quot;:&quot;2&quot;,
                &quot;period&quot;:&quot;5000&quot;,
                &quot;alternate_interval&quot;:&quot;5000&quot;,
                &quot;term_interval&quot;:&quot;10000&quot;,
                &quot;block_num&quot;:&quot;720&quot;,
                &quot;vote_unit_price&quot;:&quot;1&quot;,
                &quot;init_proposer&quot;: {
                    &quot;1&quot;:[&quot;scocy5ZTaFykhRGGYxN9KSEkpxCD1cd72&quot;, &quot;qaXhH7gJcdfpapmWkbHdLNqUFq3Vst6Am&quot;]
                }
            }
        }
    }
}
</code></pre>
<blockquote>
<p>注意： 以上将 address、p2p、proposer_num等改为实际数据即可<br>
命令添加群组，一次性添加两个以上群组 不生效,须一个一个添加<br>
用命令行创建平行链，如果命令写错不报命令的错误，而会报其他的问题    例如 地址不在白名单<br>
后续加入群组的节点，需要重启才会同步块数据<br>
在进行提案的时候经常会导致xuper链或其他调整的链快高不一致的情况，这时候重启解决问题<br>
如果同一个链既要增加节点又要删减节点，那么是需要分开提案来做的</p>
</blockquote>
<pre><code class="language-bash">new
pJsfQecriScf4ZA6MHhjEpMMAiBbct5Sv
24rqLhCMozBJrmsXhtrR68wLAr72zDfuEL
qJV7qfGdf2GAZUcrx6v71ahQ69nYYWGpa


huainan2
app0001202007302056480751681 

2A8cTP6dFjKoZPXyCx7T5APbQKCUzbgBsx
pecX9eVDd368J3GBfdPNtVUgnTu4nvD7b
28HsYtaS1p7DqZ3QqXzCSsEk47dX4seU6o

app0001202007310000590617583

2A8cTP6dFjKoZPXyCx7T5APbQKCUzbgBsx
27xoBigJ6HRqtqQtohvaoMyBt55uECtNbV
sMbiWCdbiAHn84p77zjepAzGn3F7ztzAD


app0001202008011331415497000
2A8cTP6dFjKoZPXyCx7T5APbQKCUzbgBsx
pecX9eVDd368J3GBfdPNtVUgnTu4nvD7b
qaXhH7gJcdfpapmWkbHdLNqUFq3Vst6Am




</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[docker容器保持后台运行的两种方式]]></title>
        <id>https://chriswsq.github.io/post/docker-rong-qi-bao-chi-hou-tai-yun-xing-de-liang-chong-fang-shi/</id>
        <link href="https://chriswsq.github.io/post/docker-rong-qi-bao-chi-hou-tai-yun-xing-de-liang-chong-fang-shi/">
        </link>
        <updated>2020-09-14T08:39:50.000Z</updated>
        <content type="html"><![CDATA[<!-- more -->
<p>有的服务支持服务后台运行，那么在用docker启动服务时可以使用前台运行的命令，除此之外一些服务器是不支持前台运行，那么就要靠其他方式使用进程挂在前台。</p>
<ul>
<li>方法1 ，run一个容器：</li>
</ul>
<pre><code class="language-bash">[root@localhost ~]# docker run -dit --hostname centos --name centos --restart always a8493f5f50ff /bin/bash
 
-dit 是后台运行、交互模式、分配终端，容器启动后不会退出
 如果没有it参数，run 一个容器以后，docker ps -a 容器状态就exited了
--restart always 容器可以随docker服务启动而启动
</code></pre>
<ul>
<li>方法2， run 一个容器，并一直发送ping包</li>
</ul>
<pre><code class="language-bash">[root@localhost ~]# docker run -d --name test --hostname test a8493f5f50ff ping 127.0.0.1
</code></pre>
<p>这样容器就一直在发送ping包，不会退出。<br>
意思就是然容器一直运行一个程序，不退出，容器就不会停止</p>
<p>本文链接地址 https://www.rootop.org/pages/3736.html</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[记一次服务器内存过高问题]]></title>
        <id>https://chriswsq.github.io/post/ji-yi-ci-fu-wu-qi-nei-cun-guo-gao-wen-ti/</id>
        <link href="https://chriswsq.github.io/post/ji-yi-ci-fu-wu-qi-nei-cun-guo-gao-wen-ti/">
        </link>
        <updated>2020-09-10T09:40:31.000Z</updated>
        <content type="html"><![CDATA[<!-- more -->
<p>监控系统发邮件告警，一台服务器内存过高，已达 90% 以上，之前这台机器内存就高到85%以上，上去看了下服务正常 top 指令看了下没什么太高的进程占用，就重启了下上面的服务（前提是上面的服务重启是没什么影响的）。</p>
<p>今天有来告警了，内存占用率已经到90%，照常上去看了下<br>
使用 free -m 查看内存，一共是16G  已经用了 13G<br>
top  查看进程占用资源情况，发现最高的应用占用也不过百分之五左右</p>
<p>怀疑是被攻击了，看了下进程  ps  aux<br>
发现进程很多   ps aux |wc  -l    进程都6000多了，大部分的进程都是 sshd: root@notty  的进程，网上查了下这是被 ssh 暴力破解了，很奇怪，端口号改了不是22 了 ，攻击者怎么知道的呢。。。。</p>
<p>先解决问题吧</p>
<pre><code class="language-bash"> for i in  `ps  aux | grep root@notty | more | awk  '{print $2}'` ;do kill $i ;done
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[docker部署Prometheus监控服务器及容器并发送告警]]></title>
        <id>https://chriswsq.github.io/post/docker-bu-shu-prometheus-jian-kong-fu-wu-qi-ji-rong-qi-bing-fa-song-gao-jing/</id>
        <link href="https://chriswsq.github.io/post/docker-bu-shu-prometheus-jian-kong-fu-wu-qi-ji-rong-qi-bing-fa-song-gao-jing/">
        </link>
        <updated>2020-09-03T02:44:39.000Z</updated>
        <summary type="html"><![CDATA[<p>记录一下利用prometheus监控服务器信息和容器服务并发送邮件告警</p>
]]></summary>
        <content type="html"><![CDATA[<p>记录一下利用prometheus监控服务器信息和容器服务并发送邮件告警</p>
<!-- more -->
<h2 id="基本原理">基本原理</h2>
<p>Prometheus的基本原理是通过HTTP协议周期性抓取被监控组件的状态，任意组件只要提供对应的HTTP接口就可以接入监控。不需要任何SDK或者其他的集成过程。这样做非常适合做虚拟化环境监控系统，比如VM、Docker、Kubernetes等。输出被监控组件信息的HTTP接口被叫做exporter 。目前互联网公司常用的组件大部分都有exporter可以直接使用，比如Varnish、Haproxy、Nginx、MySQL、Linux系统信息(包括磁盘、内存、CPU、网络等等)。</p>
<h2 id="相关组件">相关组件</h2>
<ul>
<li>Prometheus: Prometheus Daemon负责定时去目标上抓取metrics(指标)数据，每个抓取目标需要暴露一个http服务的接口给它定时抓取。</li>
<li>Grafana: 接入prometheus数据，图形化展示监控信息</li>
<li>Node-exporter: 负责收集 host 硬件和操作系统数据。它将以容器方式运行在所有 host 上。</li>
<li>Cadvisor: 负责收集容器数据。它将以容器方式运行在所有 host 上。</li>
<li>Alertmanager: 警告管理器，用来进行报警。</li>
</ul>
<table>
<thead>
<tr>
<th>主机名</th>
<th>ip</th>
<th>服务</th>
</tr>
</thead>
<tbody>
<tr>
<td>test-1</td>
<td>192.168.40.6</td>
<td>cadvisor、node-exporter、grafana、prometheus、alertmanager.yml</td>
</tr>
<tr>
<td>test-2</td>
<td>192.168.40.7</td>
<td>node-exporter、cadvisor</td>
</tr>
<tr>
<td>test-3</td>
<td>192.168.40.8</td>
<td>node-exporter、cadvisor</td>
</tr>
</tbody>
</table>
<h2 id="安装docker-docker-compose">安装docker、docker-compose</h2>
<h3 id="安装docker">安装docker</h3>
<pre><code class="language-bash"># 安装依赖包
yum install -y yum-utils device-mapper-persistent-data lvm2
# 添加Docker软件包源
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
# 安装Docker CE
yum install docker-ce -y
# 启动
systemctl start docker
# 开机启动
systemctl enable docker
# 查看Docker信息
docker info
</code></pre>
<h3 id="安装docker-compose">安装docker-compose</h3>
<pre><code class="language-bash">curl -L https://github.com/docker/compose/releases/download/1.23.2/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose
chmod +x /usr/local/bin/docker-compose
</code></pre>
<h2 id="添加配置文件">添加配置文件</h2>
<pre><code class="language-bash">mkdir -p /usr/local/src/config
cd /usr/local/src/config
</code></pre>
<h3 id="添加prometheusyml配置文件">添加prometheus.yml配置文件</h3>
<p>vim prometheus.yml</p>
<pre><code class="language-bash"># my global config
global:
  scrape_interval: 15s
  evaluation_interval: 15s

alerting:
  alertmanagers:
  - static_configs:
    - targets:
       - 192.168.40.6:9093

rule_files:
  - &quot;/etc/prometheus/config/rule/*rule.yml&quot;

scrape_configs:
  - job_name: 'prometheus'
    scrape_interval: 5s
    static_configs:
        - targets: ['192.168.40.6:9090']

  - job_name: 'cadvisor'
    scrape_interval: 5s
    static_configs:
        - targets: ['192.168.40.6:8080', '192.168.40.7:8080', '192.168.40.8']

  - job_name: 'node-exporter'
    scrape_interval: 5s
    static_configs:
        - targets: ['192.168.40.6:9100']
          labels:
            instance: test-1 - 192.168.40.6
            service: node-service
        - targets: ['192.168.40.7:9100']
          labels:
            instance: test-2 - 192.168.40.7
            service: node-service
        - targets: ['192.168.40.8:9100']
          labels:
            instance: test-3 - 192.168.40.8
            service: node-service
</code></pre>
<h3 id="添加邮件告警配置文件">添加邮件告警配置文件</h3>
<p>添加配置文件alertmanager.yml，配置收发邮件邮箱<br>
vim alertmanager.yml</p>
<pre><code class="language-bash">global:
  # The smarthost and SMTP sender used for mail notifications.  用于邮件通知的智能主机和SMTP发件人。
  smtp_smarthost: 'smtp.163.com:25'
  smtp_from: 'xxxxxxxxx@163.com'
  smtp_auth_username: 'xxxxxxxxxxxxxxx@163.com'
  smtp_auth_password: 'xxxxxxxxxxxxx'
  # The auth token for Hipchat.    Hipchat的身份验证令牌。

templates:
  - '/etc/alertmanager/default-monitor.tmpl'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 5m
  receiver: 'mail'


receivers:
- name: 'mail'
  email_configs:
  - to: 'xxxxxxxxxxxxxxxx@163.com, xxxxxxxxxxxxxx@qq.com'
    send_resolved: true #告警恢复通知
    html: '{{ template &quot;default-monitor.html&quot; . }}'  #应用那个模板
    headers: { Subject: &quot;[WARN] 报警邮件&quot; } #邮件主题信息 如果不写headers也可以在模板中定义默认加载email.default.subject这个模板
</code></pre>
<h3 id="添加报警规则">添加报警规则</h3>
<pre><code class="language-bash">mkdir -p /usr/local/src/config/rule
cd /usr/local/src/config/rule
</code></pre>
<p>创建两个文件</p>
<p>node-exporter-record-rule.yml<br>
node-exporter-alert-rule.yml</p>
<p>第一个文件用于记录规则，第二个是报警规则。<br>
由于之前我们在prometheus.yml中已经引用了所有已rule结尾的文件，所以我们不用在修改prometheus.yml配置文件。</p>
<p>创建node-exporter-record-rule.yml</p>
<pre><code class="language-bash">groups:
  - name: node-exporter-record
    rules:
    - expr: up{job=~&quot;node-exporter&quot;}
      record: node_exporter:up
      labels:
        desc: &quot;节点是否在线, 在线1,不在线0&quot;
        unit: &quot; &quot;
        job: &quot;node-exporter&quot;
    - expr: time() - node_boot_time_seconds{}
      record: node_exporter:node_uptime
      labels:
        desc: &quot;节点的运行时间&quot;
        unit: &quot;s&quot;
        job: &quot;node-exporter&quot;
##############################################################################################
#                              cpu                                                           #
    - expr: (1 - avg by (environment,instance) (irate(node_cpu_seconds_total{job=&quot;node-exporter&quot;,mode=&quot;idle&quot;}[5m])))  * 100
      record: node_exporter:cpu:total:percent
      labels:
        desc: &quot;节点的cpu总消耗百分比&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;

    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total{job=&quot;node-exporter&quot;,mode=&quot;idle&quot;}[5m])))  * 100
      record: node_exporter:cpu:idle:percent
      labels:
        desc: &quot;节点的cpu idle百分比&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;

    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total{job=&quot;node-exporter&quot;,mode=&quot;iowait&quot;}[5m])))  * 100
      record: node_exporter:cpu:iowait:percent
      labels:
        desc: &quot;节点的cpu iowait百分比&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;


    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total{job=&quot;node-exporter&quot;,mode=&quot;system&quot;}[5m])))  * 100
      record: node_exporter:cpu:system:percent
      labels:
        desc: &quot;节点的cpu system百分比&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;

    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total{job=&quot;node-exporter&quot;,mode=&quot;user&quot;}[5m])))  * 100
      record: node_exporter:cpu:user:percent
      labels:
        desc: &quot;节点的cpu user百分比&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;

    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total{job=&quot;node-exporter&quot;,mode=~&quot;softirq|nice|irq|steal&quot;}[5m])))  * 100
      record: node_exporter:cpu:other:percent
      labels:
        desc: &quot;节点的cpu 其他的百分比&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;
##############################################################################################


##############################################################################################
#                                    memory                                                  #
    - expr: node_memory_MemTotal_bytes{job=&quot;node-exporter&quot;}
      record: node_exporter:memory:total
      labels:
        desc: &quot;节点的内存总量&quot;
        unit: byte
        job: &quot;node-exporter&quot;

    - expr: node_memory_MemFree_bytes{job=&quot;node-exporter&quot;}
      record: node_exporter:memory:free
      labels:
        desc: &quot;节点的剩余内存量&quot;
        unit: byte
        job: &quot;node-exporter&quot;

    - expr: node_memory_MemTotal_bytes{job=&quot;node-exporter&quot;} - node_memory_MemFree_bytes{job=&quot;node-exporter&quot;}
      record: node_exporter:memory:used
      labels:
        desc: &quot;节点的已使用内存量&quot;
        unit: byte
        job: &quot;node-exporter&quot;

    - expr: node_memory_MemTotal_bytes{job=&quot;node-exporter&quot;} - node_memory_MemAvailable_bytes{job=&quot;node-exporter&quot;}
      record: node_exporter:memory:actualused
      labels:
        desc: &quot;节点用户实际使用的内存量&quot;
        unit: byte
        job: &quot;node-exporter&quot;

    - expr: (1-(node_memory_MemAvailable_bytes{job=&quot;node-exporter&quot;} / (node_memory_MemTotal_bytes{job=&quot;node-exporter&quot;})))* 100
      record: node_exporter:memory:used:percent
      labels:
        desc: &quot;节点的内存使用百分比&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;

    - expr: ((node_memory_MemAvailable_bytes{job=&quot;node-exporter&quot;} / (node_memory_MemTotal_bytes{job=&quot;node-exporter&quot;})))* 100
      record: node_exporter:memory:free:percent
      labels:
        desc: &quot;节点的内存剩余百分比&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;
##############################################################################################
#                                   load                                                     #
    - expr: sum by (instance) (node_load1{job=&quot;node-exporter&quot;})
      record: node_exporter:load:load1
      labels:
        desc: &quot;系统1分钟负载&quot;
        unit: &quot; &quot;
        job: &quot;node-exporter&quot;

    - expr: sum by (instance) (node_load5{job=&quot;node-exporter&quot;})
      record: node_exporter:load:load5
      labels:
        desc: &quot;系统5分钟负载&quot;
        unit: &quot; &quot;
        job: &quot;node-exporter&quot;

    - expr: sum by (instance) (node_load15{job=&quot;node-exporter&quot;})
      record: node_exporter:load:load15
      labels:
        desc: &quot;系统15分钟负载&quot;
        unit: &quot; &quot;
        job: &quot;node-exporter&quot;

##############################################################################################
#                                 disk                                                       #
    - expr: node_filesystem_size_bytes{job=&quot;node-exporter&quot; ,fstype=~&quot;ext4|xfs&quot;}
      record: node_exporter:disk:usage:total
      labels:
        desc: &quot;节点的磁盘总量&quot;
        unit: byte
        job: &quot;node-exporter&quot;

    - expr: node_filesystem_avail_bytes{job=&quot;node-exporter&quot;,fstype=~&quot;ext4|xfs&quot;}
      record: node_exporter:disk:usage:free
      labels:
        desc: &quot;节点的磁盘剩余空间&quot;
        unit: byte
        job: &quot;node-exporter&quot;

    - expr: node_filesystem_size_bytes{job=&quot;node-exporter&quot;,fstype=~&quot;ext4|xfs&quot;} - node_filesystem_avail_bytes{job=&quot;node-exporter&quot;,fstype=~&quot;ext4|xfs&quot;}
      record: node_exporter:disk:usage:used
      labels:
        desc: &quot;节点的磁盘使用的空间&quot;
        unit: byte
        job: &quot;node-exporter&quot;

    - expr:  (1 - node_filesystem_avail_bytes{job=&quot;node-exporter&quot;,fstype=~&quot;ext4|xfs&quot;} / node_filesystem_size_bytes{job=&quot;node-exporter&quot;,fstype=~&quot;ext4|xfs&quot;}) * 100
      record: node_exporter:disk:used:percent
      labels:
        desc: &quot;节点的磁盘的使用百分比&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;

    - expr: irate(node_disk_reads_completed_total{job=&quot;node-exporter&quot;}[1m])
      record: node_exporter:disk:read:count:rate
      labels:
        desc: &quot;节点的磁盘读取速率&quot;
        unit: &quot;次/秒&quot;
        job: &quot;node-exporter&quot;

    - expr: irate(node_disk_writes_completed_total{job=&quot;node-exporter&quot;}[1m])
      record: node_exporter:disk:write:count:rate
      labels:
        desc: &quot;节点的磁盘写入速率&quot;
        unit: &quot;次/秒&quot;
        job: &quot;node-exporter&quot;

    - expr: (irate(node_disk_written_bytes_total{job=&quot;node-exporter&quot;}[1m]))/1024/1024
      record: node_exporter:disk:read:mb:rate
      labels:
        desc: &quot;节点的设备读取MB速率&quot;
        unit: &quot;MB/s&quot;
        job: &quot;node-exporter&quot;

    - expr: (irate(node_disk_read_bytes_total{job=&quot;node-exporter&quot;}[1m]))/1024/1024
      record: node_exporter:disk:write:mb:rate
      labels:
        desc: &quot;节点的设备写入MB速率&quot;
        unit: &quot;MB/s&quot;
        job: &quot;node-exporter&quot;

##############################################################################################
#                                filesystem                                                  #
    - expr:   (1 -node_filesystem_files_free{job=&quot;node-exporter&quot;,fstype=~&quot;ext4|xfs&quot;} / node_filesystem_files{job=&quot;node-exporter&quot;,fstype=~&quot;ext4|xfs&quot;}) * 100
      record: node_exporter:filesystem:used:percent
      labels:
        desc: &quot;节点的inode的剩余可用的百分比&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;
#############################################################################################
#                                filefd                                                     #
    - expr: node_filefd_allocated{job=&quot;node-exporter&quot;}
      record: node_exporter:filefd_allocated:count
      labels:
        desc: &quot;节点的文件描述符打开个数&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;

    - expr: node_filefd_allocated{job=&quot;node-exporter&quot;}/node_filefd_maximum{job=&quot;node-exporter&quot;} * 100
      record: node_exporter:filefd_allocated:percent
      labels:
        desc: &quot;节点的文件描述符打开百分比&quot;
        unit: &quot;%&quot;
        job: &quot;node-exporter&quot;

#############################################################################################
#                                network                                                    #
    - expr: avg by (environment,instance,device) (irate(node_network_receive_bytes_total{device=~&quot;eth0|eth1|ens33|ens37&quot;}[1m]))
      record: node_exporter:network:netin:bit:rate
      labels:
        desc: &quot;节点网卡eth0每秒接收的比特数&quot;
        unit: &quot;bit/s&quot;
        job: &quot;node-exporter&quot;

    - expr: avg by (environment,instance,device) (irate(node_network_transmit_bytes_total{device=~&quot;eth0|eth1|ens33|ens37&quot;}[1m]))
      record: node_exporter:network:netout:bit:rate
      labels:
        desc: &quot;节点网卡eth0每秒发送的比特数&quot;
        unit: &quot;bit/s&quot;
        job: &quot;node-exporter&quot;

    - expr: avg by (environment,instance,device) (irate(node_network_receive_packets_total{device=~&quot;eth0|eth1|ens33|ens37&quot;}[1m]))
      record: node_exporter:network:netin:packet:rate
      labels:
        desc: &quot;节点网卡每秒接收的数据包个数&quot;
        unit: &quot;个/秒&quot;
        job: &quot;node-exporter&quot;

    - expr: avg by (environment,instance,device) (irate(node_network_transmit_packets_total{device=~&quot;eth0|eth1|ens33|ens37&quot;}[1m]))
      record: node_exporter:network:netout:packet:rate
      labels:
        desc: &quot;节点网卡发送的数据包个数&quot;
        unit: &quot;个/秒&quot;
        job: &quot;node-exporter&quot;

    - expr: avg by (environment,instance,device) (irate(node_network_receive_errs_total{device=~&quot;eth0|eth1|ens33|ens37&quot;}[1m]))
      record: node_exporter:network:netin:error:rate
      labels:
        desc: &quot;节点设备驱动器检测到的接收错误包的数量&quot;
        unit: &quot;个/秒&quot;
        job: &quot;node-exporter&quot;

    - expr: avg by (environment,instance,device) (irate(node_network_transmit_errs_total{device=~&quot;eth0|eth1|ens33|ens37&quot;}[1m]))
      record: node_exporter:network:netout:error:rate
      labels:
        desc: &quot;节点设备驱动器检测到的发送错误包的数量&quot;
        unit: &quot;个/秒&quot;
        job: &quot;node-exporter&quot;

    - expr: node_tcp_connection_states{job=&quot;node-exporter&quot;, state=&quot;established&quot;}
      record: node_exporter:network:tcp:established:count
      labels:
        desc: &quot;节点当前established的个数&quot;
        unit: &quot;个&quot;
        job: &quot;node-exporter&quot;

    - expr: node_tcp_connection_states{job=&quot;node-exporter&quot;, state=&quot;time_wait&quot;}
      record: node_exporter:network:tcp:timewait:count
      labels:
        desc: &quot;节点timewait的连接数&quot;
        unit: &quot;个&quot;
        job: &quot;node-exporter&quot;

    - expr: sum by (environment,instance) (node_tcp_connection_states{job=&quot;node-exporter&quot;})
      record: node_exporter:network:tcp:total:count
      labels:
        desc: &quot;节点tcp连接总数&quot;
        unit: &quot;个&quot;
        job: &quot;node-exporter&quot;

#############################################################################################
#                                process                                                    #
    - expr: node_processes_state{state=&quot;Z&quot;}
      record: node_exporter:process:zoom:total:count
      labels:
        desc: &quot;节点当前状态为zoom的个数&quot;
        unit: &quot;个&quot;
        job: &quot;node-exporter&quot;
#############################################################################################
#                                other                                                    #
    - expr: abs(node_timex_offset_seconds{job=&quot;node-exporter&quot;})
      record: node_exporter:time:offset
      labels:
        desc: &quot;节点的时间偏差&quot;
        unit: &quot;s&quot;
        job: &quot;node-exporter&quot;

#############################################################################################

    - expr: count by (instance) ( count by (instance,cpu) (node_cpu_seconds_total{ mode='system'}) )
      record: node_exporter:cpu:count
</code></pre>
<p>创建node-exporter-alert-rule.yml</p>
<pre><code class="language-bash">groups:
  - name: node-exporter-alert
    rules:
    - alert: node-exporter-down
      expr: node_exporter:up == 0
      for: 1m
      labels:
        severity: 'critical'
      annotations:
        summary: &quot;instance: {{ $labels.instance }} 宕机了&quot;
        description: &quot;instance: {{ $labels.instance }} \n- job: {{ $labels.job }} 关机了， 时间已经1分钟了。&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;



    - alert: node-exporter-cpu-high
      expr:  node_exporter:cpu:total:percent &gt; 80
      for: 3m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} cpu 使用率高于 {{ $value }}&quot;
        description: &quot;instance: {{ $labels.instance }} \n- job: {{ $labels.job }} CPU使用率已经持续三分钟高过80% 。&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;

    - alert: node-exporter-cpu-iowait-high
      expr:  node_exporter:cpu:iowait:percent &gt;= 12
      for: 3m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} cpu iowait 使用率高于 {{ $value }}&quot;
        description: &quot;instance: {{ $labels.instance }} \n- job: {{ $labels.job }} cpu iowait使用率已经持续三分钟高过12%&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;


    - alert: node-exporter-load-load1-high
      expr:  (node_exporter:load:load1) &gt; (node_exporter:cpu:count) * 1.2
      for: 3m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} load1 使用率高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;


    - alert: node-exporter-memory-high
      expr:  node_exporter:memory:used:percent &gt; 85
      for: 3m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} memory 使用率高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;


    - alert: node-exporter-disk-high
      expr:  node_exporter:disk:used:percent &gt; 88
      for: 10m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} disk 使用率高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;


    - alert: node-exporter-disk-read:count-high
      expr:  node_exporter:disk:read:count:rate &gt; 3000
      for: 2m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} iops read 使用率高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;


    - alert: node-exporter-disk-write-count-high
      expr:  node_exporter:disk:write:count:rate &gt; 3000
      for: 2m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} iops write 使用率高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;





    - alert: node-exporter-disk-read-mb-high
      expr:  node_exporter:disk:read:mb:rate &gt; 60
      for: 2m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} 读取字节数 高于 {{ $value }}&quot;
        description: &quot;&quot;
        instance: &quot;{{ $labels.instance }}&quot;
        value: &quot;{{ $value }}&quot;


    - alert: node-exporter-disk-write-mb-high
      expr:  node_exporter:disk:write:mb:rate &gt; 60
      for: 2m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} 写入字节数 高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;


    - alert: node-exporter-filefd-allocated-percent-high
      expr:  node_exporter:filefd_allocated:percent &gt; 80
      for: 10m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} 打开文件描述符 高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;


    - alert: node-exporter-network-netin-error-rate-high
      expr:  node_exporter:network:netin:error:rate &gt; 4
      for: 1m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} 包进入的错误速率 高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;

    - alert: node-exporter-network-netin-packet-rate-high
      expr:  node_exporter:network:netin:packet:rate &gt; 35000
      for: 1m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} 包进入速率 高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;


    - alert: node-exporter-network-netout-packet-rate-high
      expr:  node_exporter:network:netout:packet:rate &gt; 35000
      for: 1m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} 包流出速率 高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;


    - alert: node-exporter-network-tcp-total-count-high
      expr:  node_exporter:network:tcp:total:count &gt; 40000
      for: 1m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} tcp连接数量 高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;


    - alert: node-exporter-process-zoom-total-count-high
      expr:  node_exporter:process:zoom:total:count &gt; 10
      for: 10m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} 僵死进程数量 高于 {{ $value }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;


    - alert: node-exporter-time-offset-high
      expr:  node_exporter:time:offset &gt; 0.03
      for: 2m
      labels:
        severity: info
      annotations:
        summary: &quot;instance: {{ $labels.instance }} {{ $labels.desc }}  {{ $value }} {{ $labels.unit }}&quot;
        description: &quot;&quot;
        value: &quot;{{ $value }}&quot;
        instance: &quot;{{ $labels.instance }}&quot;
</code></pre>
<h3 id="添加告警模板">添加告警模板</h3>
<pre><code class="language-bash">mkdir  template
vim template/default-monitor.tmpl
</code></pre>
<pre><code class="language-tmpl">{{ define &quot;default-monitor.html&quot; }}
{{ range .Alerts }}
=========start==========&lt;br&gt;
告警程序: prometheus_alert &lt;br&gt;
告警级别: {{ .Labels.severity }} 级 &lt;br&gt;
告警类型: {{ .Labels.alertname }} &lt;br&gt;
故障主机: {{ .Labels.instance }} &lt;br&gt;
告警主题: {{ .Annotations.summary }} &lt;br&gt;
告警详情: {{ .Annotations.description }} &lt;br&gt;
触发时间: {{ .StartsAt.Format &quot;2019-08-04 16:58:15&quot; }} &lt;br&gt;
=========end==========&lt;br&gt;
{{ end }}
{{ end }}
</code></pre>
<h2 id="编写docker-compose文件">编写docker-compose文件</h2>
<p>vim    docker-compose-monitor.yml</p>
<pre><code class="language-yaml">version: '2'

networks:
    monitor:
        driver: bridge

services:
  prometheus:
    image: prom/prometheus:v2.16.0
    container_name: prometheus
    restart: always
    ports:
     - 9090:9090
    volumes:
     - /bsn/prometheus/prometheus:/prometheus
     - /bsn/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
     - /bsn/prometheus/alert/alert.rules:/usr/local/prometheus/rules/alert.rules
     - /etc/localtime:/etc/localtime
    command:
     - '--config.file=/etc/prometheus/prometheus.yml'
     - '--storage.tsdb.path=/prometheus/'
     - '--storage.tsdb.retention.time=90d'
    depends_on:
     - alertmanager
  grafana:
    image: grafana/grafana:6.4.2
    container_name: grafana
    restart: always
    volumes:
     - /bsn/prometheus/grafana:/var/lib/grafana
     - /bsn/prometheus/grafana/grafana.ini:/etc/grafana/grafana.ini
     - /etc/localtime:/etc/localtime
    ports:
     - 3000:3000
    depends_on:
     - prometheus
  alertmanager:
    image: prom/alertmanager:v0.21.0-rc.0
    container_name: alertmanager
    volumes:
      - /bsn/prometheus/alert/alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - /bsn/prometheus/alert/email.tmpl:/etc/alertmanager/template/email.tmpl
      - /etc/localtime:/etc/localtime
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
    ports:
      - 9093:9093
    restart: always

    node-exporter:
        image: quay.io/prometheus/node-exporter
        container_name: node-exporter
        hostname: $HOSTNAME
        restart: always
        ports:
            - &quot;9100:9100&quot;
        volumes:
          - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro
          - /proc:/host/proc:ro
          - /sys:/host/sys:ro
          - /:/rootfs:ro
        restart: always
        command:
          - '--path.procfs=/host/proc'
          - '--path.sysfs=/host/sys'
          - '--path.rootfs=/rootfs'

    cadvisor:
        image: google/cadvisor:latest
        container_name: cadvisor
        hostname: cadvisor
        restart: always
        volumes:
            - /:/rootfs:ro
            - /var/run:/var/run:rw
            - /sys:/sys:ro
            - /var/lib/docker/:/var/lib/docker:ro
        ports:
            - &quot;8080:8080&quot;

</code></pre>
<h2 id="启动docker-compose">启动docker-compose</h2>
<pre><code class="language-bash">#启动容器：
docker-compose -f /usr/local/src/config/docker-compose-monitor.yml up -d
#删除容器：
docker-compose -f /usr/local/src/config/docker-compose-monitor.yml down
#重启容器：
docker restart id
</code></pre>
<p>在其他节点分别启动cadvisor和node-exporter容器</p>
<pre><code class="language-yaml">version: '3'

services:
    node-exporter:
        image: quay.io/prometheus/node-exporter
        container_name: node-exporter
        hostname: $HOSTNAME
        restart: always
        ports:
            - &quot;9100:9100&quot;
        volumes:
          - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro
          - /proc:/host/proc:ro
          - /sys:/host/sys:ro
          - /:/rootfs:ro
        restart: always
        command:
          - '--path.procfs=/host/proc'
          - '--path.sysfs=/host/sys'
          - '--path.rootfs=/rootfs'
    cadvisor:
        image: google/cadvisor:latest
        container_name: cadvisor
        hostname: cadvisor
        restart: always
        volumes:
            - /:/rootfs:ro
            - /var/run:/var/run:rw
            - /sys:/sys:ro
            - /var/lib/docker/:/var/lib/docker:ro
        ports:
            - &quot;8080:8080&quot;
</code></pre>
<p><strong>容器启动如下：</strong><br>
<img src="https://chriswsq.github.io/post-images/1599121054830.png" alt="" loading="lazy"></p>
<p><strong>prometheus targets界面如下：</strong><br>
<img src="https://chriswsq.github.io/post-images/1599121262766.png" alt="" loading="lazy"></p>
<blockquote>
<p>备注：如果State为Down，应该是防火墙问题，参考下面防火墙配置。</p>
</blockquote>
<p><strong>prometheus targets界面如下：</strong><br>
<img src="https://chriswsq.github.io/post-images/1599121408543.png" alt="" loading="lazy"></p>
<blockquote>
<p>备注：如果没有数据，同步下时间。</p>
</blockquote>
<h2 id="配置grafana">配置grafana</h2>
<h3 id="添加prometheus数据源">添加Prometheus数据源</h3>
<figure data-type="image" tabindex="1"><img src="https://chriswsq.github.io/post-images/1599121507210.png" alt="" loading="lazy"></figure>
<h3 id="配置dashboards">配置dashboards</h3>
<p><strong>说明：可以用自带模板，也可以去https://grafana.com/dashboards，下载对应的模板。</strong></p>
<p>添加监控服务器模板 此处用的模板id是   8919   也可使用（1860）</p>
<p><img src="https://chriswsq.github.io/post-images/1599121616143.png" alt="" loading="lazy"><br>
<img src="https://chriswsq.github.io/post-images/1599121699226.png" alt="" loading="lazy"><br>
<img src="https://chriswsq.github.io/post-images/1599121761143.png" alt="" loading="lazy"></p>
<p>添加监控容器模板   此处用  893   也可用（8321）</p>
<figure data-type="image" tabindex="2"><img src="https://chriswsq.github.io/post-images/1599122084044.png" alt="" loading="lazy"></figure>
<p>也可用综合的 9276</p>
<h2 id="告警">告警</h2>
<p>停止192.168.40.7 的 cadvisor和node-exporter容器<br>
<img src="https://chriswsq.github.io/post-images/1599122289244.png" alt="" loading="lazy"></p>
<p>收到告警邮件<br>
<img src="https://chriswsq.github.io/post-images/1599122330270.png" alt="" loading="lazy"></p>
<blockquote>
<p>注： 如果日期有问题则将告警模板的触发时间参数改为 <code>{{ .StartsAt.Format &quot;2019-08-04 16:58:15&quot; }} &lt;br&gt;</code> 即可。</p>
</blockquote>
<p>参考博客：<br>
https://juejin.im/post/6844903809517371406<br>
https://blog.csdn.net/w342164796/article/details/105079231/<br>
https://blog.csdn.net/aixiaoyang168/article/details/98474494</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[k8s 如何关联pvc到特定的pv?]]></title>
        <id>https://chriswsq.github.io/post/k8s-ru-he-guan-lian-pvc-dao-te-ding-de-pv/</id>
        <link href="https://chriswsq.github.io/post/k8s-ru-he-guan-lian-pvc-dao-te-ding-de-pv/">
        </link>
        <updated>2020-08-27T05:59:30.000Z</updated>
        <summary type="html"><![CDATA[<p>部署有状态的应用时需要挂载相应的配置文件，了解下最常用的pv（nfs）和pvc绑定关系</p>
]]></summary>
        <content type="html"><![CDATA[<p>部署有状态的应用时需要挂载相应的配置文件，了解下最常用的pv（nfs）和pvc绑定关系</p>
<!-- more -->
<p>首先肯定要在放置配置文件的地方配置nfs服务</p>
<p>如何关联pvc到特定的pv?</p>
<p>我们可以使用对 pv 打 label 的方式，具体如下：</p>
<p>创建 pv，指定 label</p>
<p><code>cat nfs-test-pv.yaml</code></p>
<pre><code class="language-yaml">apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-test-pv
#  namespace: test-pv-test
  labels:
    pv: nfs-test-pv
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteMany
  nfs:
    # FIXME: use the right IP
    server: 192.168.40.6
    path: &quot;/test/&quot;
</code></pre>
<p>然后创建 pvc，使用 matchLabel 来关联刚创建的 pv:nfs-test-pv</p>
<p><code>cat nfs-test-pvc.yaml</code></p>
<pre><code class="language-yaml">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-test-pvc
#  namespace: test-pv-test
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: &quot;&quot;
  resources:
    requests:
      storage: 90Mi
  selector:
    matchLabels:
      pv: nfs-test-pv
</code></pre>
<p>下面开始测试：</p>
<p><strong>创建pv</strong></p>
<p><code>kubectl apply -f nfs-test-pv.yaml</code></p>
<p><strong>查看pv</strong></p>
<pre><code>kubectl get pv
[root@test-1 ~]# kubectl get pv
NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS   REASON   AGE
ca-service   100Mi      RWX            Retain           Bound    default/ca-service-pvc                           30m

</code></pre>
<p><strong>然后创建 pvc</strong><br>
<code>kubectl apply -f nfs-test-pvc.yaml</code></p>
<p><strong>查看pvc</strong></p>
<pre><code>[root@test-1 ~]# kubectl get pvc
NAME             STATUS   VOLUME       CAPACITY   ACCESS MODES   STORAGECLASS   AGE
ca-service-pvc   Bound    ca-service   100Mi      RWX                           30m
</code></pre>
<p>已都正确绑定</p>
]]></content>
    </entry>
</feed>