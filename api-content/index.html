{"posts":[{"title":"k8s 如何关联pvc到特定的pv?","content":"部署有状态的应用时需要挂载相应的配置文件，了解下最常用的pv（nfs）和pvc绑定关系首先肯定要在放置配置文件的地方配置nfs服务如何关联pvc到特定的pv?我们可以使用对pv打label的方式，具体如下：创建pv，指定labelcatnfs-test-pv.yamlapiVersion:v1kind:PersistentVolumemetadata:name:nfs-test-pv#namespace:test-pv-testlabels:pv:nfs-test-pvspec:capacity:storage:100MiaccessModes:-ReadWriteManynfs:#FIXME:usetherightIPserver:192.168.40.6path:&quot;/test/&quot;然后创建pvc，使用matchLabel来关联刚创建的pv:nfs-test-pvcatnfs-test-pvc.yamlapiVersion:v1kind:PersistentVolumeClaimmetadata:name:nfs-test-pvc#namespace:test-pv-testspec:accessModes:-ReadWriteManystorageClassName:&quot;&quot;resources:requests:storage:90Miselector:matchLabels:pv:nfs-test-pv下面开始测试：创建pvkubectlapply-fnfs-test-pv.yaml查看pvkubectlgetpv[root@test-1~]#kubectlgetpvNAMECAPACITYACCESSMODESRECLAIMPOLICYSTATUSCLAIMSTORAGECLASSREASONAGEca-service100MiRWXRetainBounddefault/ca-service-pvc30m然后创建pvckubectlapply-fnfs-test-pvc.yaml查看pvc[root@test-1~]#kubectlgetpvcNAMESTATUSVOLUMECAPACITYACCESSMODESSTORAGECLASSAGEca-service-pvcBoundca-service100MiRWX30m已都正确绑定","link":"https://chriswsq.github.io/post/k8s-ru-he-guan-lian-pvc-dao-te-ding-de-pv/"},{"title":"k8s pv,pvc无法删除问题","content":"解决k8spv,pvc无法删除问题一般删除步骤为：先删pod再删pvc最后删pv但是遇到pv始终处于“Terminating”状态，而且delete不掉。如下：[root@test-1pv]#kubectlgetpvNAMECAPACITYACCESSMODESRECLAIMPOLICYSTATUSCLAIMSTORAGECLASSREASONAGEca-service100MiRWXRetainTerminatingdefault/ca-service-pvc17h解决方法：直接删除k8s中的记录：kubectlpatchpvca-service-p'{&quot;metadata&quot;:{&quot;finalizers&quot;:null}}'","link":"https://chriswsq.github.io/post/k8s-pvpvc-wu-fa-shan-chu-wen-ti/"},{"title":"kubeadm部署k8s","content":"部署环境主机名centos版本IPdocker-versionflanel-versionkeepalived-version主机配置k8s-master17.4192.168.40.618.09.9v0.11.0v1.3.52C4Gk8s-node17.4192.168.40.718.09.9v0.11.0v1.3.52C4Gk8s-node27.4192.168.40.818.09.9v0.11.0v1.3.52C4G二、高可用架构注：此图为负载为loadbanacer为云上的负载方案在此就说下本人在这里犯的小错误1.拉镜像时各个节点都要拉取2.部署flannel时因为网络的原因可以先down下载，在执行kubectlapply-fkube-flannel.ymlwgethttps://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml3.部署后会发现主节点Ready而从节点都是NotReady，这是因为master节点的这个文件/etc/cni/net.d/10-flannel.conflist是因为安装flannel生成的，而node节点是通过master分配生成的所以少一个参数&quot;cniVersion&quot;:&quot;0.3.1&quot;,直接把maser推过去到node节点，或者手动改下就可以了kubeadm部署kubeadmjoin192.168.154.6:6443--tokenhum394.uf5ehv9ye6w661bz--discovery-token-ca-cert-hashsha256:6bd3388a191ae0afb2ca86102e655a2ced4c6e6b6bef82afebbdef8154a57b61node{&quot;name&quot;:&quot;cbr0&quot;,&quot;plugins&quot;:[{&quot;type&quot;:&quot;flannel&quot;,&quot;delegate&quot;:{&quot;hairpinMode&quot;:true,&quot;isDefaultGateway&quot;:true}},{&quot;type&quot;:&quot;portmap&quot;,&quot;capabilities&quot;:{&quot;portMappings&quot;:true}}]}master{&quot;name&quot;:&quot;cbr0&quot;,&quot;cniVersion&quot;:&quot;0.3.1&quot;,&quot;plugins&quot;:[{&quot;type&quot;:&quot;flannel&quot;,&quot;delegate&quot;:{&quot;hairpinMode&quot;:true,&quot;isDefaultGateway&quot;:true}},{&quot;type&quot;:&quot;portmap&quot;,&quot;capabilities&quot;:{&quot;portMappings&quot;:true}}]}参考https://www.kubernetes.org.cn/6632.html","link":"https://chriswsq.github.io/post/kubeadm-bu-shu-k8s/"},{"title":"logstash配置","content":"补上一份自己在工作中用的一份logstash的配置文件，万变不离其宗，可以参看此内容这一份匹配的日志是，前部分是字符串，后一部分是json格式的日志input{kafka{bootstrap_servers=&gt;&quot;kafka1:9092,kafka2:9092,kafka3:9092&quot;topics=&gt;[&quot;tomcat&quot;]type=&gt;&quot;tomcat&quot;codec=&gt;json{charset=&gt;&quot;UTF-8&quot;}}}filter{if[type]==&quot;tomcat&quot;{grok{match=&gt;{'message'=&gt;'%{TIMESTAMP_ISO8601:access_time}%{LOGLEVEL:loglevel}%{INT:number}---\\[%{DATA:thread_name}\\]%{URIHOST:type_name}%{SPACE}*:%{HOSTNAME:status}%{HOSTNAME:action}%{URIHOST:usetime}((?&lt;request&gt;(.*)(?=Creating)/?)|)(%{GREEDYDATA:sql}|)'}}json{source=&gt;&quot;request&quot;#target=&gt;&quot;parsedJson&quot;remove_field=&gt;[&quot;request&quot;]}mutate{remove_field=&gt;[&quot;agent&quot;]remove_field=&gt;[&quot;[ecs][version]&quot;,&quot;[fields][log_topics]&quot;,&quot;[fields][registry_file]&quot;,&quot;[host][name]&quot;,&quot;[input][type]&quot;,&quot;[@version]&quot;,&quot;[tag]&quot;,&quot;[id]&quot;,&quot;[score]&quot;]}}}output{if[type]==&quot;tomcat&quot;{elasticsearch{hosts=&gt;[&quot;elasticsearch1:9200&quot;,&quot;elasticsearch2:9200&quot;,&quot;elasticsearch3:9200&quot;]index=&gt;&quot;tomcat-%{+YYYY.MM.dd}&quot;user=&gt;'elastic'password=&gt;'d27DuDQjTChIdv3sE8oI'}}}这一份是整个的日志都是json格式input{kafka{bootstrap_servers=&gt;&quot;kafka1:9092,kafka2:9092,kafka3:9092&quot;topics=&gt;[&quot;gateway&quot;]type=&gt;&quot;gateway&quot;codec=&gt;json{charset=&gt;&quot;UTF-8&quot;}}}filter{if[type]==&quot;gateway&quot;{json{source=&gt;&quot;message&quot;#target=&gt;&quot;doc&quot;remove_field=&gt;[&quot;message&quot;]}mutate{remove_field=&gt;[&quot;agent&quot;]remove_field=&gt;[&quot;[ecs][version]&quot;,&quot;[fields][log_topics]&quot;,&quot;[fields][registry_file]&quot;,&quot;[host][name]&quot;,&quot;[input][type]&quot;,&quot;[@version]&quot;,&quot;[tag]&quot;,&quot;[id]&quot;,&quot;[score]&quot;]}}}output{if[type]==&quot;gateway&quot;{elasticsearch{hosts=&gt;[&quot;elasticsearch1:9200&quot;,&quot;elasticsearch2:9200&quot;,&quot;elasticsearch3:9200&quot;]index=&gt;&quot;gateway-%{+YYYY.MM.dd}&quot;user=&gt;'elastic'password=&gt;'d27DuDQjTChIdv3sE8oI'}}}这是一份整个日志都是字符串的格式input{kafka{bootstrap_servers=&gt;&quot;kafka1:9092,kafka2:9092,kafka3:9092&quot;topics=&gt;[&quot;gb_core_ngx&quot;]type=&gt;&quot;gb_core_ngx&quot;codec=&gt;json{charset=&gt;&quot;UTF-8&quot;}}}filter{if[type]==&quot;gb_core_ngx&quot;{grok{match=&gt;{&quot;message&quot;=&gt;&quot;%{URIHOST:hostname}:\\[%{TIMESTAMP_ISO8601:local_time}\\](%{NUMBER:status}|-)%{NUMBER:siteid}-%{WORD:idc_num}-%{HOSTNAME:Random_value}(%{HOSTNAME:X_system}|-)*(%{URIHOST:remote_hostname}|-),%{URIHOST:outsite_hostname}\\[(%{URIHOST:upstream_addr}|-)]%{NUMBER:request_time}(%{NUMBER:upstraem_response_time}|-)(%{WORD:request_method}|-)(%{HOSTNAME:http_host}|-)(%{GREEDYDATA:uri}|-)(%{NUMBER:body_bytes_sent}|-)\\|(%{IPORHOST:client_ip}|-)&quot;}}mutate{remove_field=&gt;&quot;tags&quot;remove_field=&gt;&quot;port&quot;remove_field=&gt;&quot;prospector&quot;remove_field=&gt;&quot;beat&quot;remove_field=&gt;&quot;source&quot;remove_field=&gt;&quot;offset&quot;remove_field=&gt;&quot;fields&quot;remove_field=&gt;&quot;host&quot;remove_field=&gt;&quot;@version&quot;remove_field=&gt;&quot;message&quot;remove_field=&gt;&quot;input&quot;convert=&gt;[&quot;body_bytes_sent&quot;,&quot;float&quot;]convert=&gt;[&quot;request_time&quot;,&quot;float&quot;]convert=&gt;[&quot;upstream_response_time&quot;,&quot;float&quot;]add_field=&gt;{&quot;uuid&quot;=&gt;&quot;%{siteid}-%{idc_num}-%{Random_value}&quot;}}#date{#match=&gt;[&quot;local__time&quot;,&quot;yyyy-MM-ddHH:mm:ss&quot;]#}geoip{source=&gt;&quot;client_ip&quot;target=&gt;&quot;geoip&quot;database=&gt;&quot;/opt/GeoLite2-City.mmdb&quot;add_field=&gt;[&quot;[geoip][coordinates]&quot;,&quot;%{[geoip][longitude]}&quot;]add_field=&gt;[&quot;[geoip][coordinates]&quot;,&quot;%{[geoip][latitude]}&quot;]}mutate{convert=&gt;[&quot;[geoip][coordinates]&quot;,&quot;float&quot;]}}}output{if[type]==&quot;gb_core_ngx&quot;{elasticsearch{hosts=&gt;[&quot;elasticsearch1:9200&quot;,&quot;elasticsearch2:9200&quot;,&quot;elasticsearch3:9200&quot;]index=&gt;&quot;logstash-gb_core_ngx-%{+YYYY.MM.dd}&quot;user=&gt;'elastic'password=&gt;'d27DuDQjTChIdv3sE8oI'}}}添加&amp;组合字段filter{grok{match=&gt;{&quot;message&quot;=&gt;'&quot;(%{GREEDYDATA:cust_date})&quot;,&quot;(%{TIME:cust_time})&quot;,&quot;(%{NUMBER:author})&quot;'}add_field=&gt;{&quot;date_time&quot;=&gt;&quot;%{cust_date};%{cust_time}&quot;}}date{match=&gt;[&quot;date_time&quot;,&quot;yyyy-MM-dd;hh:mm:ss&quot;]target=&gt;&quot;@timestamp&quot;add_field=&gt;{&quot;debug&quot;=&gt;&quot;timestampMatched&quot;}}","link":"https://chriswsq.github.io/post/logstash-pei-zhi/"},{"title":"elk用户认证问题","content":"elk7+的用户认证免费了，不过没有集成在安装包里，还需要自己再进行一些操作，在此记录下自己再配置用户认证时遇到的问题开启安全功能首先在es配置文件中开启安全功能.....xpack.security.enabled:truexpack.security.transport.ssl.enabled然后重启es服务生成证书，配置Node间SSL通信创建ca证书$bin/elasticsearch-certutilca-v一路回车即可默认的CA证书存放在$ES_HOME目录中这个命令生成格式为PKCS#12名称为elastic-stack-ca.p12的keystore文件，包含CA证书和私钥。创建节点间认证用的证书$./bin/elasticsearch-certutilcert--caelastic-stack-ca.p12一路回车即可配置ES节点使用这个证书$mkdirconfig/certs$mvelastic-*config/certs/llconfig/certs/-rw-------1elasticsearchroot3443Jun915:22elastic-certificates.p12-rw-------1elasticsearchroot2527Jun915:22elastic-stack-ca.p12拷贝这个目录到所有的ES节点中config/certs目录中不需要拷贝CA证书文件，只拷贝cert文件即可配置elasticsearch.yml配置文件，注意所有的node节点都需要配置，这里的配置是使用PKCS#12格式的证书。$vimconfig/elasticsearch.ymlxpack.security.enabled:truexpack.security.transport.ssl.enabled:truexpack.security.transport.ssl.verification_mode:certificate#认证方式使用证书xpack.security.transport.ssl.keystore.path:certs/elastic-certificates.p12xpack.security.transport.ssl.truststore.path:certs/elastic-certificates.p12测试能够正常启动了。好了，我们再来继续之前的生成密码：在随意一台节点即可。生成密码有两个命令，一个为自动生成，一个为手动生成自动bin/elasticsearch-setup-passwordsauto手动bin/elasticsearch-setup-passwordsinteractive这里我们选择自动[root@elasticsearch1elasticsearch]#bin/elasticsearch-setup-passwordsautoInitiatingthesetupofpasswordsforreserveduserselastic,apm_system,kibana,logstash_system,beats_system,remote_monitoring_user.Thepasswordswillberandomlygeneratedandprintedtotheconsole.Pleaseconfirmthatyouwouldliketocontinue[y/N]yChangedpasswordforuserapm_systemPASSWORDapm_system=EKPbqziKu4v3P0MYoYIJChangedpasswordforuserkibanaPASSWORDkibana=48Sf7tIFArn4rmyMW2x4Changedpasswordforuserlogstash_systemPASSWORDlogstash_system=O09XtxP495uNRxHRZxtCChangedpasswordforuserbeats_systemPASSWORDbeats_system=ULC810uP0sVCAKhbwxvEChangedpasswordforuserremote_monitoring_userPASSWORDremote_monitoring_user=zqnO9YBFsoRu5QIsgOi6ChangedpasswordforuserelasticPASSWORDelastic=d27DuDQjTChIdv3sE8oI查看集群节点数量：[root@test-1conf]#curl-uelastic192.168.154.6:9200/_cat/nodesEnterhostpasswordforuser'elastic':10.0.0.67328810.120.080.43ilmr-node-110.0.0.92148010.120.070.22dilmrt*node-210.0.0.93127910.000.030.24dilmrt-node-3kibana的安装配置编辑配置文件$catkibana/config/kibana.yml|grep-Ev&quot;^$|^#&quot;server.port:5601server.host:&quot;0.0.0.0&quot;server.name:&quot;mykibana&quot;elasticsearch.hosts:[&quot;http://localhost:9200&quot;]kibana.index:&quot;.kibana&quot;elasticsearch.username:&quot;kibana&quot;elasticsearch.password:&quot;UKuHceHWudloJk9NvHlX&quot;#i18n.locale:&quot;en&quot;i18n.locale:&quot;zh-CN&quot;xpack.security.encryptionKey:Hz*9yFFaPejHvCkhT*ddNx%WsBgxVSCQ页面访问kibana输入上面生成的管理员elastic的用户和密码，就可以登陆了，我们查看一下license许可吧：一个使用永不过期的Basic许可的免费License，开启了基本的Auth认证和集群间SSL/TLS认证的Elasticsearch集群就创建完毕了。等等，你有没有想过Kibana的配置文件中使用着明文的用户名密码，这里只能通过LInux的权限进行控制了，有没有更安全的方式呢，有的，就是keystore。kibanakeystore安全配置参考官方查看kibana-keystore命令帮助：$./bin/kibana-keystore--helpUsage:bin/kibana-keystore[options][command]AtoolformanagingsettingsstoredintheKibanakeystoreOptions:-V,--versionoutputtheversionnumber-h,--helpoutputusageinformationCommands:create[options]CreatesanewKibanakeystorelist[options]Listentriesinthekeystoreadd[options]&lt;key&gt;Addastringsettingtothekeystoreremove[options]&lt;key&gt;Removeasettingfromthekeystore首先我们创建keystore：$bin/kibana-keystorecreateCreatedKibanakeystorein/opt/elk74/kibana-7.4.2-linux-x86_64/data/kibana.keystore#默认存放位置增加配置：我们要吧kibana.yml配置文件中的敏感信息，比如：elasticsearch.username和elasticsearch.password，给隐藏掉，或者直接去掉；所以这里我们增加两个配置：分别是elasticsearch.password和elasticsearch.username:#查看add的命令帮助：$./bin/kibana-keystoreadd--helpUsage:add[options]&lt;key&gt;AddastringsettingtothekeystoreOptions:-f,--forceoverwriteexistingsettingwithoutprompting-x,--stdinreadsettingvaluefromstdin-s,--silentpreventalllogging-h,--helpoutputusageinformation#创建elasticsearch.username这个key：注意名字必须是kibana.yml中的key$./bin/kibana-keystoreaddelasticsearch.usernameEntervalueforelasticsearch.username:******#输入key对应的value，这里是kibana连接es的账号：kibana#创建elasticsearch.password这个key$./bin/kibana-keystoreaddelasticsearch.passwordEntervalueforelasticsearch.password:********************#输入对应的密码：UKuHceHWudloJk9NvHlX好了，我们把kibana.yml配置文件中的这两项配置删除即可，然后直接启动kibana，kibana会自动已用这两个配置的。最终的kibana.yml配置如下：server.port:5601server.host:&quot;0.0.0.0&quot;server.name:&quot;mykibana&quot;elasticsearch.hosts:[&quot;http://localhost:9200&quot;]kibana.index:&quot;.kibana&quot;#i18n.locale:&quot;en&quot;i18n.locale:&quot;zh-CN&quot;xpack.security.encryptionKey:Hz*9yFFaPejHvCkhT*ddNx%WsBgxVSCQ这样配置文件中就不会出现敏感信息了，达到了更高的安全性。类似的Keystore方式不只是Kibana支持，ELK的产品都是支持的。注意：两个证书文件属主须是启动用户，certs权限为744即可，权限不够会报错,其他节点权限也一样elk_elasticsearch1.1.rqtnwnm1sqzw@test-1|uncaughtexceptioninthread[main]elk_elasticsearch1.1.rqtnwnm1sqzw@test-1|ElasticsearchSecurityException[failedtoloadSSLconfiguration[xpack.security.transport.ssl]];nested:ElasticsearchException[failedtoinitializeSSLTrustManager-notpermittedtoreadtruststorefile[/usr/share/elasticsearch/config/certs/elastic-certificates.p12]];nested:AccessDeniedException[/usr/share/elasticsearch/config/certs/elastic-certificates.p12];elk_elasticsearch1.1.jp9k598g34hq@test-1|&quot;...6more&quot;]}elk_elasticsearch1.1.yk71h6aom47z@test-1|&quot;atorg.elasticsearch.node.Node.&lt;init&gt;(Node.java:481)~[elasticsearch-7.7.0.jar:7.7.0]&quot;,elk_elasticsearch1.1.w26656n32a86@test-1|&quot;...6more&quot;]}elk_elasticsearch1.1.w26656n32a86@test-1|ElasticsearchSecurityException[failedtoloadSSLconfiguration[xpack.security.transport.ssl]];nested:ElasticsearchException[failedtoinitializeSSLTrustManager-notpermittedtoreadtruststorefile[/usr/share/elasticsearch/config/certs/elastic-certificates.p12]];nested:AccessDeniedException[/usr/share/elasticsearch/config/certs/elastic-certificates.p12];elk_elasticsearch1.1.rqtnwnm1sqzw@test-1|Likelyrootcause:java.nio.file.AccessDeniedException:/usr/share/elasticsearch/config/certs/elastic-certificates.p12elk_elasticsearch1.1.jp9k598g34hq@test-1|ElasticsearchSecurityException[failedtoloadSSLconfiguration[xpack.security.transport.ssl]];nested:ElasticsearchException[failedtoinitializeSSLTrustManager-notpermittedtoreadtruststorefile[/usr/share/elasticsearch/config/certs/elastic-certificates.p12]];nested:AccessDeniedException[/usr/share/elasticsearch/config/certs/elastic-certificates.p12];参考自logstash配置参考","link":"https://chriswsq.github.io/post/elk-yong-hu-ren-zheng-wen-ti/"},{"title":"ELK告警之elastalert部署及配置","content":"ELK7版本的日志告警Elastalert配置安装Elastalert环境CentOS：7.6Python：3.6.9pip：19.3elastalert：0.2.0elk：7.7.0配置Python3.6.9环境安装依赖包yum-yinstallwgetopensslopenssl-develgccgcc-c++下载包wgethttps://www.python.org/ftp/python/3.6.9/Python-3.6.9.tgz安装tarxfPython-3.6.9.tgzcdPython-3.6.9./configure--prefix=/usr/local/python--with-opensslmake&amp;&amp;makeinstall配置mv/usr/bin/python/usr/bin/python_oldln-s/usr/local/python/bin/python3/usr/bin/pythonln-s/usr/local/python/bin/pip3/usr/bin/pippipinstall--upgradepip注意,所有依赖python2的脚本或者命令,需要更改为python2.7,因为现在默认的python版本为3.6,例如sed-i'1s/python/python2.7/g'/usr/bin/yumsed-i'1s/python/python2.7/g'/usr/libexec/urlgrabber-ext-down验证$python-VPython3.6.9$pip-Vpip19.3from/usr/local/python/lib/python3.6/site-packages/pip(python3.6).安装elastalert下载包gitclonehttps://github.com/Yelp/elastalert.gitcdelastalert安装pipinstall&quot;elasticsearch&lt;7,&gt;6&quot;pipinstall-rrequirements.txtpythonsetup.pyinstall安装成功后可以看到四个命令ll/usr/local/python/bin/elastalert*/usr/local/python/bin/elastalert/usr/local/python/bin/elastalert-create-index/usr/local/python/bin/elastalert-rule-from-kibana/usr/local/python/bin/elastalert-test-rule软连接到/usr/bin下,方便使用ln-s/usr/local/python/bin/elastalert*/usr/binelastalert报警执行的命令,会根据报警规则执行相应操作。elastalert-create-index会创建一个索引，ElastAlert会把执行记录存放到这个索引中，默认情况下，索引名叫elastalert_status。其中有4个_type，都有自己的@timestamp字段，所以同样也可以用kibana来查看这个索引的日志记录情况。elastalert-rule-from-kibana从Kibana3已保存的仪表盘中读取Filtering设置，帮助生成config.yaml里的配置。不过注意，它只会读取filtering，不包括queries。elastalert-test-rule测试自定义配置中的rule设置。使用官方文档：https://elastalert.readthedocs.io规则文档：https://elastalert.readthedocs.io/en/latest/ruletypes.html主配置文件首先是主配置文件的模板为config.yaml.example,生成全局配置cpconfig.yaml.exampleconfig.yamlvimconfig.yaml#用来加载rule的目录，默认是example_rulesrules_folder:rules#用来设置定时向elasticsearch发送请求，也就是告警执行的频率run_every:seconds:30#用来设置请求里时间字段的范围buffer_time:seconds:30#elasticsearch的host地址,端口es_host:node01es_port:9200es_username:elasticelasticsearch认证用户es_password:d27DuDQjTChIdv3sE8oI#elastalert产生的日志在elasticsearch中的创建的索引writeback_index:elastalert_statuswriteback_alias:elastalert_alerts#失败重试的时间限制alert_time_limit:days:2创建告警索引执行elastalert-create-index命令在ES创建索引，这不是必须的步骤，但是强烈建议创建。因为对于审计和测试很有用，并且重启ES不影响计数和发送alert[root@test-1elastalert]#elastalert-create-indexElasticVersion:7.7.0ReadingElastic6indexmappings:Readingindexmapping'es_mappings/6/silence.json'Readingindexmapping'es_mappings/6/elastalert_status.json'Readingindexmapping'es_mappings/6/elastalert.json'Readingindexmapping'es_mappings/6/past_elastalert.json'Readingindexmapping'es_mappings/6/elastalert_error.json'Newindexelastalert_statuscreatedDone!看到这个输出，就说明创建成功了，也可以请求一下看看：[root@test-1elastalert]#curl-uelastic127.0.0.1:9200/_cat/indices?vEnterhostpasswordforuser'elastic':healthstatusindexuuidprirepdocs.countdocs.deletedstore.sizepri.store.sizegreenopenelastalert_status_statusx-ewAZWNR0-vKhvO1gMCnA1100416b208bgreenopenelastalert_statuswsOzkDHnT-G0TOzMowBuDQ1100416b208bgreenopen.apm-agent-configurationadCDI2foQaybW_GKiexeFg1100416b208bgreenopenelastalert_status_past1f6xlhtEQvqZSj6Tmyi0pw1100416b208bgreenopenelastalert_status_silenceRxi1GdubRdq94cgNShsm_A1100416b208bgreenopen.kibana_2pXVkkt3aS6CxT0k83AM8Ag116916254.5kb127.2kbgreenopenca-db-2020.06.11Mh7Yk1XcQb2T7ACTBzKHbQ1159059.3kb34.9kbgreenopen.kibana_1lCyb8DQqRZ2PxexfMxAmTQ111013.7kb6.8kbgreenopentomcat-2020.06.100M34ANeqSxerM-BYj0-O_A113035.6kb17.8kbgreenopengateway-2020.06.10fKGjgXg-QiCsCRHT6XxuRw1114430808.8kb404.4kbgreenopen.tasksXDOZG-L5TNOeHHCR7hcd4w111012.8kb6.4kbgreenopen.security-7aPEpVA4eSXSZYWowWT_LZw11431158.7kb79.3kbgreenopenca-server-2020.06.11Ue3C6cSvR4-6LoUdEiLI4A1126048.3kb27.3kbgreenopen.apm-custom-linkrvjLGExARVattrRQnFSH2A1100416b208bgreenopen.kibana_task_manager_1amwNW3LbT2eat9fZTAPdyw11515102.7kb56.3kbgreenopenpeer_test-2020.06.109ig45KJcQwy2y4YYFwVKGA1112850920.7kb460.3kbgreenopen.async-searchAcrUB2uUSGueXXr_ueHHzw111119.1kb9.5kbgreenopenelastalert_status_errorwtpRUid1S5CA3Oa6_wAOpg1100416b208bRule配置所有的告警规则，通过在rule目下创建配置文件进行定义，这里简单创建一个来作为演示。首先我已经在elk集群中配置了一个NGINX日志采集的流水线，现在去kibana中利用检索规则，过滤出我想要的告警内容，比如我想让状态码是404的请求，触发告警通知，就用如下语句进行查询：response:404其中group是kafka里边定义的组，后边是状态码，还可以写更多条件进行匹配。然后来到服务器添加一条规则：vimnginx_404.yamlname:Nginx_erruse_strftine_index:trueindex:nginx_info*type:anyaggregation:seconds:10filter:-query:query_string:query:&quot;response:404&quot;alert:-&quot;email&quot;email:-&quot;test@qq.com&quot;smtp_host:smtp.163.comsmtp_port:25smtp_auth_file:/opt/elastalert/smtp_auth_file.yamlfrom_addr:test01@163.comemail_reply_to:teast02@163.com注意里边在配置邮件通知的时候，还需要引用外部的一个文件，这个文件里用于存放对应邮箱的用户名密码。vim/opt/elastalert/smtp_auth_file.yaml15726632807@163.comZSMIKAVFCLTASBHXV规则测试刚刚已经添加了一条规则，现在可以用自身的命令测试一下刚刚添加的规则。elastalert-test-rule--configconfig.yamlnginx_404.yaml","link":"https://chriswsq.github.io/post/elk-gao-jing-zhi-elastalert-bu-shu-ji-pei-zhi/"},{"title":"关于Logstash中grok插件的正则表达式例子","content":"日志收集里最重要的就是logstash对日志的过滤，Logstash里提供了一系列的filter来让我们转换日志。Grok就是这些filters里最重要的一个插件，下面就了解下Grok提供的常用Patterns说明及举例大多数Linux使用人员都有过用正则表达式来查询机器中相关文件或文件里内容的经历，在Grok里，我们也是使用正则表达式来识别日志里的相关数据块。有两种方式来使用正则表达式：直接写正则来匹配用Grok表达式映射正则来匹配常用表达式GREEDYDATA匹配任意字符USERNAME或USER用户名，由数字、大小写及特殊字符(._-)组成的字符串比如：1234、Bob、Alex.Wong等EMAILLOCALPART电子邮件用户名部分，首位由大小写字母组成，其他位由数字、大小写及特殊字符(_.+-=:)组成的字符串。注意，国内的QQ纯数字邮箱账号是无法匹配的，需要修改正则比如：stone、Gary_Lu、abc-123等EMAILADDRESS电子邮件比如：wwwwaf@abc.com、Gadf_adf@gmail.com、abc-123@163.com等HTTPDUSERApache服务器的用户，可以是EMAILADDRESS或USERNAMEINT整数，包括0和正负整数比如：0、-123、43987等BASE10NUM或NUMBER十进制数字，包括整数和小数比如：0、18、5.23等BASE16NUM十六进制数字，整数比如：0x0045fa2d、-0x3F8709等WORD字符串，包括数字和大小写字母比如：String、3529345、ILoveYou等NOTSPACE不带任何空格的字符串SPACE空格字符串{SPACE}***匹配任意连续空格QUOTEDSTRING或QS带引号的字符串比如：&quot;Thisisanapple&quot;、'Whatisyourname?'等UUID标准UUID比如：550E8400-E29B-11D4-A716-446655440000MACMAC地址，可以是Cisco设备里的MAC地址，也可以是通用或者Windows系统的MAC地址IPIP地址，IPv4或IPv6地址比如：127.0.0.1、FE80:0000:0000:0000:AAAA:0000:00C2:0002等HOSTNAME主机名称IPORHOSTIP或者主机名称HOSTPORT主机名(IP)+端口比如：127.0.0.1:3306、api.stozen.net:8000等PATH路径，Unix系统或者Windows系统里的路径格式比如：/usr/local/nginx/sbin/nginx、c:\\windows\\system32\\clr.exe等URIPROTOURI协议比如：http、ftp等URIHOSTURI主机比如：www.baidu.net、10.0.0.1:22等URIPATHURI路径比如：//www.baidu.net/abc/、/api.php等URIPARAMURI里的GET参数比如：?a=1&amp;b=2&amp;c=3URIPATHPARAMURI路径+GET参数比如：//www.google.net/abc/api.php?a=1&amp;b=2&amp;c=3URI完整的URI比如：http://www.google.net/abc/api.php?a=1&amp;b=2&amp;c=3日期时间表达式MONTH月份名称比如：Jan、January等MONTHNUM月份数字比如：03、9、12等MONTHDAY日期数字比如：03、9、31等DAY星期几名称比如：Mon、Monday等YEAR年份数字HOUR小时数字MINUTE分钟数字SECOND秒数字TIME时间比如：00:01:23DATE_US美国日期格式比如：10-15-1982、10/15/1982等DATE_EU欧洲日期格式比如：15-10-1982、15/10/1982、15.10.1982等ISO8601_TIMEZONEISO8601时间格式比如：+10:23、-1023等TIMESTAMP_ISO8601ISO8601时间戳格式比如：2016-07-03T00:34:06+08:00DATE日期，美国日期%{DATE_US}或者欧洲日期%{DATE_EU}DATESTAMP完整日期+时间比如：07-03-201600:34:06HTTPDATEhttp默认日期格式比如：03/Jul/2016:00:36:53+0800Log表达式LOGLEVEL日志等级比如：Alert、alert、ALERT、Error等创建自己的Grok表达式在业务领域中，可能会有越来越多的日志格式出现在我们眼前，而Grok的默认表达式显然已无法满足我们的需求（比如用户身份证号、手机号等信息），所以，我们需要自己动手添加些表达式。表达式正则表达式说明DATE_CHS%{YEAR}[./-]%{MONTHNUM}[./-]%{MONTHDAY}中国人习惯的日期格式ZIPCODE_CHS[1-9]\\d{5}国内邮政编码GAME_ACCOUNT[a-zA-Z][a-zA-Z0-9_]{4,15}游戏账号，首字符为字母，4-15位字母、数字、下划线组成其他grok正则表达式：(?(.)(?=Report)/?)获取Report之前的字符grok正则表达式：(?(?=Report)(.)/?)获取Report之后的字符参考","link":"https://chriswsq.github.io/post/guan-yu-logstash-zhong-grok-cha-jian-de-zheng-ze-biao-da-shi-li-zi/"},{"title":"生产环境部署fabric","content":"环境准备ansible临时添加主机信息修改服务器密码dockerdocker-compose挂载数据盘传送镜像文件ansible临时添加主机信息因为后续需要根据不同节点传递不同的镜像文件，所以先临时添加节点信息。例如：龙岩节点longyan[longyan]ly1ansible_host=121.204.20.15ly2ansible_host=121.204.220.15ly3ansible_host=121.204.20.14ly4ansible_host=121.204.20.15ly5ansible_host=121.204.20.11ly6ansible_host=121.204.20.1ly7ansible_host=121.204.22.14ly8ansible_host=121.204.20.14ly9ansible_host=121.204.20.14ly10ansible_host=121.204.220.1[fz:vars]ansible_ssh_port=650ansible_ssh_pass='123456'修改服务器密码/application/ansible-playbook/optimization/#推送ansible服务器秘钥ansible-playbook-ehost=fzpush-ssh.yaml#更改服务器密码ansible-playbook-ehost=fzchangepw.yaml安装docker安装docker#安装依赖ansiblelongyan-mshell-a&quot;yuminstall-yyum-utils;yum-config-manager--add-repohttps://download.docker.com/linux/centos/docker-ce.repo&quot;#从ansible主机传送docker-ce-18.03.1.ce-1.el7.centos.x86_64.rpm文件ansiblelongyan-mcopy-a&quot;src=/home/ansible-playbook/optimization/common/docker-ce-18.03.1.ce-1.el7.centos.x86_64.rpmdest=/root/&quot;#安装ansiblelongyan-mshell-a&quot;yum-yinstalldocker-ce-18.03.1.ce-1.el7.centos.x86_64.rpm&quot;拷贝daemon.json文件/application/ansible-playbook/localnode/fabirc/peer/install-peer/template/daemon.json安装docker-compose可在传送镜像是安装也可执行下面命令安装curl-Lhttps://get.daocloud.io/docker/compose/releases/download/1.12.0/docker-compose-`uname-s`-`uname-m`&gt;./docker-composemv./docker-compose/usr/local/bin/docker-composechmod+x/usr/local/bin/docker-composedocker-compose-v挂载数据盘#查看数据盘fdisk-l#磁盘分区fdisk/dev/vdbnp回车回车回车w#格式化磁盘mkfs.xfs-nftype=1/dev/vdb1#添加永久挂载echo'/dev/vdb1/bsnxfsdefaults00'&gt;&gt;/etc/fstab#创建挂在目录mkdir/bsn#初始化挂载mount-a#查看挂在情况df-hT传送镜像文件根据节点不同的服务类型来传送不同的镜像传送服务节点的镜像使用ansible服务器上/application/ansible-playbook/optimization/copy.yaml文件进行传送-hosts:&quot;{{host}}&quot;remote_user:roottasks:#-name:安装zip#yum:name=unzipstate=latest-name:创建镜像目录/bsn/isofile:path:/bsn/isostate:directorymode:'0755'-name:传镜像copy:src:/application/ansible-playbook/common/iso/{{item}}dest:/bsn/isowith_items:#-bsnCa-tomcat9.zip#-zipkin.zip-docker-compose-fabric-peer1.4.3.1.zip-fabric-tools1.4.3.1.zip-fabric-couchdb.zip-fabric-ccenv.zip-docker-ce-18.03.1.ce-1.el7.centos.x86_64.rpm-fabric-ca-postgresql.zip-fabric-ca-service.zip-webase-mysql.zip-fabric-gateway.zip-fisco-gm-0.3.zip-webase-mysql.zip-webasechainmanager.zip-webasesign.zip-gateway-service-v4.zip-zipkin.zip-mcs.zip-name:解压shell:cd/bsn/iso&amp;&amp;unzip\\*.zipignore_errors:yes#根据节点所需镜像进行传送，可先将不需要的注释掉peer节点-fabric-peer1.4.3.1.zip-fabric-tools1.4.3.1.zip-fabric-couchdb.zip-fabric-ccenv.zipca节点-fabric-ca-postgresql.zip-fabric-ca-service.zip-webase-mysql.zip网关节点-fabric-gateway.zip-rabbitmq.zip-consul.zipfisco节点-fisco-gm-0.3.zipfisco签名服务链管理-webase-mysql.zip-webasechainmanager.zip-webasesign.zip微服务节点-zipkin.zip-mcs.zippeer节点（z1z2z3）ansible-playbook-ehost=fzcopy.yaml-lz1,z2,z3#保留copy.yaml文件中四个文件，其他注释ca节点（z6）ansible-playbook-ehost=fzcopy.yaml-lz6#保留copy.yaml文件中三个文件，其他注释网关节点（z4）ansible-playbook-ehost=fzcopy.yaml-lz4#保留copy.yaml文件中三个文件，其他注释注：因镜像文件较大，传送时间较长，需耐心等候传送镜像服务结束之后将临时创建的变量文件移除掉（一般移除至/mnt下）部署peer服务修改peer变量文件/etc/ansible/inventory/peer1peer2peer3两个方面调整，添加主机组、主机组添加至子组（例如：peer1）#添加主机组[fuzhou_peer1]fuzhou_peer1ansible_host=121.204.220.16ansible_ssh_port=22[fuzhou_peer1:vars]peer=peer1name=fuzhouMSP=Fuzhounode1=172.31.0.3node2=172.31.0.1node3=172.31.0.2cjq=172.31.0.2#添加子组[peer1:children]suzhou_peer1上传peer相关文件fuzhou-peer1.zipfuzhou-peer2.zipfuzhou-peer3.zip部署peer123执行目录：/application/ansible-playbook/localnode/fabirc/peer/install-peeransible-playbook-ehost=peer1install-peer1.yaml-lfuzhou_peer1ansible-playbook-ehost=peer2install-peer1.yaml-lfuzhou_peer2ansible-playbook-ehost=peer3install-peer1.yaml-lfuzhou_peer3部署ca服务修改ca变量文件/etc/ansible/inventory/ca两个方面调整，添加主机组、主机组添加至子组#添加群组[fuzhou_ca]fuzhou_caansible_host=121.204.220.14ansible_ssh_port=22name=fuzhou#添加子组[ca:children]fuzhou_ca加入链码部分会报错，待采集器部署完后之后手动加入链码并安装（命令可见playbook文件底部）上传ca相关文件fuzhou-ca.zip部署ca-mysql执行目录/application/ansible-playbook/localnode/fabirc/ca/install-mysqlansible-playbook-ehost=cainstall-mysql.yaml-lfuzhou_ca部署ca-service执行目录：/application/ansible-playbook/localnode/fabirc/ca/install-caansible-playbook-ehost=cainstall-service.yaml-lfuzhou_ca检查状态查看数据库表状态dockerexeccjq-gate-mysqlmysql-uroot-pRed80@80-e&quot;usebsnflowdb;select*fromtb_nc_config&quot;部署gateway服务修改ca变量文件/etc/ansible/inventory/gateway_M两个方面调整，添加主机组、主机组添加至子组#添加主机组[fuzhou_gateway_M]fuzhou_gateway_Mansible_host=121.204.220.10ansible_ssh_port=22[fuzhou_gateway_M:vars]name=fuzhouMSP=FuzhoutitleName=福州code=ORG20200804#根据当天日志填写即可node1=172.31.0.31#三个peer节点的内网地址node2=172.31.0.23node3=172.31.0.26sign=1.1.1.1#没有fisco填写1.1.1.1ca=172.31.0.24mcs=1.1.1.1#没有微服务填写1.1.1.1#添加子组[gateway_M:children]nanping_gateway_M上传gateway相关文件fuzhou-certs.zip部署gateway执行目录：/application/ansible-playbook/localnode/fabirc/gateway/install-gatewayansible-playbook-ehost=gateway_Minstall.yaml-lfuzhou_gateway_M部署采集器服务修改cjq变量文件[fuzhou_cjq]fuzhou_cjqansible_host=121.204.220.18ansible_ssh_port=22[fuzhou_cjq:vars]name=fuzhouMSP=FuzhoutitleName=福州code=ORG20200804node1=172.31.0.31node2=172.31.0.23node3=172.31.0.26ca=172.31.0.24peerport=22fiscocron=false#没有fisco则填写falsefiscosign=1.1.1.1#没有fisco则填写1.1.1.1xupercron=false#没有xupercron则填写falsemcs=1.1.1.1#没有微服务则填写1.1.1.1gmfabriccron=false#没有微服务则填写false上传采集器相关文件fuzhou-collect.zip部署CJQ执行目录：/application/ansible-playbook/localnode/fabirc/cjq/install-cjqansible-playbook-ehost=cjqcjq.yaml-lfuzhou_cjq查看服务日志/bsn/go/src/node-collect/logstail-100f/bsn/go/src/node-collect/logs/catalina.out如果出现以下报错，则需要重新加入链，安装链2020-08-0413:57:55.381ERROR4179---[ost-startStop-1]c.b.n.u.C.BaseConfigUtils$Companion:/bsn/go/src/node-collect/webapps/nodecollectConf/conf/xuperChainBlockResult.properties(Nosuchfileordirectory)2020-08-0413:57:55.487INFO4179---[ost-startStop-1]c.b.n.utils.NetUtils$Companion:请求地址：http://1.1.1.1:5005/WeBASE-Chain-Manager/chain/all2020-08-0414:00:02.770INFO4179---[ost-startStop-1]c.b.n.utils.NetUtils$Companion:处理异常：Connectiontimedout(Connectiontimedout),附加信息：fisco获取所有链编号2020-08-0414:00:02.794ERROR4179---[ost-startStop-1]c.b.n.u.C.BaseConfigUtils$Companion:/bsn/go/src/node-collect/webapps/nodecollectConf/conf/sm2FabricBlockResult.properties(Nosuchfileordirectory)2020-08-0414:00:02.830INFO4179---[ost-startStop-1]c.b.nodecollector.task.FiscoTimeTask:fiscofbTimetaskopenorclose==false2020-08-0414:00:02.831INFO4179---[ost-startStop-1]c.b.nodecollector.task.FiscoTimeTask:fiscochannelsUploadTime==12000002020-08-0414:00:02.831INFO4179---[ost-startStop-1]c.b.nodecollector.task.FiscoTimeTask:fiscoblocksUploadTime==12000002020-08-0414:00:02.831INFO4179---[ost-startStop-1]c.b.nodecollector.task.FiscoTimeTask:fiscopeerResourceUploadTime==600000在peer的3个节点执行#加入链码dockerexecclipeerchanneljoin-bblocktx/netchannel.block#安装链码dockerexecclipeerchaincodeinstallblocktx/cc_sys.3.0.pak","link":"https://chriswsq.github.io/post/sheng-chan-huan-jing-bu-shu-fabric/"},{"title":"Gridea添加Valine评论系统","content":"给自己的博客添加评论系统参考：https://kveln.cn/post/qE678A4ce/注意点：修改主题模板在post.ejs文章详情页模板中文章末尾的位置添加下面的代码&lt;!--载入js，在&lt;/body&gt;之前插入即可--&gt;&lt;!--Leancloud操作库:--&gt;&lt;scriptsrc=&quot;//cdn1.lncld.net/static/js/3.0.4/av-min.js&quot;&gt;&lt;/script&gt;&lt;!--Valine的核心代码库--&gt;&lt;scriptsrc=&quot;/media/scripts/Valine.min.js&quot;&gt;&lt;/script&gt;&lt;divclass=&quot;comment&quot;&gt;&lt;/div&gt;&lt;script&gt;newValine({//AV对象来自上面引入av-min.js(老司机们不要开车➳♡゛扎心了老铁)av:AV,el:'.comment',lang:'zh-cn',//设置评论语言emoticon_url:'https://cloud.panjunwen.com/alu',emoticon_list:[&quot;狂汗.png&quot;,&quot;不说话.png&quot;,&quot;汗.png&quot;,&quot;坐等.png&quot;,&quot;献花.png&quot;,&quot;不高兴.png&quot;,&quot;中刀.png&quot;,&quot;害羞.png&quot;,&quot;皱眉.png&quot;,&quot;小眼睛.png&quot;,&quot;暗地观察.png&quot;],app_id:'你的id',app_key:'你的key',placeholder:'评论留言'});&lt;/script&gt;添加位置添加到&lt;!DOCTYPEhtml&gt;&lt;html&gt;&lt;head&gt;&lt;%-include('./_blocks/head',{siteTitle:`${post.title}|${themeConfig.siteName}`})%&gt;&lt;linkrel=&quot;canonical&quot;href=&quot;&lt;%=post.link%&gt;&quot;/&gt;&lt;/head&gt;&lt;body&gt;&lt;%-include('./_blocks/header')%&gt;&lt;divclass=&quot;main&quot;&gt;&lt;divclass=&quot;main-inner&quot;&gt;&lt;divclass=&quot;content&quot;&gt;&lt;articleclass=&quot;post&quot;&gt;&lt;h2class=&quot;post_titlesm_margin&quot;&gt;&lt;a&gt;&lt;%=post.title%&gt;&lt;/a&gt;&lt;/h2&gt;&lt;script&gt;functionlan(){if(document.getElementById(&quot;lan&quot;).innerText==&quot;繁&quot;){vars=document.getElementById(&quot;tongwenlet_cn&quot;);if(s!=null){document.body.removeChild(s)}vars=document.createElement(&quot;script&quot;);s.language=&quot;javascript&quot;;s.type=&quot;text/javascript&quot;;s.src=&quot;https://cdn.jsdelivr.net/gh/qyxtim/Static@1.1/bookmarklet_tw.js&quot;;s.id=&quot;tongwenlet_cn&quot;;document.body.appendChild(s);document.getElementById(&quot;lan&quot;).innerHTML=&quot;简&quot;}else{if(document.getElementById(&quot;lan&quot;).innerText==&quot;簡&quot;){vars=document.getElementById(&quot;tongwenlet_cn&quot;);if(s!=null){document.body.removeChild(s)}vars=document.createElement(&quot;script&quot;);s.language=&quot;javascript&quot;;s.type=&quot;text/javascript&quot;;s.src=&quot;https://cdn.jsdelivr.net/gh/qyxtim/Static@1.1/bookmarklet_cn.js&quot;;s.id=&quot;tongwenlet_cn&quot;;document.body.appendChild(s);document.getElementById(&quot;lan&quot;).innerHTML=&quot;繁&quot;}}};&lt;/script&gt;&lt;sectionclass=&quot;post_details&quot;&gt;&lt;iclass=&quot;iconfonticon-calendar&quot;&gt;&lt;/i&gt;&lt;spanstyle=&quot;margin-right:15px&quot;&gt;&lt;%=post.dateFormat%&gt;&lt;/span&gt;&lt;iclass=&quot;iconfonticon-browse&quot;&gt;&lt;/i&gt;&lt;spanstyle=&quot;margin-right:15px&quot;&gt;&lt;spanid=&quot;busuanzi_value_page_pv&quot;&gt;&lt;/span&gt;Views&lt;/span&gt;&lt;iclass=&quot;iconfonticon-category&quot;&gt;&lt;/i&gt;&lt;spanclass=&quot;weaklink&quot;style=&quot;margin-right:15px&quot;&gt;&lt;%post.tags.forEach(function(tag,index){%&gt;&lt;ahref=&quot;&lt;%=tag.link%&gt;&quot;class=&quot;tag&quot;&gt;&lt;%=tag.name%&gt;&lt;/a&gt;&lt;%if(index!==post.tags.length-1){%&gt;|&lt;%}%&gt;&lt;%});%&gt;&lt;/span&gt;&lt;iclass=&quot;iconfonticon-caret-down&quot;&gt;&lt;/i&gt;&lt;spanstyle=&quot;margin-right:15px&quot;&gt;&lt;%=post.stats.words%&gt;字&lt;/span&gt;&lt;iclass=&quot;iconfonticon-naozhong&quot;&gt;&lt;/i&gt;&lt;spanstyle=&quot;margin-right:15px&quot;&gt;&lt;%=post.stats.text%&gt;&lt;/span&gt;&lt;aid=&quot;lan&quot;href=&quot;javascript:void(0);&quot;onclick=&quot;lan();&quot;title=&quot;调整简繁体&quot;style=&quot;margin-right:15px;&quot;&gt;繁&lt;/a&gt;&lt;%if(post.isTop){%&gt;&lt;fontcolor=7D26CD&gt;置顶&lt;/font&gt;&lt;%}%&gt;&lt;/section&gt;&lt;%if(themeConfig.showFeatureImage&amp;&amp;post.feature){%&gt;&lt;imgclass=&quot;featureImg&quot;alt=&quot;featureimg&quot;src=&quot;&lt;%=post.feature%&gt;&quot;referrerpolicy=&quot;no-referrer&quot;&gt;&lt;%}%&gt;&lt;divstyle=&quot;display:flex&quot;&gt;&lt;divclass=&quot;md_block&quot;id=&quot;md_block&quot;&gt;&lt;divclass=&quot;round-shape-one&quot;&gt;&lt;/div&gt;&lt;%-post.content%&gt;&lt;spanid=&quot;footnote&quot;&gt;&lt;/span&gt;&lt;divid=&quot;warn&quot;&gt;&lt;/div&gt;&lt;/div&gt;&lt;divclass=&quot;toc-container&quot;&gt;&lt;%-post.toc%&gt;&lt;/div&gt;&lt;/div&gt;&lt;divid=&quot;fullPage&quot;&gt;&lt;canvasid=&quot;canvas&quot;&gt;&lt;/canvas&gt;&lt;/div&gt;&lt;/article&gt;&lt;divid=&quot;eof&quot;&gt;&lt;span&gt;EOF&lt;/span&gt;&lt;/div&gt;&lt;divclass=&quot;round-shape-one&quot;&gt;&lt;/div&gt;&lt;section&gt;&lt;divclass=&quot;doc_comments&quot;&gt;&lt;!--载入js，在&lt;/body&gt;之前插入即可--&gt;&lt;!--Leancloud操作库:--&gt;&lt;scriptsrc=&quot;//cdn1.lncld.net/static/js/3.0.4/av-min.js&quot;&gt;&lt;/script&gt;&lt;!--Valine的核心代码库--&gt;&lt;scriptsrc=&quot;/media/scripts/Valine.min.js&quot;&gt;&lt;/script&gt;&lt;divclass=&quot;comment&quot;&gt;&lt;/div&gt;&lt;script&gt;newValine({//AV对象来自上面引入av-min.js(老司机们不要开车➳♡゛扎心了老铁)av:AV,el:'.comment',lang:'zh-cn',//设置评论语言emoticon_url:'https://cloud.panjunwen.com/alu',emoticon_list:[&quot;狂汗.png&quot;,&quot;不说话.png&quot;,&quot;汗.png&quot;,&quot;坐等.png&quot;,&quot;献花.png&quot;,&quot;不高兴.png&quot;,&quot;中刀.png&quot;,&quot;害羞.png&quot;,&quot;皱眉.png&quot;,&quot;小眼睛.png&quot;,&quot;暗地观察.png&quot;],app_id:'nCwKjHbRmhxBO6Dl20UfM5yw-gzGzoHsz',app_key:'DiIo4l1dsGeJ9VKtPoGOjfN5',placeholder:'唠两块钱儿'});&lt;/script&gt;&lt;%if(typeofcommentSetting!=='undefined'&amp;&amp;commentSetting.showComment){%&gt;&lt;%if(commentSetting.commentPlatform==='gitalk'){%&gt;&lt;divid=&quot;gitalk-container&quot;&gt;&lt;/div&gt;&lt;%-include('./_blocks/gitalk')%&gt;&lt;%}%&gt;&lt;%if(commentSetting.commentPlatform==='disqus'){%&gt;&lt;%-include('./_blocks/disqus')%&gt;&lt;%}%&gt;&lt;%}%&gt;&lt;/div&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;script&gt;&quot;usestrict&quot;;!function(){for(varn=document.getElementsByTagName(&quot;pre&quot;),e=n.length,s=0;s&lt;e;s++){n[s].innerHTML='&lt;spanclass=&quot;line-number&quot;&gt;&lt;/span&gt;'+n[s].innerHTML+'&lt;spanclass=&quot;cl&quot;&gt;&lt;/span&gt;';for(vara=n[s].innerHTML.split(/\\n/).length,r=0;r&lt;a-1;r++){n[s].getElementsByTagName(&quot;span&quot;)[0].innerHTML+=&quot;&lt;span&gt;&quot;+(r+1)+&quot;&lt;/span&gt;&quot;}}}();letmainNavLinks=document.querySelectorAll(&quot;.markdownIt-TOCa&quot;);window.addEventListener(&quot;scroll&quot;,event=&gt;{letfromTop=window.scrollY;mainNavLinks.forEach((link,index)=&gt;{letsection=document.getElementById(decodeURI(link.hash).substring(1));letnextSection=nullif(mainNavLinks[index+1]){nextSection=document.getElementById(decodeURI(mainNavLinks[index+1].hash).substring(1));}if(section.offsetTop&lt;=fromTop){if(nextSection){if(nextSection.offsetTop&gt;fromTop){link.classList.add(&quot;currentToc&quot;);}else{link.classList.remove(&quot;currentToc&quot;);}}else{link.classList.add(&quot;currentToc&quot;);}}else{link.classList.remove(&quot;currentToc&quot;);}});});varlist=document.querySelectorAll(&quot;.katex&quot;);for(vari=0;i&lt;list.length;i++){list[i].style.display=&quot;unset&quot;};varh=document.documentElement,b=document.body,st=&quot;scrollTop&quot;,sh=&quot;scrollHeight&quot;,progress=document.querySelector(&quot;.progress&quot;),scroll;document.addEventListener(&quot;scroll&quot;,function(){scroll=(h[st]||b[st])/((h[sh]||b[sh])-h.clientHeight)*100;progress.style.setProperty(&quot;--scroll&quot;,scroll+&quot;%&quot;)});varwxScale=newWxScale({fullPage:document.querySelector(&quot;#fullPage&quot;),canvas:document.querySelector(&quot;#canvas&quot;)});varimgBox=document.querySelectorAll(&quot;#md_blockimg&quot;);for(vari=0;i&lt;imgBox.length;i++){imgBox[i].onclick=function(e){wxScale.start(this)}};content=&quot;本文最后更新于&lt;%=post.dateFormat%&gt;，已超过1年没有更新，涉及的内容可能已经失效！&quot;;vardate1=&quot;&lt;%=post.date%&gt;&quot;;date1=date1.replace(&quot;-&quot;,&quot;/&quot;);vardate2=newDate();vardate3=date2.getTime()-newDate(date1).getTime();vardays=Math.floor(date3/(24*3600*1000));if(days&gt;=365){document.getElementById(&quot;warn&quot;).innerHTML=content};&lt;/script&gt;&lt;style&gt;#magnifyImg{position:fixed;left:0;top:0;text-align:center;width:100%;display:none;z-index:9999}#magnifyImgimg{object-fit:contain;background:#eaecef;padding:15px;border-radius:10px;height:auto;width:auto;vertical-align:middle}&lt;/style&gt;&lt;%-include('./_blocks/footer')%&gt;&lt;scripttype=&quot;text/javascript&quot;asyncsrc=&quot;&lt;%=themeConfig.domain%&gt;/media/js/prism.js&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;","link":"https://chriswsq.github.io/post/gridea-tian-jia-valine-ping-lun-xi-tong/"},{"title":"cita服务","content":"根据公司需求研究部署cita服务关于CITACITA是一个面向企业级应用的支持智能合约的高性能区块链内核，旨在为企业级区块链应用提供一个稳固、高效、灵活、可适应未来的运行平台。CITA将区块链节点的必要功能解耦为六个微服务：RPC，Auth，Consensus，Chain，Executor，Network。各组件之间通过消息总线交换信息相互协作。通过配置和定制相应的服务，CITA能够满足企业级用户的全部需要。基础环境docker、docker-compose部署方式docker-compose镜像版本cita/cita-release:20.2.0-secp256k1-sha3注意必选配置项有super_admin和nodes，系统不提供默认配置。安装CITA客户端工具创建目录mkdir-p/bsn/cita2.切换目录cd/bsn/cita3.下载CITA-CLI安装包wgethttps://github.com/citahub/cita-cli/releases/download/20.2.2/cita-cli-x86_64-musl-tls-20.2.2.tar.gz解压程序tarzxvfcita-cli-x86_64-musl-tls-20.2.2.tar.gz5.复制CITA-CLI到系统可执行文件目录下cp-rpcita-cli/usr/local/bin/docker-compose部署cita服务1.生成节点配置文件docker-compose-conf.ymlversion:'3'networks:main:services:config:container_name:configenvironment:-SUPER_ADMIN=0x4b5ae4567ad5d9fb92bc9afd6a657e6fa13a2523-NODES_CONFIG=192.168.40.6:4000,192.168.40.7:4000,192.168.40.8image:cita/cita-release:20.2.0-secp256k1-sha3hostname:confignetworks:main:aliases:-configvolumes:-./:/opt/cita-runcommand:|bash-c'echo&quot;Createconfigfiles...&quot;;citacreate--super_admin&quot;$$SUPER_ADMIN&quot;--nodes&quot;$$NODES_CONFIG&quot;;echo&quot;Doneconfig&quot;'dockercreatenetworkmaindocker-compose-fdocker-compose-conf.yml创建3个共识节点的目录结构如下:$lstest-chain/012template$ls0addressconsensus.tomlforever.tomllogsauth.tomldatagenesis.jsonnetwork.tomlchain.tomlexecutor.tomljsonrpc.tomlprivkey将配置文件拷贝到其他节点privkey:存放私钥address:存放地址*.toml:各个微服务配置文件，详细说明见微服务说明genesis.json：生成genesis块文件，其中timestamp为时间戳，秒为单位；prevhash-指前一个块哈希，这里是默认值；而alloc指部署到创世块的合约内容；test-chain/template目录下是模板文件，包括这个链的共识节点地址test-chain/template/-authorities.list，系统参数test-chain/template/init_data.yml,节点端口地址test-chain/template/nodes.list等信息logs:记录链运行的日志信息data:数据存储CITA有一些保留端口，设置节点网络端口，或者自定义端口的时候要避免产生端口冲突。保留端口有：默认的jsonrpc端口：1337到1337+N默认的websocket端口：4337到4337+N因为我们是不同的机器所以不用担心端口冲突的问题，可以修改jsonrpc.toml文件上面两个端口为一致2.节点部署文件node-1节点文件docker-compose-node0.ymlversion:'3'networks:main:services:node0:container_name:node0image:cita/cita-release:20.2.0-secp256k1-sha3hostname:node0networks:main:aliases:-node0volumes:-./:/opt/cita-runports:-&quot;1337:1337&quot;command:|bash-c'while[[!-d/opt/cita-run/test-chain]];dosleep1;done;sleep10;citasetuptest-chain/0;citastarttest-chain/0;sleepinfinity'node-2节点文件docker-compose-node1.ymlversion:'3'networks:main:services:node1:container_name:node1image:cita/cita-release:20.2.0-secp256k1-sha3hostname:node1networks:main:aliases:-node1volumes:-./:/opt/cita-runports:-&quot;1337:1337&quot;command:|bash-c'while[[!-d/opt/cita-run/test-chain]];dosleep1;done;sleep10;citasetuptest-chain/1;citastarttest-chain/1;sleepinfinity'node-2节点文件docker-compose-node2.yml...3.启动服务docker-compose-node0.ymldocker-compose-node1.ymldocker-compose-node2.yml验证CITA是否运行正常进入容器执行命令检查服务状态dockerexec-itnode0bashroot@node0:/opt/cita-run#citatoptest-chain/0root204210Aug25?00:00:00cita-foreverroot205120420Aug25?00:05:51cita-auth-cauth.tomlroot206920420Aug25?00:02:10cita-bft-cconsensus.toml-pprivkeyroot206120420Aug25?00:00:24cita-chain-cchain.tomlroot205420420Aug25?00:01:50cita-executor-cexecutor.tomlroot206220420Aug25?00:00:55cita-jsonrpc-cjsonrpc.tomlroot205320420Aug25?00:05:40cita-network-cnetwork.toml检查7个服务是否都已经启动。分别去其他节点依次检查检查日志有无报错情况cita-network.log日志会有ERROR-[NodeManager]Cannotgetnodestatusfromknown_addr,thisshould·nothappen!忽略即可","link":"https://chriswsq.github.io/post/cita-fu-wu/"},{"title":"Kubernetes 基礎教學（三）Helm 介紹與建立 Chart","content":"接下來在這篇文章中，我們會介紹一個建立Kubernetes應用變得輕鬆簡單的好幫手：HelmHelm在如何建立一個Pod以及Kubernetes進階三元件我們介紹了多個Kubernetes的元件與他們所對應到的yaml設定檔。假設我們今天有一個複雜的服務，裡面同時包含了很多種設定檔時，如何同時做好版本控制、管理、更新這些設定檔就變得不太容易，且要快速部署這個含有多個設定檔的服務也變得困難。因此Helm就是一個用來解決上述問題的工具。簡單來說，Helm就是一個管理設定檔的工具。他會把Kubernetes一個服務中各種元件裡的yaml檔統一打包成一個叫做chart的集合，然後透過給參數的方式，去同時管理與設定這些yaml檔案。使用一個現有HelmChart接下來我們要來示範用一個現有的HelmChart來嘗試部署一個Wordpress的服務。首先，我們的第一步當然就是要下載Helm。MacOS中我們可以直接使用Homebrew安裝，其他環境可以參考Helm的Github。brewinstallkubernetes-helm下載完後，我們記得要Helm把Cluster配置初始化helminit接下來讓我們安裝Wordpress的Chart，我們可以直接透過指令helminstallstable/wordpress這個指令會讓我們直接到ChartRepository去載入Chart檔並將它部署到我們的KubernetesCluster上，我們現在可以透過指令檢查我們的ClusterkubectlgetallNAMEREADYSTATUSRESTARTSpod/peddling-hog-mariadb-01/1Running0pod/peddling-hog-wordpress-7bf6d69c8b1/1Running1​NAMETYPECLUSTER-IPservice/peddling-hog-mariadbClusterIP10.109.96.113service/peddling-hog-wordpressLoadBalancer10.101.157.184EXTERNAL-IPPORT(S)&lt;none&gt;3306/TCP&lt;pending&gt;80:30439/TCP,443:31824/TCP​NAMEREADYUP-TO-DATEAVAILABLEdeployment.apps/peddling-hog-wordpress1/111​NAMEDESIREDreplicaset.apps/peddling-hog-wordpress-7bf6d69c8b1​NAMEREADYAGEstatefulset.apps/peddling-hog-mariadb1/160s可以看到我們透過Chart一次就安裝與部署了兩個Pod、兩個Service以及其他各種元件。如果要一次把所有Chart所安裝的元件刪除，我們可以先透過helmlist列出我們所有的Chart。NAMEREVISIONUPDATEDSTATUSCHARTpeddling-hog1FriApr2616:08:302019DEPLOYEDwordpress然後輸入helmdeletepeddling-hog就可以一次把所有元件刪除。Chart的運作方式嘗試完從Chart部署元件後，我們可以進一步來暸解Chart是如何運作的。我們可以到Wordpresschart的Github上觀察這個Chart的檔案結構，或是透過指令來建立一個最簡單的Charthelmcreatehelm-demo接下來我們來看看./helm-demo的資料夾.├──Chart.yaml├──charts├──templates│├──deployment.yaml│├──ingress.yaml│└──service.yaml└──values.yaml把這個Chart的檔案結構化簡後就如上所見。Chart.yaml定義了這個Chart的Metadata，包括Chart的版本、名稱、敘述等charts在這個資料夾裡可以放其他的Chart，這裡稱作SubChatemplates定義這個Chart服務需要的Kubernetes元件。但我們並不會把各元件的參數寫死在裡面，而是會用參數的方式代入values.yaml定義這個Chart的所有參數，這些參數都會被代入在templates中的元件。例如我們會在這邊定義nodePorts給service.yaml、定義replicaCount給deployment.yaml、定義hosts給ingress.yaml等等從上面的檔案結構可以看到，我們透過編輯values.yaml，就可以對所有的yaml設定檔做到版本控制與管理。並透過install/delete的方式一鍵部署/刪除。如何建立自己的Chart了解了Chart大致上是如何運作後，我們就可以來實際建立一個簡單的Chart。我們的目標是要透過deployment、service、ingress來讓使用者在輸入blue.demo.com時可以得到一隻小鯨魚。而首先，我們一樣輸入指令helmcreatehelm-demo之後我們就先借看一下在ingress章節有使用過的yaml檔們。deployment.yamlapiVersion:extensions/v1beta1kind:Deploymentmetadata:name:blue-nginxspec:replicas:2template:metadata:labels:app:blue-nginxspec:containers:-name:nginximage:hcwxd/blue-whaleports:-containerPort:3000service.yamlapiVersion:v1kind:Servicemetadata:name:blue-servicespec:type:NodePortselector:app:blue-nginxports:-protocol:TCPport:80targetPort:3000ingress.yamlapiVersion:extensions/v1beta1kind:Ingressmetadata:name:webspec:rules:-host:blue.demo.comhttp:paths:-backend:serviceName:blue-serviceservicePort:80然後我們就可以嘗試來把上述yaml檔中可以作為參數的部分抽取出來，在這邊為了降低複雜度，我們只簡單挑幾個參數出來，然後我們就可以把這些參數寫到values.yaml中。values.yamlreplicaCount:2​image:repository:hcwxd/blue-whale​service:type:NodePortport:80​ingress:enabled:true​hosts:-host:blue.demo.compaths:[/]把參數提取出來後，我們就來依樣畫葫蘆地把template中其他三個yaml檔寫成可以接受參數的方式：deployment.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:{{include&quot;value-helm-demo.fullname&quot;.}}spec:replicas:{{.Values.replicaCount}}selector:matchLabels:app:{{include&quot;value-helm-demo.fullname&quot;.}}template:metadata:labels:app:{{include&quot;value-helm-demo.fullname&quot;.}}spec:containers:-name:{{.Chart.Name}}image:'{{.Values.image.repository}}'ports:-containerPort:3000service.yamlapiVersion:v1kind:Servicemetadata:name:{{include&quot;value-helm-demo.fullname&quot;.}}spec:type:{{.Values.service.type}}ports:-port:{{.Values.service.port}}targetPort:3000protocol:TCPselector:app:{{include&quot;value-helm-demo.fullname&quot;.}}ingress.yaml{{-if.Values.ingress.enabled-}}{{-$fullName:=include&quot;value-helm-demo.fullname&quot;.-}}apiVersion:extensions/v1beta1kind:Ingressmetadata:name:{{$fullName}}spec:rules:{{-range.Values.ingress.hosts}}-host:{{.host|quote}}http:paths:{{-range.paths}}-backend:serviceName:{{$fullName}}servicePort:80{{-end}}{{-end}}{{-end}}寫好後，我們就可以來一鍵部署我們的這三份檔案囉。我們可以直接在/helm-demo資料夾下輸入指令helminstall.NAME:gilded-peacockLASTDEPLOYED:MonMay616:31:272019NAMESPACE:defaultSTATUS:DEPLOYED部署成功後顯示的NAME:gilded-peacock就是這個Chart部署後的名稱囉（在Helm中稱部署出去的這個實體為release）。我們可以再透過指令helmlist列出我們目前所有的releases。接下來我們可以用kubectlgetall來看到我們目前的kubernetes狀況NAMEREADYSTATUSRESTARTSpod/gilded-peacock-helm-demo-5fc59647591/1Running0pod/gilded-peacock-helm-demo-5fc59647591/1Running0​NAMETYPECLUSTER-IPEXTERNAL-IPPORT(S)service/gilded-peacock-helm-demoNodePort10.106.164.53&lt;none&gt;80:30333/TCP​NAMEREADYUP-TO-DATEAVAILABLEdeployment.apps/gilded-peacock-helm-demo2/222​NAMEDESIREDCURRENTreplicaset.apps/gilded-peacock-helm-demo-5fc596475922這邊就可以看到我們所指定的資源都有按照chart的配置建立起來囉，所以打開blue.demo.com就可以看到一隻我們透過Helm實際部署出的小鯨魚。以上，我們就完成Helm的實際部署囉！而其他常用的Helm指令還有helmdelete--purgeRELEASE_NAME刪除一個release（--purge這個flag可以把該RELEASE_NAME釋放出來供之後重複使用）。helmupgradeRELEASE_NAMECHART_PATH如果有更新Chart的檔案時，可以透過upgrade去更新對應的Release。helmlintCHART_PATH檢查你的Chart檔案有沒有錯誤的語法。helmpackageCHART_PATH打包並壓縮整個Chart資料夾的檔案。kubectl額外補充簡寫覺得每次下指令都要打kubectl很花時間的話，可以透過alias來節省時間，例如設定aliaskbs=kubectl。kubectl中的各項資源的名稱其實也都有內建的簡寫，可以透過指令kubectlapi-resources去看到各個資源的簡寫，例如deployments可以簡寫成deploy、services簡寫成svc等。auto-complete覺得kubectl的指令沒有auto-complete很痛苦的話可以參考官網的教學。像是如果使用zsh的話，就可以透過指令echo&quot;if[$commands[kubectl]];thensource&lt;(kubectlcompletionzsh);fi&quot;&gt;&gt;~/.zshrc來啟用kubectl的auto-completecreatevsapply在之前提到透過yaml建立資源時，我們都用了kubectlcreate-f。但其實也可以使用kubectlapply-f達成建立與更新資源，雖然在單純建立的使用情景上沒有差別，但其它用法上的差別可見kubectlapplyvskubectlcreate。小結呼！希望這三篇Kubernetes基礎教學能對你有幫助！再次溫故知新一下：在系列文的第一篇文章中，我們了解了構成Kubernetes的四個重要元素：Pod、Node、Master、Cluster，並安裝好了我們要實際動手玩Kubernetes前需要的套件與工具。而在第二篇文章中，我們實際動手操作了Kubernetes中的Pod、Service、Deployment、Ingress。在最後的這篇文章中，我們了解了讓Kubernetes部署變得好輕鬆好棒棒的工具Helm，並實際透過Helm部署了一個Helm。在學會了上述的各個基礎後，如果想要實際在各個雲端服務上操作Kubernetes的話，歡迎接下來可以進一步去了解Kops！原文链接參考資料：HelmOfficialDocumenthttps://helm.sh/docs/KubernetesOfficialDocumenthttps://kubernetes.io/docs/home/Kubernetes30天學習筆記byzxcvbniushttps://ithelp.ithome.com.tw/articles/10192401五分鐘Kubernetes有感https://medium.com/@evenchange4/%E4%BA%94%E5%88%86%E9%90%98-kubernetes-%E6%9C%89%E6%84%9F-e51f093cb10b十分鐘帶你理解Kubernetes核心概念http://dockone.io/article/932Kubernetes學習筆記https://gcpug-tw.gitbook.io/kuberbetes-in-action/[教學]用Drone,Kubernetes跟Helm，以及RBAC來建置你的CI/CD流程https://medium.com/@cloudsanchen/ci-cd-with-drone-kubernetes-and-helm-part-1-69c147046ffa","link":"https://chriswsq.github.io/post/kubernetes-ji-chu-jiao-xue-san-helm-jie-shao-yu-jian-li-chart/"},{"title":"Kubernetes 基础教学（二）实作范例：Pod、Service、Deployment、Ingress","content":"Kubernetes（K8S）是一個可以幫助我們管理微服務（microservices）的系統，他可以自動化地部署及管理多台機器上的多個容器（Container）。簡單來說，他可以做到：Kubernetes（K8S）是一个可以帮助我们管理微服务（microservices）的系统，他可以自动化地部署及管理多台机器上的多个容器（Container）。简单来说，他可以做到：同时部署多个容器到多台机器上（Deployment）服务的乘载量有变化时，可以对容器做自动扩展（Scaling）管理多个容器的状态，自动侦测并重启故障的容器（Management）在系列文的上一篇文章中，我们了解了构成Kubernetes的四个重要元素：Pod、Node、Master、Cluster，并安装好了我们要实际动手玩Kubernetes前需要的套件与工具。接下来在这篇文章中，我们会透过例子来实际建立那些在Kubernetes中常见的元件们。如何建立一个Pod撰写Pod的身分证还记得我们在介绍Kubernetes时有提到，每个Pod都有一个身分证，也就是属于这个Pod的.yaml档。我们透过撰写下面的这个.yaml档就可以建立出Pod。kubernetes-demo.yamlapiVersion:v1kind:Podmetadata:name:kubernetes-demo-podlabels:app:demoAppspec:containers:-name:kubernetes-demo-containerimage:hcwxd/kubernetes-demoports:-containerPort:3000apiVersion该元件版本号kind该元件是什么属性，常见有Pod、Node、Service、Namespace、ReplicationController等metadataname指定该Pod的名称labels指定该Pod的标签，这里我们暂时帮它上标签为app:demoAppspeccontainer.name指定运行出的Container的名称container.image指定Container要使用哪个Image，这里会从DockerHub上搜寻container.ports指定该Container有哪些portnumber是允许外部资源存取透过kubectl建立Pod有了身份证后，我们就可以透过kubectl指令来建立Podkubectlcreate-fkubernetes-demo.yaml看到pod/kubernetes-demo-podcreated的字样就代表我们建立成功我们的第一个Pod了。我们可以再透过指令kubectlgetpods看到我们运行中的Pod：NAMEREADYSTATUSRESTARTSAGEkubernetes-demo-pod1/1Running060s连线到我们Pod的服务资源建立好我们的Pod之后，打开浏览器的localhost:3000我们会发现怎么什么都看不到。这是因为在Pod中所指定的port，跟我们本机端的port是不相通的。因此，我们必须还要透过kubectlport-forward，把我们两端的port做mapping。kubectlport-forwardkubernetes-demo-pod3000:3000做好mapping后，再打开浏览器的localhost:3000，我们就可以迎接一只可爱的小鲸鱼啰！Kubernetes进阶三元件了解完如何从无到有建立一个KubernetesCluster并产生一个Pod后，接下来我们要认识在现实应用中，我们还会搭配到哪些Kubernetes的进阶元件。其中最重要的三个进阶元件就是：Service、Ingress、Deployment。Service还记得上面提到我们在连线到一个Pod的服务资源时，会使用到port-forward的指令。但如果我们有多个Pods想要同时被连线时，我们就可以用到Service这个进阶元件。简单来说，Service就是Kubernetes中用来定义「一群Pod要如何被连线及存取」的元件。要建立一个Service，一样要撰写属于他的身分证。service.yamlapiVersion:v1kind:Servicemetadata:name:my-servicespec:selector:app:demoApptype:NodePortports:-protocol:TCPport:3001targetPort:3000nodePort:30390apiVersion该元件的版本号kind该元件是什么属性，常见有Pod、Node、Service、Namespace、ReplicationController等metadataname指定该Pod的名称specselector该Service的连线规则适用在哪一群Pods，还记得我们在建立Pod的时候，会帮它上label，这时就可以透过app:demoApp，去找到那群label的app属性是demoApp的Pods们portstargetPort指定我们Pod上允许外部资源存取PortNumberport指定我们Pod上的targetPort要mapping到Service中ClusterIP中的哪个portnodePort指定我们Pod上的targetPort要mapping到Node上的哪个port接下来我们先重新建立我们的Podkubectlcreate-fkubernetes-demo.yaml接下来我们透过service.yaml来建立我们的Service元件kubectlcreate-fservice.yaml然后我们可以透过kubectlgetservices取得我们新建立Service的资料NAMETYPECLUSTER-IPEXTERNAL-IPPORT(S)AGEmy-serviceNodePort10.110.237.205&lt;none&gt;3001:30391/TCP60s有了建立好的Service后，我们可以透过两种方式连线我们的Pod的服务资源。首先，要从外部连线到我们的Pod资源服务，我们必须要先有我们的KubernetesCluster（在这边是minikube）对外开放的IP。我们先透过指令minikubeip得到我们minikube的ip192.168.99.100接着打开我们的浏览器，输入上面的ip加上我们在yaml档指定的nodePort，在这边是192.168.99.100:30390，就会得到我们的小鲸鱼了。而如果不从浏览器，而是直接从minikube里面连线到我们的Pod则要先透过指令minikubesshssh进入我们的minikubecluster，接着输入指令curl&lt;CLUSTER-IP&gt;:&lt;port&gt;其中CLUSTER-IP就是我们用kubectlgetservices得到我们Service的IP，而port就是我们在yaml档指定的port，在这边合起来就是10.110.237.205:3001，于是我们curl10.110.237.205:3001就可以在minikube里面得到我们的小鲸鱼啰！Deployment了解了Service后，接下来要来了解第二个进阶元件：Deployment。今天当我们同时要把一个Pod做横向扩展，也就是复制多个相同的Pod在Cluster中同时提供服务，并监控如果有Pod当机我们就要重新把它启动时，如果我们要一个Pod一个Pod透过指令建立并监控是很花时间的。因此，我们可以透过Deployment这个特殊元件帮我们达成上述的要求。同样要建立一个Deployment，要先撰写属于他的身分证。deployment.yamlapiVersion:apps/v1kind:Deploymentmetadata:name:my-deploymentspec:replicas:3template:metadata:labels:app:demoAppspec:containers:-name:kubernetes-demo-containerimage:hcwxd/kubernetes-demoports:-containerPort:3000selector:matchLabels:app:demoAppapiVersion该元件的版本号kind该元件是什么属性，常见有Pod、Node、Service、Namespace、ReplicationController等metadataname指定该Pod的名称specreplicas指定要建立多少个相同的Pod，在这边给的数字是所谓的DesireState，当Cluster运行时如果Pod数量低于此数字，Kubernetes就会自动帮我们增加pod，反之就会帮我们关掉Podtemplate指定这个Deployment建立的Pod们统一的设定，包括metadata以及这些Pod的Containers，这边我们就沿用之前建立Pod的设定selector指定这个Deployment的规则要适用到哪些Pod，在这边就是指定我们在template中指定的labels接下来我们就可以透过指令kubectlcreate-fdeployment.yaml建立好我们的Deployment，这时我们可以查看我们的Deployment有没有被建立好kubectlgetdeployNAMEREADYUP-TO-DATEAVAILABLEAGEmy-deployment3/33360s接着我们在看Pod们有没有乖乖按照Deployment建立kubectlgetpodsNAMEREADYSTATUSRESTARTSAGEmy-deployment-5454f687cd-bxjfz1/1Running060smy-deployment-5454f687cd-gszbr1/1Running060smy-deployment-5454f687cd-k6zfv1/1Running060s这边我们可以看到三个Pod都被建立好了，我们就成功做到了Pod的横向扩展。而除了Pod的横向扩展外，Deployment的另外一个好处就是可以帮我们做到无停机的系统升级（ZeroDowntimeRollout）。也就是说，当我们要更新我们的Pod时，Kubernetes并不会直接砍掉我们所有的Pod，而是会建立新的Pod，等新的Pod开始正常运行后，再来取代旧的Pod。举例来说，假设我们现在想要更新我们Pod对外的Port，我们可以先透过指令kubectleditdeploymentsmy-deployment接着我们会看到我们的Yaml档apiVersion:extensions/v1beta1kind:Deploymentmetadata:annotations:deployment.kubernetes.io/revision:'2'creationTimestamp:'2019-04-26T04:18:26Z'generation:2labels:app:demoAppname:my-deploymentnamespace:defaultresourceVersion:'328692'selfLink:/apis/extensions/v1beta1/namespaces/default/deployments/my-deploymentuid:56608fb5-67da-11e9-933f-08002789461fspec:progressDeadlineSeconds:600replicas:3revisionHistoryLimit:10selector:matchLabels:app:demoAppstrategy:rollingUpdate:maxSurge:25%maxUnavailable:25%type:RollingUpdatetemplate:metadata:creationTimestamp:nulllabels:app:demoAppspec:containers:-image:hcwxd/kubernetes-demoimagePullPolicy:Alwaysname:kubernetes-demo-containerports:-containerPort:3000protocol:TCPresources:{}terminationMessagePath:/dev/termination-logterminationMessagePolicy:FilednsPolicy:ClusterFirstrestartPolicy:AlwaysschedulerName:default-schedulersecurityContext:{}terminationGracePeriodSeconds:30我们把其中containerPort:3000改成3001后储存，Kubernetes就会开始帮我们进行更新。这时我们继续用指令kubectlgetpods就会看到NAMEREADYSTATUSRESTARTSAGEmy-deployment-5454f687cd-bxjf1/1Running060smy-deployment-5454f687cd-gszb1/1Terminating060smy-deployment-5454f687cd-k6zf1/1Running060smy-deployment-78dc8dcb89-59270/1ContainerCreating01smy-deployment-78dc8dcb89-dwtl1/1Running05s从上面可以看到，Kubernetes会永远保持有3个Pods在正常运作，如果有新的Pod还在ContainerCreating的阶段时，他还不会关掉对应要被取代的Pod。而在过一段时间我们输入同样指令可以看到kubectlrollouthistorydeploymentmy-deployment看到我们目前更改过的版本deployment.extensions/my-deploymentREVISIONCHANGE-CAUSE1&lt;none&gt;2&lt;none&gt;从上面可以看出来，我们目前有两个版本，如果我们发现版本2的程式有问题，想要先让服务先恢复成版本1的程式（Rollback）时，我们还可以透过指令kubectlrolloutundodeploymy-deployment让我们的Pod都恢复成版本1。甚至之后如果版本变的较多后，我们也可以指定要Rollback到的版本kubectlrolloutundodeploymy-deployment--to-revision=2Ingress了解完了Service跟Deployment後，接下來就輪到概ㄓ念稍微複雜的Ingress元件了。在上面有提到Service就是Kubernetes中用來定義「一群Pod要如何被連線及存取」的元件。但在Service中，我們是將每個Service元件對外的portnumber跟Node上的portnumber做mapping，這樣在我們的Service變多時，portnumber以及分流規則的管理變得相當困難。而Ingress可以透過HTTP/HTTPS，在我們眾多的Service前搭建一個reverse-proxy。這樣Ingress可以幫助我們統一一個對外的portnumber，並且根據hostname或是pathname決定封包要轉發到哪個Service上，如同下圖的比較：在Kubernetes中，Ingress这项服务其实是由IngressResources、IngressServer、IngressController构成。其中IngressResources就是定义Ingress的身分证，而IngressServer则是实体化用来接收HTTP/HTTPS连线的网路伺服器。但实际上，IngressServer有各式各样的实作，就如同市面上的WebServer琳琅满目一样。因此，IngressController就是一个可以把定义好的IngressResources设定转换成特定IngressServer实作的角色。举例来说，Kubernetes由官方维护的两种IngressController就有ingress-gce跟ingress-nginx，分别可以对应转换成GCE与Nginx。也有其他非官方在维护的Controller，详细的列表可见官网的additional-controllers。接下来我们要来试着建立一个Ingress物件去根据hostname转发封包到不同的Pod上面。所以第一步，我们要用Deployment建立好几个不同的Pod。在这边我们直接透过准备好的两个Image来建立其中的Container，blue-whale这个Image里的程式会监听3000port然后在浏览器上被存取时会吐出蓝色的鲸鱼，purple-whale则会吐出紫色的鲸鱼。deployment.yamlapiVersion:extensions/v1beta1kind:Deploymentmetadata:name:blue-nginxspec:replicas:2template:metadata:labels:app:blue-nginxspec:containers:-name:nginximage:hcwxd/blue-whaleports:-containerPort:3000---apiVersion:extensions/v1beta1kind:Deploymentmetadata:name:purple-nginxspec:replicas:2template:metadata:labels:app:purple-nginxspec:containers:-name:nginximage:hcwxd/purple-whaleports:-containerPort:3000接着我们就可以透过kubectlcreate-fdeployment.yaml建立好我们的Pod。AMEREADYSTATUSRESTARTSAGEblue-nginx-6b68c797c7-28tkz1/1Running060sblue-nginx-6b68c797c7-8ww8l1/1Running060spurple-nginx-84854fd7c-8g4nl1/1Running060spurple-nginx-84854fd7c-tmrbs1/1Running060s建立好了Pod们后，接下来我们就要建立这些Pod对外的各自Service，在这边我们会把各至Container上的3000port全部都转到80port上。service.yamlapiVersion:v1kind:Servicemetadata:name:blue-servicespec:type:NodePortselector:app:blue-nginxports:-protocol:TCPport:80targetPort:3000---apiVersion:v1kind:Servicemetadata:name:purple-servicespec:type:NodePortselector:app:purple-nginxports:-protocol:TCPport:80targetPort:3000透过kubectlcreate-fservice.yaml建立好我们的service。NAMETYPECLUSTER-IPEXTERNAL-IPPORT(S)blue-serviceNodePort10.111.192.164&lt;none&gt;80:30492/TCPpurple-serviceNodePort0.107.21.77&lt;none&gt;80:32086/TCP最后，我们就可以来建立我们的主角Ingress了！在这边我们的Ingress只有很简单的规则，他会把所有发送到blue.demo.com的封包交给serviceblue-service负责，而根据上面service.yaml的定义，他会再转交给blue-nginx这个Pod。而发送给purple.demo.com则会转交给purple-nginx。在这边，我们要先记得使用指令minikubeaddonsenableingress来启用minikube的ingress功能。接着，我们就来撰写ingress的身分证。ingress.yamlapiVersion:extensions/v1beta1kind:Ingressmetadata:name:webspec:rules:-host:blue.demo.comhttp:paths:-backend:serviceName:blue-serviceservicePort:80-host:purple.demo.comhttp:paths:-backend:serviceName:purple-serviceservicePort:80我们一样透过kubectlcreate-fingress.yaml来建立我们的ingress物件。并使用kubectlgetingress来查看我们的ingress状况：NAMEHOSTSADDRESSPORTSAGEwebblue.demo.com,purple.demo.com10.0.2.158060s接下来我们要来测试ingress有没有乖乖帮我们转发。因为我们的Cluster实际上对外的ip都是我们透过指令minikubeip会看到的192.168.99.100，这样我们要怎么同时让这个ip可以是我们设定规则中的blue.demo.com以及purple.demo.com呢？因为我们知道在DNS解析网址时，会先查找本机上/etc/hosts后才会到其他DNSServer上寻找。所以我们可以透过一个小技巧，在本机上把blue.demo.com以及purple.demo.com都指向192.168.99.100。透过指令echo192.168.99.100blue.demo.com&gt;&gt;/etc/hostsecho192.168.99.100purple.demo.com&gt;&gt;/etc/hosts或是透过sudovim/etc/hosts手动加上这两条规则，我们就成功搞定DNS可以来测试了。接下来我们打开浏览器，输入blue.demo.com就可以得到熟悉的蓝色小鲸鱼然后输入purple.demo.com就可以得到紫色小鲸鱼啰！在实际建立过Pod、Service、Deployment还有Ingress后，在接下来的文章，我们要来介绍一个可以让这个建立流程变得更简单的工具，也就是Kubernetes中的PackageManager：Helm！","link":"https://chriswsq.github.io/post/kubernetes-ji-chu-jiao-xue-er-shi-zuo-fan-li-podservicedeploymentingress/"},{"title":"kubernetes基础教学（一）原理","content":"Kubernetes（K8S）是一个可以帮助我们管理微服务（microservices）的系统，他可以自动化地部署及管理多台机器上的多个容器（Container）。更进一步地说，Kubernetes想解决的问题是：「手动部署多个容器到多台机器上并监测管理这些容器的状态非常麻烦。」而Kubernetes要提供的解法：提供一个平台以较高层次的抽象化去自动化操作与管理容器们。打开Kubernetes的官网，我们可以看到关于Kubernetes服务的描述为：Automatedcontainerdeployment,scaling,andmanagement而白话来说，上面的描述表示他可以做到：同时部署多个容器到多台机器上（Deployment）服务的乘载量有变化时，可以对容器做自动扩展（Scaling）管理多个容器的状态，自动侦测并重启故障的容器（Management）Kubernetes四元件在了解Kubernetes如何帮助我们管理容器们前，我们先要由小到大依序了解组成Kubernetes的四种最基本的元件：Pod、WorkerNode、MasterNode、Cluster。PodKubernetes运作的最小单位，一个Pod对应到一个应用服务（Application），举例来说一个Pod可能会对应到一个APIServer。每个Pod都有一个身分证，也就是属于这个Pod的yaml档一个Pod里面可以有一个或是多个Container，但一般情况一个Pod最好只有一个Container同一个Pod中的Containers共享相同资源及网路，彼此透过localportnumber沟通WorkerNodeKubernetes运作的最小硬体单位，一个WorkerNode（简称Node）对应到一台机器，可以是实体机如你的笔电、或是虚拟机如AWS上的一台EC2或GCP上的一台ComputerEngine。每个Node中都有三个组件：kubelet、kube-proxy、ContainerRuntime。小提醒：在WorkerNode与MasterNode的组件部分，因为Kubernetes本身其实都抽象的很好，所以在Kubernetes「基础的」使用上如何不了解这些组建也不会有非常大的影响。kubelet该Node的管理员，负责管理该Node上的所有Pods的状态并负责与Master沟通kube-proxy该Node的传讯员，负责更新Node的iptables，让Kubernetes中不在该Node的其他物件可以得知该Node上所有Pods的最新状态kube-apiserver管理整个Kubernetes所需API的接口（Endpoint），例如从CommandLine下kubectl指令就会把指令送到这里负责Node之间的沟通桥梁，每个Node彼此不能直接沟通，必须要透过apiserver转介负责Kubernetes中的请求的身份认证与授权etcd用来存放KubernetesCluster的资料作为备份，当Master因为某些原因而故障时，我们可以透过etcd帮我们还原Kubernetes的状态kube-controller-manager负责管理并运行Kubernetescontroller的组件，简单来说controller就是Kubernetes里一个个负责监视Cluster状态的Process，例如：NodeController、ReplicationController这些Process会在Cluster与预期状态（desirestate）不符时尝试更新现有状态（currentstate）。例如：现在要多开一台机器以应付突然增加的流量，那我的预期状态就会更新成N+1，现有状态为N，这时相对应的controller就会想办法多开一台机器controller-manager的监视与尝试更新也都需要透过访问kube-apiserver达成kube-scheduler整个Kubernetes的Pods调度员，scheduler会监视新建立但还没有被指定要跑在哪个Node上的Pod，并根据每个Node上面资源规定、硬体限制等条件去协调出一个最适合放置的Node让该Pod跑ClusterKubernetes中多个Node与Master的集合。基本上可以想成在同一个环境里所有Node集合在一起的单位。基本运作与安装因原文为外网才能访问所以转载原文链接","link":"https://chriswsq.github.io/post/kubernetes-ji-chu-xue-xi-yi-yuan-li/"},{"title":"k8s——apiVersion对照表","content":"不同的控制器选用不同的apiVersion对照表kindapiVersionCertificateSigningRequestcertificates.k8s.io/v1beta1ClusterRoleBindingrbac.authorization.k8s.io/v1ClusterRolerbac.authorization.k8s.io/v1ComponentStatusv1ConfigMapv1ControllerRevisionapps/v1CronJobbatch/v1beta1DaemonSetextensions/v1beta1Deploymentextensions/v1beta1Endpointsv1Eventv1HorizontalPodAutoscalerautoscaling/v1Ingressextensions/v1beta1Jobbatch/v1LimitRangev1Namespacev1NetworkPolicyextensions/v1beta1Nodev1PersistentVolumeClaimv1PersistentVolumev1PodDisruptionBudgetpolicy/v1beta1Podv1PodSecurityPolicyextensions/v1beta1PodTemplatev1ReplicaSetextensions/v1beta1ReplicationControllerv1ResourceQuotav1RoleBindingrbac.authorization.k8s.io/v1Rolerbac.authorization.k8s.io/v1Secretv1ServiceAccountv1Servicev1StatefulSetapps/v1apiVersion分别是什么意思？名称中带有'alpha'的alphaAPI版本是Kubernetes中新功能的早期候选者。这些可能包含错误，并且不能保证将来能够正常工作。测试在API版本名称意味着测试已取得进展过去的阿尔法水平，并且该功能最终将被列入Kubernetes“测试”。尽管其工作方式可能会发生变化，并且对象的定义方式可能会发生完全变化，但功能本身很有可能以某种形式将其纳入Kubernetes。稳定名称中不包含“alpha”或“beta”。它们是安全使用的。v1这是KubernetesAPI的第一个稳定版本。它包含许多核心对象。apps/v1apps是Kubernetes中最常见的API组，其中许多核心对象均来自v1。它包括与在Kubernetes上运行应用程序相关的功能，例如Deployments，RollingUpdates和ReplicaSets。autoscaling/v1此API版本允许根据不同的资源使用量指标对pod进行自动缩放。此稳定版本仅支持CPU扩展，但是将来的alpha和beta版本将允许您根据内存使用情况和自定义指标进行扩展。batch/v1的batchAPI组包含与批处理和作业样的任务（而不是应用类一样无限期运行Web服务器的任务）的对象。该apiVersion是这些API对象的第一个稳定版本。batch/v1beta1Beta版本，用于Kubernetes中批处理对象的新功能，特别是包括CronJobs，它使您可以在特定的时间或周期性地运行Jobs。certificate.k8s.io/v1beta1此API版本增加了用于验证网络证书以在群集中进行安全通信的功能。您可以阅读更多有关官方文档的信息。extensions/v1beta1此版本的API包括Kubernetes的许多常用新功能。在此版本中，部署，DaemonSet，副本集和Ingress都进行了重大更改。请注意，在Kubernetes1.6中，其中一些对象已从重新定位extensions到特定的API组（例如apps）。当这些对象移出测试版时，应将它们归入特定的API组，例如apps/v1。使用extensions/v1beta1正变得过时，尝试用在可能情况下，根据您的Kubernetes集群版本的特定API组。policy/v1beta1此apiVersion增加了设置容器中断预算和有关容器安全性的新规则的功能。rbac.authorization.k8s.io/v1此apiVersion包含用于Kubernetes基于角色的访问控制的附加功能。这可以帮助您保护群集。查看官方博客文章。","link":"https://chriswsq.github.io/post/k8s-apiversion-dui-zhao-biao/"},{"title":"NFS网络文件系统部署","content":"FS（NetworkFileSystem），即网络文件系统。NFS服务可以将远程Linux系统上的文件共享资源挂载到本地主机的目录上，从而使用本地主机（Linux客户端）像使用本地资源那样读写远程Linux系统上的共享资源。系统环境主机名称操作系统IP地址NFS服务器Centos764位192.168.40.6NFS客户端Centos764位192.168.40.7NFS服务配置文件的参数参数作用rorw读写root_squash当NFS客户端以root管理员访问时，映射为NFS服务器的匿名用户no_root_squash当NFS客户端以root管理员访问时，映射为NFS服务器的root管理员all_squash无论NFS客户端使用什么账户访问，均映射为NFS服务器的匿名用户sync同时将数据写入到内存与硬盘中，保证不丢失数据async优先将数据写入到内存，然后再写入硬盘；这样效率更高，但可能会丢失数据NFS服务器操作1、安装NFSyum-yinstallnfs-utils2、创建用于NFS共享的目录mkdir/testchown-Rnfsnobody/nfsdata3、编辑NFS的配置文件，添加如下内容注：NFS的配置文件默认是没有内容的vim/etc/exports/nfsdata192.168.40.*(rw,sync,root_squash)4、启动NFS服务，并加入开机启动项NFS服务需要使用RPC（RemoteProcedureCall，远程过程调用）服务将NFS服务器的IP地址和端口号等信息发送给客户端，因此，在启动NFS服务之前，还需要顺带启动rpcbind服务。systemctlstartrpcbindsystemctlenablerpcbindsystemctlstartnfs-serversystemctlenablenfs-server5、查看nfs向rpc注册的端口信息rpcinfo-plocalhost注：下图中用红框括起来的端口号需要防火墙允许6、配置firewalld防火墙，允许nfs和rpc端口firewall-cmd--permanent--add-service=nfsfirewall-cmd--permanent--add-service=mountdfirewall-cmd--permanent--add-port=111/tcpfirewall-cmd--permanent--add-port=111/udpfirewall-cmd--reloadNFS客户端操作1、使用showmount命令查询NFS服务器的远程共享信息表3：showmount命令可用的参数以及作用参数作用-e显示NFS服务器的共享列表-a显示本机挂载的文件资源的情况-v显示版本号查看能否连接到nfs服务showmount-e192.168.40.62、创建挂载目录，并挂载mkdir/nfsdatamount-tnfs192.168.40.6:/nfsdata/nfsdatadf-h3、将挂载信息写入/etc/fstab文件中，以便开机自动挂载vim/etc/fstab192.168.40.6:/nfsdata/nfsdatanfsdefaults004、测试往/nfsdata目录下写入一个文件echo&quot;welcometoxuad.com&quot;&gt;/nfsdata/xuad.txt在NFS服务器上查看/nfsdata目录下是否生成了此文件","link":"https://chriswsq.github.io/post/nfs-wang-luo-wen-jian-xi-tong-bu-shu/"},{"title":"sed&awk笔记","content":"sed&amp;awk提升工作效率的小工具删除行首空格或者tabsed-i's/^[\\t]*//g'file删除行尾空格或者tabsed-i's/[\\t]*$//g'file注释特定行sed-i'/swapfile/s/^/#/'/etc/fstabsed-i'/xvdb/s/^/#/g'/etc/fstabsed-i'/vdb/s/^/#/g'/etc/fstab取消注释sed-i'/swapfile/s/^#//'/etc/fstabsed-i'/xvdb/s/^#//g'/etc/fstabsed-i'/vdb/s/^#//g'/etc/fstab注释未注释行sed-i's/^[^#]/#&amp;/'/var/spool/cron/root首字母大写sed's/\\b[a-z]/\\U&amp;/g'file首字母小写sed's/\\b[a-Z]/\\L&amp;/g'file在包含某个字符的上一行或者下一行插入内容[root@RedHattest]#cattestfilehello[root@RedHattest]#sed-i'/hello/i\\\\up'testfile[root@RedHattest]#cattestfileuphello[root@RedHattest]#sed-i'/hello/a\\\\down'testfile[root@RedHattest]#cattestfileuphellodown[root@RedHattest]#假如有两个关键字hello，那么在每一行上面或者下面都插入内容","link":"https://chriswsq.github.io/post/sedandawk-bi-ji/"},{"title":"xuperchain添加旷工节点","content":"xuperchain通过提案动态添加旷工节点因为创建链是一次性的操作，那么后续需要改动关于xuper.json里面的参数则要通过进行提案来进行修改一共两个步骤发起提案投票编写提案文件首先查看当前块高度，因为提案文件的投票截止高度和生效高度是根据目前高度来写的./xchain-clistatus-H192.168.40.6:37101|jq'.blockchains[]|{&quot;name&quot;:.name,&quot;height&quot;:.ledger.trunkHeight}'{&quot;name&quot;:&quot;xuper&quot;,&quot;height&quot;:26505}首先需要准备一个提案的文件，json格式proposal.json{&quot;module&quot;:&quot;proposal&quot;,&quot;method&quot;:&quot;Propose&quot;,&quot;args&quot;:{&quot;min_vote_percent&quot;:51,&quot;stop_vote_height&quot;:26600},&quot;trigger&quot;:{&quot;height&quot;:26630,&quot;module&quot;:&quot;consensus&quot;,&quot;method&quot;:&quot;update_consensus&quot;,&quot;args&quot;:{&quot;name&quot;:&quot;tdpos&quot;,&quot;config&quot;:{&quot;version&quot;:&quot;21&quot;,&quot;proposer_num&quot;:&quot;3&quot;,&quot;period&quot;:&quot;3000&quot;,&quot;alternate_interval&quot;:&quot;6000&quot;,&quot;term_interval&quot;:&quot;9000&quot;,&quot;block_num&quot;:&quot;20&quot;,&quot;vote_unit_price&quot;:&quot;1&quot;,&quot;init_proposer&quot;:{&quot;1&quot;:[&quot;2B1rDQhq7W4TStSHoD88N1SUYXrCDV821v&quot;,&quot;rwGpYwpkcpMgxdGJ9KX9xSvJPiCyPsFVQ&quot;,&quot;262G4VuXBmFg6W486XqY4bj2iMotHG5ypb&quot;]}}}}}注意：提案文件里不能有注释需要注意的是当前的区块高度，来设置合理的截至计票高度和生效高度转账然后在矿工节点下，执行给自己转账的操作，并在--desc参数里传入提案./xchain-clitransfer--todpzuVdosQrF2kmzumhVeFQZa1aYcdgFpN--descproposal.json--amount1运行后会得到本次提案的交易id，需要记录下来供投票使用转账地址写当前天的节点就可以，这里的交易金额写1也行，官方是100投票对提案进行投票操作由如下命令执行：查看当前总金额./xchain-clistatus|greputxoTotal10000000523./xchain-clivotef26d670b695d9fd5da503a34d130ef19e738b35e031b18b70ad4cbbf6dfe2656--frozen26650--amount100002825031900000000注意：--frozen参数的冻结高度大于提案生效的高度，也就是大于26630。这里需要注意进行投票的节点需要有矿工账号的密钥对因为最终通过的规则是投票资源大于总资源的51%，所以需要初始token量最多的矿工账号来进行投票，并保证token数符合要求。--amount金额为大于总金额的51%查看状态待到当前块到生效高度时查看当前tdpos算法的状态[root@test-1output]#./xchain-clitdposstatus{&quot;term&quot;:21,&quot;block_num&quot;:4,&quot;proposer&quot;:&quot;rwGpYwpkcpMgxdGJ9KX9xSvJPiCyPsFVQ&quot;,&quot;proposer_num&quot;:3,&quot;checkResult&quot;:[&quot;2B1rDQhq7W4TStSHoD88N1SUYXrCDV821v&quot;,&quot;rwGpYwpkcpMgxdGJ9KX9xSvJPiCyPsFVQ&quot;,&quot;262G4VuXBmFg6W486XqY4bj2iMotHG5ypb&quot;]}注意：刚到投票生效的时间执行此命令查看的时候可能会有报错情况[root@test-1output]#./xchain-clitdposstatusrpcerror:code=Unknowndesc=leveldb:notfound等一会就好了","link":"https://chriswsq.github.io/post/xuperchain-tian-jia-kuang-gong-jie-dian/"},{"title":"生产xuperchain部署文档","content":"百度区块链部署xuperchain部署信息**版本：**3.7.0**加密方式：**国密**共识：**tdpos**链类型：**联盟链以下为初始化建链前的一次性操作（建链后再修改需要进提案），修改完后创建xuper链，并将ouput目录拷贝至其他节点，后续其他节点根据情况修改个别需要节点唯一的内容即可（keys的三个文件,netURL地址）修改/output/conf/plugins.conf文件(将default的加密方式改为国密方式)修改/output/conf/xchain.yaml文件#在utxo区域中添加`nonUtxo:true`参数utxo:nonUtxo:true##wasm合约配置wasm:driver:&quot;xvm&quot;enableUpgrade:true#能够创建平行链的节点kernel:#minNewChainAmount设置创建平行链时最少要转多少钱到同链名addressminNewChainAmount:&quot;10&quot;newChainWhiteList:-25nbSZeSjMs8GT4TiSmgofnRusg1rxxggV:true#是否开启默认的XEndorser背书服务enableXEndorser:true修改/output/data/config/xuper.json文件，拷贝至其他节点（选择挖矿节点、挖矿节点数量、address地址为主节点即可、更改时间戳、挖矿节点netURL)注意：此模板文件节点之间必须一致，否则会出现块高不一致的情况maxblocksize、proposer_num、period、alternate_interval、term_interval、block_num参数根据实际情况修改，我这里是测试环境的配置拷贝至其他节点后的操作拷贝时确保主节点模板文件没有问题，不会再更改，这样拷贝过去后，不需要重新建链，如改动/output/data/config/xuper.json文件后，需要重新生成链，重新生成neturl地址(生成后的地址IP替换为实际IP)./xchain-clinetURLgen重新生成address地址（生产环境为开发提供每个节点的address地址）./xchain-cliaccountnewkeys-f修改xchain.yaml文件p2p区域的netURL地址，连接主节点（主节点开始先别连接其他节点，待环境部署好后可停止刚开始的主节点然后添加其他节点，使全部节点互相备份）启动服务#删除data/blockchain/*文件rm-rfdata/blockchain/*#创建链./xchain-clicreateChain#启动服务节点nohup./xchain&amp;注如果是单台机器启动多节点需修改三个端口（RPC、metricPort、p2p）先启动bootnode节点先修改插件的默认加密方式为国密后，手动生成的私有公钥和秘钥都会以国密的加密方式生成。创建合约账号如何要支持群组，需要在xuper链部署一个系统合约：GroupChain（一个网络有且仅有一个）#在xuper链部署GroupChain合约#拷贝group_chain.wasm文件到outpue目录(group_chain.wasm文件由开发提供)./xchain-cliaccountnew--account1111111111111111./xchain-cliwasmdeploy--accountXC1111111111111111@xuper--cnamegroup_chain./group_chain.wasm./xchain-cliaccountcontracts--accountXC1111111111111111@xuper-H127.0.0.1:37101创建平行链此步骤在测试阶段可手动进行测试，正式环境集成在程序内创建平行链应的节点应和xuper.json文件中指定地址、xchain.conf文件中允许创建平行链的地址一致#创建群组./xchain-cliwasminvokegroup_chain--methodaddChain-a'{&quot;bcname&quot;:&quot;xchain_chriswang&quot;}'#添加节点(可在网上https://www.bejson.com中检查；可添加多个节点)./xchain-cliwasminvokegroup_chain--methodaddNode-a'{&quot;bcname&quot;:&quot;baidu_zhengqi3&quot;,&quot;ip&quot;:&quot;/ip4/192.168.52.4/tcp/40001/p2p/QmRUVTKqdYVE49VzQKEuGsr2afiwDkj1RMMtdcVdaaqLUX&quot;,&quot;address&quot;:&quot;qaXhH7gJcdfpapmWkbHdLNqUFq3Vst6Am&quot;}'#查看node节点，检查是否添加成功./xchain-cliwasmquerygroup_chain--methodlistNode-a'{&quot;bcname&quot;:&quot;xchain_chriswang&quot;}'#创建平行链#创建平行链的json文件（模版），如下：{&quot;Module&quot;:&quot;kernel&quot;,&quot;Method&quot;:&quot;CreateBlockChain&quot;,&quot;Args&quot;:{&quot;name&quot;:&quot;xchain_chriswsq&quot;,&quot;data&quot;:&quot;{\\&quot;version\\&quot;:\\&quot;1\\&quot;,\\&quot;predistribution\\&quot;:[{\\&quot;address\\&quot;:\\&quot;266L6fw9rBXSm4uBciLwRWTkonb2HCqz5a\\&quot;,\\&quot;quota\\&quot;:\\&quot;100000000000000000000\\&quot;}],\\&quot;maxblocksize\\&quot;:\\&quot;128\\&quot;,\\&quot;award\\&quot;:\\&quot;1000000\\&quot;,\\&quot;decimals\\&quot;:\\&quot;8\\&quot;,\\&quot;award_decay\\&quot;:{\\&quot;height_gap\\&quot;:31536000,\\&quot;ratio\\&quot;:1},\\&quot;gas_price\\&quot;:{\\&quot;cpu_rate\\&quot;:1000,\\&quot;mem_rate\\&quot;:1000000,\\&quot;disk_rate\\&quot;:1,\\&quot;xfee_rate\\&quot;:1},\\&quot;new_account_resource_amount\\&quot;:1000,\\&quot;crypto\\&quot;:\\&quot;gm\\&quot;,\\&quot;genesis_consensus\\&quot;:{\\&quot;name\\&quot;:\\&quot;tdpos\\&quot;,\\&quot;config\\&quot;:{\\&quot;timestamp\\&quot;:\\&quot;1559021720000000000\\&quot;,\\&quot;proposer_num\\&quot;:\\&quot;1\\&quot;,\\&quot;period\\&quot;:\\&quot;3000\\&quot;,\\&quot;alternate_interval\\&quot;:\\&quot;3000\\&quot;,\\&quot;term_interval\\&quot;:\\&quot;6000\\&quot;,\\&quot;block_num\\&quot;:\\&quot;20\\&quot;,\\&quot;vote_unit_price\\&quot;:\\&quot;1\\&quot;,\\&quot;init_proposer\\&quot;:{\\&quot;1\\&quot;:[\\&quot;266L6fw9rBXSm4uBciLwRWTkonb2HCqz5a\\&quot;]},\\&quot;init_proposer_neturl\\&quot;:{\\&quot;1\\&quot;:[\\&quot;/ip4/127.0.0.1/tcp/47101/p2p/QmP38zphQmThRkK5DtiR7iN2ERDiZ1YZF5aXgJHjszj79t\\&quot;]}}}}&quot;}}使用如下指令即可创建平行链,转了100个主链的token到平行链同名的address，作为创建链的代价：./xchain-clitransfer--tobaidu_zhengqi3--amount100--descxchain_chriswang.json02a19467620d01cea01242c94442a39e550e8ae30bc92eaffb341aeaf5f0fa66基本命令#创建普通用户,包含地址，公钥，私钥./xchain-cliaccountnewkeys-f#获取本地netURL地址./xchain-clinetURLget-H127.0.0.1:37101#重新生成本地节点的网络私钥./xchain-clinetURLgen#显示本地节点的p2p地址./xchain-clinetURLpreview#创建xuper链./xchain-clicreateChain#check服务运行状况./xchain-clistatus-H127.0.0.1:37101#查看块高确保每个节点一致变化./xchain-clistatus-H127.0.0.1:37101|jq'.blockchains[]|{&quot;name&quot;:.name,&quot;height&quot;:.ledger.trunkHeight}'#查看本节点账户余额./xchain-cliaccountbalance--keysdata/keys-H127.0.0.1:37101#查看链总账户余额（要注意如果多个链可能会有多个utxoTotal）./xchain-clistatus|greputxoTotal#查看tdpos共识的状态./xchain-clitdposstatus","link":"https://chriswsq.github.io/post/sheng-chan-xuperchain-bu-shu-wen-dang/"},{"title":"ansible学习","content":"⏰ansible自动化运维工具，可以同时对多台主机进行管理，提升工作效率。在此记录工作中用到的东西ansible基本小知识组件inventoryansible的hosts文件是存放被管理主机的，被管理主机比较少的情况下，直接在hosts中定义即可，但是以后很定会管理多台主机，而ansible可管理的主机集合就叫做inventory。在ansible中，描述你主机的默认方法是将它们列在一个文本文件中,这个文件叫inventory文件。此时可以把不同分类的机器放在不同的inventory中，达到清晰管理的目的；配置文件：/etc/ansible/ansible.cfg配置参数：inventory=目录相关inventory文件都放在/etc/ansible/inventory目录下配置实例：[root@test-1~]#grep&quot;^inventory&quot;/etc/ansible/ansible.cfginventory=/etc/ansible/inventory[root@test-1~]#ls/etc/ansible/inventory/cacjqcontext.xmlfiscosigngateway_Mguangdongpeer1peer2peer3webasenodeyc注：inventory配置的是目录，此目录所有文件都会生效ventory（清单）的使用规则（定义主机和组）#“#”开头的行表示该行为注释行，即当时行的配置不生效。#Inventory（清单）可以直接为IP地址192.168.1.7#Inventory（清单）同样支持Hostname（主机名）的方式，后跟冒号加数字表示端口号，默认22号端口ntp.magede.com：22nfs.magede.com：22#中括号内的内容表示一个分组的开始，紧随其后的主机均属于该组成员，空行后的主机亦属于该组，即web2.magedu.com这台主机也属于[webservers]组。[webservers]web1.magedu.comweb[10:20].magedu.com#[10:20]表示10~20之间的所有数字（包括10和20），即表示web10.magedu.com、web11.magedu.com.................web20.magedu.com的所有主机。web2.magedu.com[dbservers]db-a.magedu.comdb-[b:f].magedu.com#[b:f]表示b到f之间的所有数字（包括b和f），即表示db-b.magedu.com、db-c.magedu.com..........db-f.magedu.com的所有主机。定义主机变量在平时工作中，通常会遇到非标准化的需求配置，如考虑到安全性问题，业务人员通常将企业内部的web服务80端口修改为其他端口号，而该功能可以直接通过修改Inventory（清单）配置来实现，在定义主机时为其添加主机变量，以便在Playbook中使用针对某一主机的个性化要求。例如：[webservers]web1.magedu.comhttp_port=808ansible_host=11.111.111.111ansible_ssh_port=22name=shanghai#自定义http_port的端口号为808、主机ip为11.111.111.111、ssh端口为22、定义变量name为shanghai定义组变量Ansible支持定义组变量，主要是针对大量机器的变量定义需求，赋予指定组内所有主机在Playbook中可用的变量，等同于逐一给该组下的所有主机赋予同一变量。例如：[groupservers]web1.magedu.comweb2.magedu.com[groupservers:vars]ntp_server=ntp.magedu.com#定义groupservers组中所有主机ntp_server值为ntp.magedu.comnfs_server=nfs.magedu.com#定义groupservers组中的所有主机nfs_server值为nfs.magedu.com其他Inventory（清单）参数列表除了支持如上的功能外，Ansible基于SSH连接Inventory（清单）中指定的远程主机时，还内置了很多其他参数，用于指定其交互方式。下面列举了部分重要参数：ansible_ssh_host：指定连接主机ansible_ssh_port，指定SSH连接端口，默认22ansible_ssh_user：指定SSH连接用户ansible_ssh_pass：指定SSH连接密码ansible_sudo_pass：指定SSH连接时sudo密码ansible_ssh_private_key_file：指定特有私钥文件","link":"https://chriswsq.github.io/post/ansible-xue-xi/"}]}